<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>机器学习 on Org Mode</title>
    <link>https://kylestones.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 机器学习 on Org Mode</description>
    <image>
      <url>https://kylestones.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://kylestones.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 23 Aug 2018 00:00:00 +0000</lastBuildDate><atom:link href="https://kylestones.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>见招拆招</title>
      <link>https://kylestones.github.io/blog/machinelearning/seethemove/</link>
      <pubDate>Thu, 23 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/machinelearning/seethemove/</guid>
      <description>机器学习并不是若干算法的堆积，熟练掌握了“十大算法”或“二十大算法”并不能让一切问题迎刃而解。所以不能将目光仅聚焦在具体算法 的推导和编程实现上。基本算法仅能呈现典型“套路”，而现实世界任务千变万化，以有限的套路应对无限的变化，焉有不败！所以务必掌 握算法背后的思想脉络，面对现实问题时，根据任务特点对现有套路进行改造融通。无论科研创新还是应用实践，皆以此为登堂入室之始。 — 周志华 &amp;lt;机器学习&amp;gt;
还有张三丰传授太极剑法给张无忌时，张无忌忘记了剑法具体的招式，仅记住了太极剑法的指导思想，从而根据敌人的招式使用相应的制 敌之策，达到见招拆招的目的。
Easy to learn. Hard to master.
YOLO 下面举 YOLO 算法中的几个的例子
作者通过均方无法来计算预测的 binding boxes 和 ground truth 之间的误差，由于 loss function 中还包含分类错误的误差。而 由于大多数的 grid cell 都没有目标，所以不应该让有目标和没有目标的 grid cell 产生相同权重的误差，所以作者让目标位置误 差的权重 \(\lambda_{coord} = 5\) ，让没有目标的分类误差权重 \(\lambda_{noobj} = 0.5\) ，从而来平衡由于数量悬殊导致的 问题。 使用均方误差计算，size 比较大的目标相比于 size 比较小的目标产生更大的误差。所以作者使用开方之后的宽度和高度值相减然后 求平方。 作者使用 224x224 的图像在 ImageNet 上使用分类网络对检测网络进行预训练，同时作者想让检测网络输入的分辨率为 448x448 。 由于需要同时改变输入的尺寸以及网络的任务，作者先使用 448x448 的ImageNet 对网络进行 fine-tune，然后在使用检测误差进行 调优，以达到更好的效果。 作者想要使用 Anchor Boxes ，但是 R-CNN 一般都是人为设定其大小，这个是目标的先验，如果能有更好的先验，那么应该会有更好 的检测结果，所以作者使用 k-means 聚类方法来求取 Anchor Boxes 先验的大小。 k-means 算法一般使用欧式距离度量误差，而此处，作者真实关心的是两者的 IOU ，所以作者用 1 - IOU 作为损失函数。 可以发现作者始终在依据自己的实际需求，对算法进行了一些改进，而不是直接生搬硬套 。</description>
    </item>
    
    <item>
      <title>Deep Learning</title>
      <link>https://kylestones.github.io/blog/machinelearning/deeplearning/</link>
      <pubDate>Thu, 12 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/machinelearning/deeplearning/</guid>
      <description>神经网络和深度学习 神经网络概论 结构化数据(structured data)：每个特征都有清晰、明确有意义的定义；比如房屋的面积，人的身高等 非结构化数据(unstructured data)：特征无法精确定义；比如图像的像素点，音频，文字 人类很擅长处理结构化的数据，但机器很不擅长。而归功于深度学习，使得机器在非结构化数据的处理有了明显的提高；但是现在比较挣 钱的仍然是让机器处理结构化数据，如广告投放、理解处理公司的海量数据并进行预测等。吴恩达希望设计的网络可以处理结构化数据也 可以处理非结构化的数据。
每个神经元类似一个乐高积木(Lego brick) ，将许多神经元堆叠在一起就形成了一个较大的神经网络。而且并不会人为决定每个神经元 的作用，而是由神经网络自己决定每个神经元的作用。如果给神经网络足够多的训练数据，其非常擅长计算从输入到输出的精确映射。神 经网络在监督学习中效果很好很强大。
神经网络有不同的种类，有用于处理图像的 CNN(Convolution Neural Network)、处理一维序列的 RNN(Recurrent Neural Network)、以 及自动驾驶中用于处理雷达数据的混合神经网络(Hybrid Neural Network)[对于复杂的问题，需要自行构建网络的架构；和机器学习中的 算法一样，针对具体的问题，需要去做具体的优化，而不是一成不变的使用基本的算法]
scale 使得神经网络在最近流行起来，这里的 scale 并不单单指神经网络的规模，还包括数据的规模。当训练样本不是很大的时候，神 经网络与传统的机器学习算法之间的优劣并不明显，此时主要取决有人为设计算法的技巧和能力以及算法处理的细节，可能一个设计良好 的 SVM 算法结果要优于一个神经网络的效果；但是随着样本量不断变大，传统的机器学习算法的性能会在达到一定的性能之后效果变无 法继续提升，而神经网络此时的效果将明显领先于传统的算法[需要很大的样本，且网络的规模越大，性能越好]。数据、计算能力、算法 都促使了深度学习的发展；算法的主要改进都在加快算法的速度，比如使用 ReLU 函数替代 sigmoid 函数就大大加快了算法的训练速度， 因为 sigmoid 函数在自变量趋向于正负无穷大的时候，导数趋向于 0，而使用梯度下降法，梯度的减小将使得参数的变化变得缓慢，从 而学习将变得缓慢；而 ReLU 函数右侧的斜率始终为 1，由于斜率不会逐渐趋向于 0，使得算法训练速度大大提高（ReLu: rectified linear unit ，修正线性单元；修正指的是取不小于 0 的值）。速度的提升使得我们可以训练大型的网络或者在一定的时间内完成网络 的训练。而且训练神经网络的过程一般是 idea - code - experiment - idea 不断循环，迭代的更快使得验证自己的想法更加快速得到 验证，将有机会取验证更多的想法，从而更有可能找到合适的结果。
1989 年 Robert Hecht-Nielsen 证明了万能逼近定理：对于任何闭区间的一个连续函数都可以用一个隐含层的 BP 网络来逼近（完成任 意m 维到 n 维的映射）。虽然如此，但是若要模拟复杂的函数可能需要特别特别多的隐层神经元，因此现代网络总是加大网络的深度， 以让每一层的函数尽量简单，而整个网络完成复杂的映射。</description>
    </item>
    
    <item>
      <title>Machine Learning</title>
      <link>https://kylestones.github.io/blog/machinelearning/machine-learning/</link>
      <pubDate>Fri, 22 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/machinelearning/machine-learning/</guid>
      <description>ML *机器学习* machine learning: Field of study that gives computers the ability to learn without being explicitly program.
机器学习并不是仅仅是若干算法的堆积，学会“十大算法”，熟练掌握具体算法算法的推导与编程实现，并不能让所有问题迎刃而解，因为 现实世界的问题千变万化。而应该像张无忌那样，忘记张三丰传授的太极剑法的具体招式，而只记住一些规则和套路，从而根据敌人的招 式去不断变化自己的招式，达到以不变应万变的效果。或者说用 Andrew Ng 的话，要成为一个 master carpenter （顶级木匠），可以 灵活使用工具来制造桌椅，只有手艺差的木匠才会抱怨工具不合适。因此必须把握算法背后的思想脉络，针对具体的任务特点，对现有套 路进行改造融通；要记住算法是 死 的，思想才是 活 的。
数据库提供数据管理技术，机器学习提供数据分析技术。
Supervised Learning 样本有标签的时候称为监督学习
Linear Regression 首先可以根据样本输入的维数来选择参数的个数（最终依据交叉验证的结果来选择模型和特征） \[ h_{\theta}(x) = \sum_{i=1}^{n} \theta_i x_i = \theta^{T}x \] 求取 \(\theta\) 的策略：在训练集上使得 \(h(x)\) 尽可能接近 y 。那么就涉及到距离的定义，距离通常定义为两者差的平方。因此 损失函数(cost function)定义为 \[ J(\theta) = \frac{1}{2} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)})^2 \]
认为数据服从高斯分布 \(y|x;\theta \thicksim (\mu, \sigma^2)\) ，即其前置概率估计是高斯分布。</description>
    </item>
    
  </channel>
</rss>
