<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Org Mode</title>
    <link>https://kylestones.github.io/</link>
    <description>Recent content on Org Mode</description>
    <image>
      <url>https://kylestones.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://kylestones.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 03 Jun 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://kylestones.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>广告</title>
      <link>https://kylestones.github.io/blog/emotion/advertisement/</link>
      <pubDate>Fri, 03 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/emotion/advertisement/</guid>
      <description>LiveRe 广告 许久许久没有整博客，都不知道这两年是怎么虚度的。打开自己再 github 上的博客，突然发现有广告，在页面底部，来必力评论的下面。虽然来比力是可以免费使用的，对应公司需要钱来支持运营。但是看到广告还是感觉非常的不爽， 那就删掉评论的配置了，况且也不会有人来我这垃圾博客来评论。。。</description>
    </item>
    
    <item>
      <title>Algorithm fourth-edition</title>
      <link>https://kylestones.github.io/blog/apue/algotithm4/</link>
      <pubDate>Sat, 04 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/apue/algotithm4/</guid>
      <description>算法 算数表达式求值 E.W.Dijkstra&amp;#39;s Two-Stack Algorithm for Expression Evaluation – 1960s
Push operands onto the operand stack Push operators onto the operator stack Ignore left parenthesis On encountering a right parenthesis, pop an operator, pop the requisite number of operands, and push onto the operand stack the result of applying that operator to those operands. After the final right parenthesis has been processed, there is one value on the stack, which is the value of the expression.</description>
    </item>
    
    <item>
      <title>点滴生活</title>
      <link>https://kylestones.github.io/blog/emotion/life-diary/</link>
      <pubDate>Sun, 08 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/emotion/life-diary/</guid>
      <description>针眼 在 &amp;lt;2020-02-28 Fri&amp;gt; 感觉左眼疼，发现上眼皮内有个小疙瘩，非常不舒服。于是用芦荟胶涂抹，慢慢发现好像小了点，感觉已经受控。但是眼皮内总是有一些粘稠的东西，以为是芦荟胶，没有在意。无奈发现下眼皮又长了个疙瘩，而且比上眼皮的还严重。疼痛无法忍受，于是网上查了一下，感觉是针眼，也叫麦粒肿。于是 35 元买了瓶左氧氟沙星，说明书上写着一天 3 滴，可适量增减，我感觉比较严重，一天似乎滴了十几次。后来发现小疙瘩上面总是会流出一些浑浊的液体，有点担心会通过眼睛传入大脑，受到感染。另外发现左脸肿了好大一块。于是请假远程办公，连续滴眼液两天，在家还用热毛巾热敷了两次，发现肿胀小了很多。前后一个星期多呢。
一个星期内基本啥也没做，csapp 也没有坚持看。。。</description>
    </item>
    
    <item>
      <title>深度学习新闻跟踪</title>
      <link>https://kylestones.github.io/blog/machinelearning/ai-news/</link>
      <pubDate>Sat, 07 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/machinelearning/ai-news/</guid>
      <description> 摄像头是天生的神经网络 还记得之前说冯诺依曼结构的计算机，将计算和存储分离，需要移动大量的数据，严重限制了计算的性能，忆阻器可以实现类神经元，同时完成计算和存储。今天看到一个有意思的研究，说摄像头直接完成神经网络的计算，摄像头 CMOS 有望成为新一代的 GPU 。维也纳大写的研究发表在 20200304 期 nature 上，运行速度非常快，运行速度受限于电路中电子的速度，真是不能再快了。
https://www.nature.com/articles/s41586-020-2038-x
TOADD </description>
    </item>
    
    <item>
      <title>AutoML</title>
      <link>https://kylestones.github.io/blog/machinelearning/automl/</link>
      <pubDate>Sat, 04 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/machinelearning/automl/</guid>
      <description>就像使用 CNN 提取特征来代替人工设计的特征，使用 AutoML 代替人设计网络的架构，应该是可以达到更好的效果。
前提 CNN 通常需要大量的时间来设计网络的架构。当然我们可以使用迁移学习，但是 只有针对数据集设计自己的网络才能达到最好的性能 。然而设计网络架构需要专业的技能，且具有很大的挑战（对于商业应用来说代价太大）。
NAS Neural Architecture Search 就是用于搜索最优网络架构的算法。提供了深度学习的一个新的研究方向。
定义候选 building blocks 使用一个 RNN 作为控制器，用于选择拼装 building blocks 在交叉验证集上，训练拼装好的网络，使其收敛 根据网络的准确率来更新 RNN 控制器（update with policy gradient），希望其能选择出更好的网络架构 In simple terms: have an algorithm grab diierent blocks and put those blocks together to make a network. Train and test out that network. Based on your results, adjust the blocks you used to make the network and how you put them together!</description>
    </item>
    
    <item>
      <title>The New King of Comedy</title>
      <link>https://kylestones.github.io/blog/emotion/the-new-king-of-comedy/</link>
      <pubDate>Fri, 19 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/emotion/the-new-king-of-comedy/</guid>
      <description>星爷的新喜剧之王，影评很差，不过我看苦了。
自己的认真 自己始终在努力演戏，
关键是想象力，你必须用心体会当时的情境 我知道机会难得，我愿意等。永远是多远呀？那宇宙毁灭之后呢？ 众人的嘲笑 她是啥子 被直接扔出来 你有病呀，有病就去看病，别在这里传染我 你还到我面前找骂，你是不是贱呀？做个反应要什么前因后果，回回都是你，是吧 你再说一遍，我怕我听错了 被王宝强踩脸 你不照镜子吗？你觉得你有机会吗？你没有。你不用等，永远都没有。永远就是从现在开始到宇宙毁灭。人类最大的灾难就是这种人， 又不行，又没有自知之明，还不死心，整天出来搞事情 这是命；这就是命，别跟命运做斗争了好吗？ – 致命打击 入围选角 王宝强告知被周星驰选角之后，依然发现好多人，好多人在形体动作、情绪掌控、才艺展示上面有很厉害的表现，远超自己。自己的才艺 表演却只是，我可以用手发声。
家人的不理解 由于做了十几年的临时演员，始终没有很好的发展。在父亲的生日上被赶出来。
男友的背叛 蠢人好，蠢人多快乐。 这不是骗，我是收费的。 你会成功的，你是最棒的，你再给我钱，我继续说，说多少次都可以。 自我放弃 没有，我不是演员，我醒了。
someone 的认可 你做的那个寒冰掌好有层次，我觉得很到位 我不知道她的名字，但我知道她是一个演员 自己错误 对于不幸被车撞的大爷的真身表情，在以自己的演员修养来分析，认为大爷多处“表演”欠缺。
观后感 我想做演员，每天都超级努力，十几年的坚持，然而每天得到的都是讽刺嘲笑。不过仍然每天告诉自己要坚持。整容失败后，被成功选角， 而且看到墙上的标语“奇迹在这里发生”，突然感觉自己好像熬出了头，但是随后得到的确是一顿暴打。然后继续忍受着别人的大骂。终于 面对直到宇宙毁灭也没有机会，这就是命，再加上男友的背叛，选择放弃。然后王宝强的鼓励，重燃希望之火，到北京参加选角，但是仍 然看到太多太多远比自己优秀的人。不过最后被成功选上，且一年后选为最佳女主角（感觉有点快呀）
星爷还是成功讲了一个励志故事。
最后挺喜欢一句台词：我当你是朋友，你居然想睡我！还有花絮中表演生气的女演员，真的看了好多遍。</description>
    </item>
    
    <item>
      <title>乱想</title>
      <link>https://kylestones.github.io/blog/machinelearning/guess/</link>
      <pubDate>Mon, 08 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/machinelearning/guess/</guid>
      <description>目标检测 loss 改进。类似人脸识别，通过改进 loss 来提高 map ； FocalLoss 也算是一种改进 微软的 Deformable-Convolutional 应该大有用处，现在功力没有很好的发挥出来 神经网络对细节很敏感，通过在图片上粘贴一些胶带就可以成功的误导网络。是不是说明网络没有很好的区分背景？或者说背景没有 达到随机的效果？将目标完全分割出来（没有背景）用于训练会怎样？感觉人类在观察事物的时候，第一步就是先对目标进行了分割 呀？ 网络参数剪枝 先训练一个大型的网络，然后裁剪成一个小一点的网络，其性能比直接训练的同等大小的网络效果要好很多。说明现在的网络训练方 法还是有很大的问题。 人类做梦是在进行无监督学习吗？ 神经网络的训练能否设计成监督训练和无监督训练相结合的方法？ AutoML 人工设计的特征远远没有网络自己提取的特征好，人工设计的网络结构也很有可能没有自己学习的结构效果好。每个人的大脑内的神经元 也应该是各不相同，我们只是被告知什么是猫、什么是鸟，而我们的神经元会自己学习怎样链接。
婚礼现场生成 一个人的一系列帧，可以插入视频某起始帧的某个地方 可以换脸，让自己喜欢的明星来参加自己的婚礼 边缘计算 软件硬化：高通 CMDA 、思科路由器、地平线
专业话，继续推动摩尔定律。由计算变化成 AI 低功耗 软件和硬件联合优化 特殊场景的特殊问题，一定要有场景。Google X 成立了 6-7 年，但只是一地鸡毛。
做东西一定要解决实际问题 。
对话系统：对话一定是要有目的，而不是仅仅为了对话。对话是要解决实际问题的。
亚马逊老总左贝斯：将理想和现实分开，
把握住十年中不变的东西。
最终由 big data 转变到 big computer 的
发展历程 余凯：历史都是先是 toC ，然后再是 toB 。
英伟达股票大幅增长，英伟达是 toC 。
创业 创业就是赌，如果是实实在在的放在桌子上东西，那根本不是创业，那是上班。
看长线，踏踏实实做下去，更可能钓大鱼。
看短期（最近 2-3 个月）和远期（目标），不用看半年或者 1-2 年，因为到时候，你的这段时间的思考都会废掉。</description>
    </item>
    
    <item>
      <title>fast.ai</title>
      <link>https://kylestones.github.io/blog/machinelearning/fast-ai/</link>
      <pubDate>Sun, 24 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/machinelearning/fast-ai/</guid>
      <description>调参 分阶段使用多个学习速率 越接近输入的层，在 fine-tune 的时候，参数需要调节的越小，越靠近输出层，参数需要调节的越大。因此在 fine-tune 的时候，不同 的层最好使用不同的学习速率。例如 1-2 层使用 0.001 ，3-4 层使用 0.01 ，5-6 层使用 0.1 等等，越远离输出层，逐渐让学习速率 缩小 10 倍。[ Once the last layers are producing good results, we implement differential learning rates to alter the lower layers as well. The lower layers want to be altered less, so it is good practice to set each learning rate to be 10 times lower than the last. ]
找到最优学习速率 学习速率几乎是训练神经网络时的最重要的超参。可以通过让学习速率从一个很小的值开始，然后每个 mini-batch 都以指数级增长，同 时记录每个学习速率所对应的 loss ，然后画出 loss 和学习速率的关系曲线图，找到学习速率最大，但 loss 仍然在下降的点，这个学 习速率就是最好的学习速率。[ Do a trial run and train the neural network using a low learning rate, but increase it exponentially with each batch.</description>
    </item>
    
    <item>
      <title>TODO</title>
      <link>https://kylestones.github.io/blog/machinelearning/todo/</link>
      <pubDate>Tue, 19 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/machinelearning/todo/</guid>
      <description> paper 阅读 基础论文：
Visualizing and Understanding Convolutional Networks Spatial Transformer Networks 目标检测：
SSD RetinaNet Deformable Convolutional Networks cornernet AutoML:
Exploring Randomly Wired Neural Networks for Image Recognition </description>
    </item>
    
    <item>
      <title>Faster R-CNN</title>
      <link>https://kylestones.github.io/blog/machinelearning/r-cnn/</link>
      <pubDate>Thu, 28 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/machinelearning/r-cnn/</guid>
      <description>预处理 减去 RGB 像素的均值（无论训练还是测试统一使用训练样本集的均值） Rescale 在图像的长边不超过阈值的情况下，将短边 resize 到指定值 def img_rescale(img, targetSize=600, maxSize=1000): w = img.width h = img.height minDim = min(w,h) maxDim = max(w,h) scale = targetSize / minDim if scale * maxDim &amp;gt; maxSize: scale = maxSize / maxDim img = rescale(img, scale) return img 为什么要这样 rescale 呢？这样仅仅只能让多数的图像的短边统一长度，长边的长度可能各不相同；另外一些图像长边都是最大值，但 是短边各不相同。这样做有什么意义呢？保持图像的横纵比？但是图像大小不一样，要怎样去训练呢？
RoI Pooling RPN 生成的 RoI 由 (r,c,h,w) 表示，其中 (r,c) 是左上角的坐标，(h,w) 是高和宽。
RoI max pooling works by dividing the h*w RoI window into an H*W grid of sub-windows of approximate size h/H * w/W and then max-pooling the values in each sub-window into the corresponding output grid cell.</description>
    </item>
    
    <item>
      <title>Fuck you cancer</title>
      <link>https://kylestones.github.io/blog/emotion/cancer/</link>
      <pubDate>Thu, 31 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/emotion/cancer/</guid>
      <description>我们从不去想死亡，总是认为那是很遥远的事情，殊不知它随时可能悄然而至。
书籍 你在死亡中探寻生命的意义， 你见证生前死亡呼吸化作死后的空气。 新人尚不可知，故旧早已逝去； 躯体有尽时，灵魂无绝期。 读者呀，趁生之欢愉，快与时间同行， 共赴永恒生命！ --福尔克.格莱维尔 《卡伊利卡》 《当呼吸化为空气》是美国一名外科医生保罗.卡拉尼什诊断出四期肺癌晚期后的作品。作者以医生和患者双重身份，反思医疗和人性， 记录自己的一生。书中有关于生命意义的深沉思索。如何生存，死亡是我们做好的老师。TED 上有其妻子的一个演讲《当死亡降临》
摘抄
遭遇氢弹 “医生马上就到” – 那么多年奋斗即将迎来的人生巅峰，都随着这句话消失了。 无论什么大病，都能完全改变一个病人以及全家人的生活。 重大疾病不是要改变人生，而是要将你的人生打的粉碎。感觉仿佛神技降临，强烈的光突然刺进眼睛，照射出真正重要的事情； 其 实更像有谁刚刚用炸弹炸毁了你一心一意前进的道路，现在必须绕道而行 。 不管我走到哪里，死亡的阴影都会模糊任何行动的意义。 我周围这些人，身上洋溢者自信与抱负的气息，他们的生命有着无线的可能性。他们的生活如同美妙的圣诞颂歌，而我却嵌入了“倒带”的 苦恼。 挣扎 我要逼迫自己，回归手术室。因为我必须学会以不同的方式活着。我会把死神看做威风凛凛、不是造访的贵客， 但心里要清楚，即 使我是一个将死之人，我仍然活着，直到真正死去的那一刻 。 作者认为达尔文和尼采有一个观点是一致的： 生物体最重要的特征就是奋斗求生 。没有奋斗的人生，就像一幅画里身上没有条纹 的老虎。最轻易的死亡有时候并非最好的结局。 我无法前行，我仍将前行。 I can&amp;#39;t go on, I&amp;#39;ll go on. – 塞缪斯.贝克特 人一旦遭遇顽疾，最需要小心的，是价值观的不断变化。你努力思考自己到底看重些什么。感觉就像信用卡被人拿走了，你不得不学 会讨价还价。 你必须要明白，自己最看重是什么？ 我现在走路倒是不需要拐杖了，但人生的前路仍然像瘫痪病人一样，充满不确定。 每天，我都从上次复发中恢复一些，但又距离下一次复发更近一些，当然，也离死亡更近一些。 癌症的残酷之处，不仅限制了你的时间，还限制了你的精力，极大地减少了你一天里能做的事情。 这样的绝症，对于一个想要理解死亡的年轻人，难道不是一份很好的礼物吗？还有什么比亲身体验更好的理解方法呢？ 经历悲痛的五个阶段：否认 - 愤怒 - 讨价还价 - 消沉 - 接受 化疗 - 所有人都静静地躺着，任凭注射管里那些毒副作用剧烈的药物慢慢输入伸展的手臂。 生-死 如此紧要关头，问题不仅仅局限于生存还是死亡，还有到底怎样的人生才值得一活。你愿意用失去说话的能力，来交换多几个月的生 命，默默无声的度过余生吗（也许你要替自己的母亲做这个决定）？你愿意冒着丧失视力的危险，来排除致命脑出血的哪怕一点点可 能吗？你愿意右手丧失行动能力，来停止抽搐吗？你到底要让孩子的神经承受多少痛苦，才能更愿意选择死亡呢？ 很多母亲明知胎死腹中，还是要经历分娩和生产。 听到严重的病情之后，有些人会马上坚强起来：“我们会抗争，打败这鬼东西的，大夫。” 抗争的武器各有不同，有的祈祷，有的砸钱， 有的求助草药，有的输入造血干细胞。这种坚强往往不堪一击， 不切实际的乐观往往下一秒就是排山倒海的绝望 。 关于死亡 人类的命运之于神明，正如“苍蝇之于顽童” – 《李尔王》 格罗斯特伯爵 古老的圣言已经灰飞烟灭，人类终于知道，自己是这冷酷无情的广阔雨中中孤独的存在，而自己在这宇宙中诞生，也是偶然的。 – 雅克.</description>
    </item>
    
    <item>
      <title>shell</title>
      <link>https://kylestones.github.io/blog/apue/shell/</link>
      <pubDate>Thu, 31 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/apue/shell/</guid>
      <description>ssh 隧道 top vmstat iostat ps tar grep sort # -u 重复关键字只保留第一个 # -k 知道排序字段（开始,结束）和类型，可通过 info sort 具体查看 # 2n,2 表示安装第二个字段，以数字顺序排序。默认安装字段顺序 # 1rn,1 表示按照数字逆序排序 sort -u -k 2n,2 file | sort -k 1rn,1 -k 3,3 sed sed 全名叫 stream editor，流编辑器（非交互）。sed 基本上就是玩正则模式 匹配。sed 比 awk 大 2-3 岁。
基本语法 $ sed options file # 并不修改文件，只将处理的结果打印出来 $ sed -e &amp;#39;s/hello/Hello/g&amp;#39; filename # 使用 -i 将直接修改文件 $ sed -i &amp;#39;s/^/# /g&amp;#39; filename # 同时处理多条语句 $ sed -e &amp;#39;2,5s/hello/Hello/; s/world/(&amp;amp;)/g&amp;#39; filename $ sed -e &amp;#39;1,3s/hello/Hello/3&amp;#39; -e &amp;#39;3,$s/world/World/3g&amp;#39; filename # 读取 sed 脚本处理文件 $ more rep.</description>
    </item>
    
    <item>
      <title>YOLO 实现细节</title>
      <link>https://kylestones.github.io/blog/machinelearning/yolo/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/machinelearning/yolo/</guid>
      <description>之前写的乱七八糟，现在开始重构，主要针对 YOLOv3。真的是得亲自动手实现，才能真正了解一个算法。The best way to go about learning object detection is to implement the algorithms by yourself, from scratch. – Ayoosh Kathuria
主网络 主网络采用 Darknet53 ，网络为全卷积网络 FCN(Fully Convolutional Networks)，之前一直以为全卷积网络就是不包含全连接的网络， 现在才知道，全卷积网络连 Pooling 层都没有，Pooling 层使用步长为 2 的卷积代替，防止由于 Pooling 导致 low-level features 的丢失；分类之前使用 Global Average Pooling 。[Darknet53 中有全连接层呀，上面写错了]
卷积层 Darknet 的每个卷积层都是由 Conv-BN-LReLU 组成，是网络的最小重复单元。
def cbl_gen(channels, kernel_size, strides, padding): &amp;#39;&amp;#39;&amp;#39;conv-BN-LeakyReLU cell&amp;#39;&amp;#39;&amp;#39; cbl_unit = nn.HybridSequential() # 所有卷积后面都有 BN ，所以 bias 始终为 False cbl_unit.add( nn.Conv2D(channels, kernel_size=kernel_size, strides=strides, padding=padding, groups=1, use_bias=False), nn.</description>
    </item>
    
    <item>
      <title>cuda 编程基础</title>
      <link>https://kylestones.github.io/blog/cuda/cuda-basic/</link>
      <pubDate>Sun, 30 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/cuda/cuda-basic/</guid>
      <description>学习周斌老师的《NVIDIA CUDA 初级教程视频》笔记
重看 CPU CPU 芯片中大量的晶体管用于 cache3 ，可以看到芯片中很大一部分面积都是 cache3 。主要由于 CPU 始终在大量的移动数据，将硬盘 中的数据移动到内存，将内存中的数据缓存到 cache ，将 cache 中的数据移动到寄存器，将某个寄存器移动到另一个寄存器，还有反向 的操作，将数据再写回硬盘，这些都是在大量的移动数据。CPU 主要是在为串行做优化，为了高速的数据移动，设计了内存管理单元、占 用很大芯片面积的 cache3 （或许是可以称 CPU 为吞吐机的原因），为了快速执行命令设计了各种分支预测、流水线、乱序执行等等。
整体架构 CPU 可以分成 数据通道 和 控制逻辑 两个部分。 Fetch 取址 -&amp;gt; Decode 译码 -&amp;gt; Execute 执行 -&amp;gt; Memory 访存 -&amp;gt; Writeback 写回 Pipeline 流水线是指令级并行 ILP 。可以极大的减小时钟周期，但同时也可能存在延迟（啥？），会增大芯片面积，指令流前后有关系时，无法 很好的利用流水线。流水线的长度称为级，并不是越大越好，通常在 14-20 级，不公开，涉及到芯片设计的商业秘密。
Bypassing 不等待前一条指令的所有流水线执行完成，只需要得到前一条指令结果的某一个数据，直接通过一个特殊的通道传递。
Branches 分支预测 Branch Prediction ，猜测下一条指令 ，基于过去分支记录，通常准确率大于 90% 分支断定 Branch Predication ，不使用分支预测，同时执行所有的分支。可减小面积、减小错误预测 IPC instructions per cycle ；超标量 superscalar ，增加流水线的宽度， N 条流水线。从而提高 IPC</description>
    </item>
    
    <item>
      <title>笑话集锦</title>
      <link>https://kylestones.github.io/blog/emotion/joke/</link>
      <pubDate>Sun, 23 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/emotion/joke/</guid>
      <description> 孩子学习不好，被妈妈痛骂，挨骂后，儿子用哀怨的眼神看着爸爸说：你为什么要娶她？爸爸也用哀怨的眼神说：还不是因为你！ 葵花点赞手! 脑子是个好东西，希望你能有。 </description>
    </item>
    
    <item>
      <title>convolution</title>
      <link>https://kylestones.github.io/blog/machinelearning/convolution/</link>
      <pubDate>Sun, 16 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/machinelearning/convolution/</guid>
      <description>cs231n 斯坦福大学的课程 CS231n: Convolutional Neural Networks for Visual Recognition 。发现写的很好，后悔没有早点看。可惜只看了 卷积这一部分。有中文翻译的部分可以只看中文总结，后面的英文原文更方便理解。
卷积 卷积神经网络明确假设输入是 images ，根据这个假设可以大量的减少参数的个数。同时有利于更 efficient 的实现。普通的神经网 络采用全连接，用于图像中会产生太多的参数，导致 overfitting 。卷积神经网络的 layers 拥有神经元的维数为 (width,height,channels) 。
A ConvNet is made up of Layers. Every Layer has a simple API: It transforms an input 3D volume to an output 3D volume with some differentiable function that may or may not have parameters.
卷积操作的本质就是滤波器和输入的部分区域做点积。卷积的反向传播也是卷积，只是做了转置。Note that the convolution operation essentially performs dot products between the filters and local regions of the input.</description>
    </item>
    
    <item>
      <title>ubuntu18.04 install gpu mxnet</title>
      <link>https://kylestones.github.io/blog/machinelearning/gpu-mxnet-install/</link>
      <pubDate>Fri, 14 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/machinelearning/gpu-mxnet-install/</guid>
      <description>安装 Ubuntu18.04 从 中科大源 下载系统镜像，并制作 u 盘启动盘 # 使用 wget 下载 Ubuntu18.04 $ wget http://mirrors.aliyun.com/ubuntu-releases/bionic/ubuntu-18.04.1-desktop-amd64.iso # fdisk 查看 u 盘分区 $ sudo fdisk -l # 使用 dd 命令制作 u 盘启动镜像 $ dd if=ubuntu18.04.1-desktop-amd64.iso of=/dev/sdb 启动电脑，修改为 U 盘启动 关闭 uefi 安全检查，有秘钥的通通删掉 使用 U 盘启动后，选择安装 Ubuntu 手动分区，建议只有两个分区，一个根分区，一个 home 分区；这样在需要重装系统的时候可以格式化根分区，同时保留 home 分区 的内容 安装完成后，修改 root 的密码，从而可以使用 root 登录系统 安装 NVIDIA 驱动 硬件配置 GPU GTX2070 由于 GPU 较新，需要添加 PPA；如非必要可以直接使用 Ubuntu repository 查看驱动列表 安装驱动 重启使驱动生效 # Ubuntu repository 的驱动虽然比较旧，但是更加稳定，bug 较少； # 如果想要安装最先的版本，可以添加由 Ubuntu 团队维护的 PPA # PPA 仍然在测试，可能存在某些依赖问题 # 不需要手动更新，Ubuntu18.</description>
    </item>
    
    <item>
      <title>Darknet</title>
      <link>https://kylestones.github.io/blog/machinelearning/darknet/</link>
      <pubDate>Fri, 07 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/machinelearning/darknet/</guid>
      <description> darknet 以 layer 为中心，通过构建不同的 layer 构成一个 net ； 所有的层保存在 net.layers 中 net-&amp;gt;layers = calloc(net-&amp;gt;n, sizeof(layer)); darknet 需要一个“.txt”文件，每行表示一张图像的信息： &amp;lt;object-class&amp;gt; &amp;lt;x&amp;gt; &amp;lt;y&amp;gt; &amp;lt;width&amp;gt; &amp;lt;height&amp;gt; 每个单元格会预测B个边界框（bounding box）以及边界框的置信度（confidence score）。 所谓置信度其实包含两个方面，一是这个边界框含有目标的可能性大小，二是这个边界框的准确度。前者记为 Pr(object)，后者记为 预测框与实际框（ground truth）的 IOU 。很多人可能将 Yolo 的置信度看成边界框是否含有目标的概率，但是其实它是两个因子的 乘积，预测框的准确度也反映在里面。 中心坐标的预测值 (x,y) 是相对于每个单元格左上角坐标点的偏移值，并且单位是相对于单元格大小的。而边界框的 w 和 h 预测值 是相对于整个图片的宽与高的比例，这样理论上4个元素的大小应该在 [0,1] 范围。通过 sigmoid 函数保证 (x,y) 在 0-1 之间， 这样，每个边界框的预测值实际上包含5个元素：(x,y,w,h,c)，其中前4个表征边界框的大小与位置，而最后一个值是置信度。 加速库： NNPACK </description>
    </item>
    
    <item>
      <title>Positive Psychology</title>
      <link>https://kylestones.github.io/blog/emotion/positive-psychology/</link>
      <pubDate>Sun, 02 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/emotion/positive-psychology/</guid>
      <description>为什么需要积极心理学 需要集中研究，带来更大的收货 并不是治愈了抑郁，就会快乐 积极心理学可以有效预防压抑等 Question very often create reality 测试实验：30 s 观察一幅画，问这幅画中有多少个几何图形？那么在这 30 S 内，测试者会进全力去数这幅画的几何图形的个数，但是 由于这幅画里有太多太多的几何图形，被测试者在有限的 30s 内根本没有办法数过来。而再给这些测试者提问。图片上的大巴里面有几 个孩子？左下角的钟表的时间是几点？右侧的颜色？几乎没有几个人知道。
这种注意力可以帮助我们投入到一件事情上，但是如果只是给自己提问这一个问题，那么我们忽略其他。
生活中，大多数人会问自己什么问题。我的缺点是什么，有什么不足？这没有什么不对，但是，如果我们只是给自己提问这一个问题，我 们会忽略掉我们的优点，我们将只能看到自己的缺点。again，question create reality.
Stavros and Torres 一本关于婚姻的书中说：We see what we look for and we miss much of what we are not looking for even though it is there. Our experience of the world is heavily influenced by where we place our attention.
冥想 专注一件事情：动作、姿势、呼吸、同情心、爱、善心、火焰等等都可以 基础：深呼吸 deep breathing ，腹部呼吸 锻炼：集中-分心-集中-分心… 从而更留心专心 冥想时没有目的。</description>
    </item>
    
    <item>
      <title>文学待修养</title>
      <link>https://kylestones.github.io/blog/book/literature/</link>
      <pubDate>Sun, 02 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/book/literature/</guid>
      <description>文化水不平 大家 俄国大师: 普希金、屠格涅夫、果戈里《死魂灵》、契诃夫（三大伟大短片小说家之一）、莱蒙托夫、托尔斯泰、陀思妥耶夫斯基 米兰.昆德拉 《不能承受的生命之轻》 捷克作家 肖斯塔科维奇 《肖七》 （请在我们脏的时候爱我们） 马克.罗斯柯 《红色中的赭色和红色》 《绿色和粟色》、马克.夏加尔 《我与村庄》《生日》《七个手指头的自画像》、亨利.马斯 蒂、 《莫斯科不相信眼泪》、《两个人的车站》 美国文学奠基者： 马克吐温、杰克.伦敦 《野性的呼唤》 《热爱生命》 《黄祸》 《史无前例的入侵》 、海明威 中国伤痕作家： 苏童、余华、莫言、王朔、张贤亮 崖山之后再无中国 ； 昭武九姓 80 年代，大师的年代 亚里士多德、孔子、释迦牟尼几乎处于同一个年代 李约瑟难题 西汉： 绿林起义，绿林军 贝尼托.墨索里尼 四大博物馆 法国 巴黎 卢浮宫 英国 伦敦 大英博物馆 俄罗斯 圣彼得堡 埃尔米塔什博物馆 美国 纽约 大都会博物馆 保罗.高更、巴勃罗.毕加索、达.芬奇、伦勃朗.哈尔曼松.凡.莱茵
日本 日本人 = 机器人 非常非常有纪律性 服务员早上笑容灿烂，晚上依然不会有一点折扣 所有的细节都会考虑的很周到 英语不好 95% 高中、45% 本科 有全球 20% 的活火山；人口多，耕地少，较大比例的 6 级以上地震 黑泽明 《罗生门》 《乱》 坂本龙一、 久石让 小野丽莎、山口百惠 自由，忘记仇恨的民族 影视 导演：科恩兄弟、伍迪艾伦、斯皮尔伯格 鲍勃.</description>
    </item>
    
    <item>
      <title>Mask R-CNN</title>
      <link>https://kylestones.github.io/blog/machinelearning/maskrcnn/</link>
      <pubDate>Tue, 27 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/machinelearning/maskrcnn/</guid>
      <description>训练 train 输入 mini-batch 论文指出每个 GPU 上面跑 2 张图片，共 8 个 GPU ，所以 batch-size = 16 TODO ；每张图片上 sample 256 个 anchor 用于训练。
image spatial 图片大小
到底是用多大尺寸的图片训练网络的？ paper 中将图像的短边 resize 为 800 （测试的时候也会 resize 吗？）; 800*1024 ? 800*800 ? 1024*1024 ? 任意 size ? TODO
anchor anchor 用于生产目标的最初候选区域
生成 anchor 的方法
anchor scale [32, 64, 128, 256, 512] anchor ratio [0.5, 1, 2] 在 RPN 网络最后一层的 feature map 上每个点生成一组 anchor （按照指定的 scale 和 ratio） 判断为 positive anchor 的标准：</description>
    </item>
    
    <item>
      <title>mAP</title>
      <link>https://kylestones.github.io/blog/machinelearning/map/</link>
      <pubDate>Sat, 24 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/machinelearning/map/</guid>
      <description>词语解释 mAP 是 Mean Average Precision 的缩写，是检测算法的评价指标。这个名词中包含两个平均，其中 Mean 取的是不同类别的平均值， Average 取的是不同的召回率的平均值，当然计算的是正确率 Precision 的平均值。另外 coco 数据集还取了不同 IoU 阈值的平均。
另外想说一下，recall 这个单词被国内广泛翻译称召回率，大多数人根本无法从字面理解这个召回率到底是个什么鬼。而周志华老师在 其西瓜书中，将 recall 翻译成查全率，将 precision 翻译成查准率。一下子直观了很多。查准率表示模型查找到结果的准确率，就是 你说这些都是好瓜，但其中真正是好瓜的比例；查全率表示模型找到所有正样本的比例，就是说在所有好瓜中，你判别出来了多少。
计算方法 不同的数据集有不同的 mAP 计算方法。主要包括 PASCAL 07 、PASCAL 10、COCO 数据集的计算方法较常用。
PASCAL 数据集使用的都是固定的 IoU 阈值（默认为 0.5），就是只要预测的 box 和真实的 box 的 IoU 大于等于 0.5 ，就认为检测正 确（当然类别也必须正确）。所不同的是 PASCAL 07 只计算 11 个查准率 precision 的平均值，而 PASCAL 10 则要求所有的检测结果 都用于计算 AP 。
PASCAL 07 只计算查全率 recall 在 0.1-1 之间，以 0.01 为间隔，共 11 个点所对应的查准率的平均值。而 PASCAL 10 则在计算 PR 曲线的 AUC 。coco 数据集则会分别计算 IoU 为 0.</description>
    </item>
    
    <item>
      <title>mxnet</title>
      <link>https://kylestones.github.io/blog/machinelearning/gluon/</link>
      <pubDate>Wed, 31 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/machinelearning/gluon/</guid>
      <description>mxnet 常用包 from mxnet import gluon # 提供简单易用的 mxnet 接口 from mxnet import nd from mxnet import init # 用于权重参数初始化 from mxnet import autograd # 自动求导 from mxnet.gluon import nn # 用于构建网络结构 from mxnet.gluon import data as gdata from mxnet.gluon.data.vision import transforms # 用于变换数据 import sys import time 常用函数 transfroms.Compose 实现数据格式的转换，转换成 (height, width, channel) 格式，以及变成浮点数；这是 mxnet 要求的格式； 同时可以实现 argument gluon.data.DataLoader 读取数据，可以实现随机读取随机的 batch ，指定 batch_size ，指定同时读取数据的线程数；返回值可 迭代的对象，分别为数据和标签对 gluon.loss 常用的标准损失函数 gluon.Trainer 设定学习算法（SGD、Adam 等），设置学习速率，等等训练参数 x.</description>
    </item>
    
    <item>
      <title>python</title>
      <link>https://kylestones.github.io/blog/machinelearning/python/</link>
      <pubDate>Wed, 31 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/machinelearning/python/</guid>
      <description>Python 常识
调试 logging logging.debug ，不要再胡乱使用 print 了
use pdb 在Python 文件内 import pdb ，然后在需要调试的开始行添加 pdb.set_trace() shell 中 python -m pdb test.py pdb.run() 使用 args 查看入参，没有查看局部变量的方法
axis axis = 0 表示第一维数据，axis = -1 表示最后一维数据；就是 np.array(.).shape 输出的顺序
数组索引 Python 可以使用数组作为数组的索引 # 生成数组 # 切片可以指定 start:stop:step , a[::-1] 可以将数组逆序 &amp;gt;&amp;gt;&amp;gt; import numpy as np &amp;gt;&amp;gt;&amp;gt; np.arange(5) array([0, 1, 2, 3, 4]) # 增加一个维度 &amp;gt;&amp;gt;&amp;gt; a array([[0, 1], [0, 1]]) # 将 -1 维度的每一个值都变成一个新的数组（每个数组中只包含一个值） &amp;gt;&amp;gt;&amp;gt; a[:,:,np.</description>
    </item>
    
    <item>
      <title>something</title>
      <link>https://kylestones.github.io/blog/machinelearning/summarize/</link>
      <pubDate>Wed, 31 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/machinelearning/summarize/</guid>
      <description>https://pan.baidu.com/s/1hs3Z2ao https://www.aliyun.com/zixun/wenji/1283158.html https://www.cnblogs.com/objectDetect/p/5947169.html https://www.zhihu.com/question/43609045/answer/132235276 https://blog.csdn.net/shentanyue/article/details/82109580?utm_source=blogxgwz1 https://blog.csdn.net/qq_33783896/article/details/80675398
网络架构 解释resnet、优缺点以及适用范围 解释inception net、优缺点以及适用范围 densenet结构优缺点以及应用场景 dilated conv优缺点以及应用场景 moblenet、shufflenet的结构 卷积核参数 一个标准卷积层参数的个数是 input*ksize*ksize*output ，不知道为什么自己会两次犯了相同错却仍然不自知。
如果卷积分组 group ，那么每个卷积核的大小将变小，变为 imput/group*ksize*ksize ，一个 group 的参数为 input/group*ksize*ksize*output/group ；所有 group 组成整个卷积层的参数，input/group*ksize*ksize*output/group*group = input/group*ksize*ksize*output 。即参数会变为原来的 1/group 倍。
deepwith conv 对输入的每一个 channel 分别进行独立的卷积操作，此时输入的 channel 个数必然等于输出 channel 的个数，此卷积 层参数的个数为 ksize*ksize*input。
感受野 vgg 论文上说明两次 3x3 卷积可以达到 5x5 卷积核的效果、三层 3x3 卷积可以达到 7x7 卷积核的效果； 现在终于理解作者的意思， 使用两次 3x3 卷积，第二层卷积核的感受野就是 5x5 ，而三层 3x3 卷积，第三层卷积的感受野为 7x7 。
感受野计算公式：
Dilated conv vs Deconvolution Dilated convolution 在卷积核的每两个值中间插入 d-1 个空洞 Deconvolution 在 feature map 上插入像素值为 0 的点 Unlike dilated convolutions, which have same output size as input size (if input borders are properly padded) &amp;#34;deconvolution&amp;#34; layers actually produce upsampling (larger input then output) 数学公式：</description>
    </item>
    
    <item>
      <title>目标检测</title>
      <link>https://kylestones.github.io/blog/machinelearning/objectdetection/</link>
      <pubDate>Fri, 24 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/machinelearning/objectdetection/</guid>
      <description>YOLO 非常敬佩作者 Joseph Redmon ，没有使用开源框架，而是自己使用 C 和 cuda 另外写了一套框架 DarkNet ，并且将其开源。而且 license 写的非常有意思，有点狂放不羁，可能这就是大牛该有的样子
Darknet is public domain. Do whatever you want with it. Stop emailing me about it! YOLO 是 You Only Look Once 的简写。当然 YOLO 很可能让人想起另一句话 You only live once, but if you do it right, once is enough. – Mae West ，个人感觉作者可能有点故意让两者混淆。
区别于 RNN 系列的文章，需要先查找 region proposals ，然后在之上进行目标检测。YOLO 只需要运行一遍卷积神经网络就可以完成目 标检测，所以其最主要的优点就是 速度 。而且 mAP (mean Average Precision) 随着算法的改进也表现的相当不错。</description>
    </item>
    
    <item>
      <title>见招拆招</title>
      <link>https://kylestones.github.io/blog/machinelearning/seethemove/</link>
      <pubDate>Thu, 23 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/machinelearning/seethemove/</guid>
      <description>机器学习并不是若干算法的堆积，熟练掌握了“十大算法”或“二十大算法”并不能让一切问题迎刃而解。所以不能将目光仅聚焦在具体算法 的推导和编程实现上。基本算法仅能呈现典型“套路”，而现实世界任务千变万化，以有限的套路应对无限的变化，焉有不败！所以务必掌 握算法背后的思想脉络，面对现实问题时，根据任务特点对现有套路进行改造融通。无论科研创新还是应用实践，皆以此为登堂入室之始。 — 周志华 &amp;lt;机器学习&amp;gt;
还有张三丰传授太极剑法给张无忌时，张无忌忘记了剑法具体的招式，仅记住了太极剑法的指导思想，从而根据敌人的招式使用相应的制 敌之策，达到见招拆招的目的。
Easy to learn. Hard to master.
YOLO 下面举 YOLO 算法中的几个的例子
作者通过均方无法来计算预测的 binding boxes 和 ground truth 之间的误差，由于 loss function 中还包含分类错误的误差。而 由于大多数的 grid cell 都没有目标，所以不应该让有目标和没有目标的 grid cell 产生相同权重的误差，所以作者让目标位置误 差的权重 \(\lambda_{coord} = 5\) ，让没有目标的分类误差权重 \(\lambda_{noobj} = 0.5\) ，从而来平衡由于数量悬殊导致的 问题。 使用均方误差计算，size 比较大的目标相比于 size 比较小的目标产生更大的误差。所以作者使用开方之后的宽度和高度值相减然后 求平方。 作者使用 224x224 的图像在 ImageNet 上使用分类网络对检测网络进行预训练，同时作者想让检测网络输入的分辨率为 448x448 。 由于需要同时改变输入的尺寸以及网络的任务，作者先使用 448x448 的ImageNet 对网络进行 fine-tune，然后在使用检测误差进行 调优，以达到更好的效果。 作者想要使用 Anchor Boxes ，但是 R-CNN 一般都是人为设定其大小，这个是目标的先验，如果能有更好的先验，那么应该会有更好 的检测结果，所以作者使用 k-means 聚类方法来求取 Anchor Boxes 先验的大小。 k-means 算法一般使用欧式距离度量误差，而此处，作者真实关心的是两者的 IOU ，所以作者用 1 - IOU 作为损失函数。 可以发现作者始终在依据自己的实际需求，对算法进行了一些改进，而不是直接生搬硬套 。</description>
    </item>
    
    <item>
      <title>VGG GoogLeNet ResNet</title>
      <link>https://kylestones.github.io/blog/machinelearning/vgg-googlenet-resnet/</link>
      <pubDate>Wed, 22 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/machinelearning/vgg-googlenet-resnet/</guid>
      <description>VGG VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION 论文表明增加网络的深度可能提高网络的性能，论文成功将网络的深度推到 16-19 层 使用 small size (3x3) filter ：两层 3x3 的卷积与 5x5 的卷积等效，三层 3x3 的卷积与 7x7 的卷积等效；选用小尺寸的 filter 可以减小参数 第一层卷积的滤波器的个数是 64 ，之后每经过一个 max-pooling 层都将滤波器的个数乘 2，直到最大为 512 之后不再继续翻倍 Not all the conv layer are followed by max-pooling. 论文为了训练 19 层深层网络，先构建了一些浅层的网络，用于预训练，然后逐渐使用预训练好的浅层网络参数对深层的网络进行初始化。 不过文章中作者也指出，写完 paper 后发现，使用随机初始化是不需要预训练的。看来大神们发 paper 也是历经坎坷呀。
GoogLeNet 强调算法的重要性，比硬件、大的数据量更加重要。Most of the progress is not just the result of more powerful hardware, larger dataset and bigger models, but mainly a consequence of new ideas, algorithms and improved network architechtures.</description>
    </item>
    
    <item>
      <title>卷积神经网络进化</title>
      <link>https://kylestones.github.io/blog/machinelearning/revolution/</link>
      <pubDate>Wed, 22 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/machinelearning/revolution/</guid>
      <description>总体趋势：选取的函数越来越简单，手工设计的部分越来越少
CNN Yann Lecun 在 1998 的 LeNet 奠定了神经网络的基本架构 : CONV - POLING - FC 。
激活函数 在经典的神经网络以及 LeNet 中使用的激活函数都是 sigmoid 函数。sigmoid 函数是非线性函数，且在输入较大或者较小的时候斜率会 变得很小，不利于参数的学习。
从 AlexNet 开始，激活函数变成了 ReLU ，为分段线性，且 non-saturating，大大加快了网络的训练速度。同时为防止过拟合，提出了 Dropout 方法， Dropout 随机的使网络中的一些节点失活，使得节点不能过度依赖某一个输入，从而权重得以分散开来，另外使用随机 失活的网络，有预训练的效果，类似于先训练一个简单的网络，然后在没有失活的大型网络上 fine-tune 。虽然 AlexNet 网络与 LeNet 的架构基本相同，但由于其 ReLU 和 Dropout 等方法的使用，网络使用了 120 万张训练图片，从数据中学到了更本质的特征，将 cumulative match character (CMC) top5的正确率一下子提升了 10% ，成功掀起了深度学习的研究热潮。
何凯明大神在一次报告中使用 RevoLUtion 来表示 ReLU 对深度学习的贡献，同时使用红色字体高亮了单词中的 ReLU ，非常形象。
Network in NetWork 除了 mini-batch size 外，网络的一层的输入维度为 height * width * channels ，可以通过 polling 操作来减小 height 和 width ，但是怎样减少 channel 的个数呢？ 1 * 1 卷积可以大显身手。当然，如果你愿意也可以用来增加 channel 的个数。</description>
    </item>
    
    <item>
      <title>人脸识别</title>
      <link>https://kylestones.github.io/blog/machinelearning/facerecognition/</link>
      <pubDate>Fri, 17 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/machinelearning/facerecognition/</guid>
      <description>FaceNet A Unified Embedding for Face Recognition and Clustering
原来使用卷积神经网络来提取人脸的特征通常都是使用 softmax-loss 来训练网络，以期望网络的到的 embedding 足够好。本文作者直 接使用 embedding 的误差来训练网络，然后通过计算 embedding 的欧式距离来实现人脸验证。
triplet loss 每次使用三张图像，一个是 anchor ，另外两张中一张图像与 anchor 是同一个人，另一张是不同的人。
\begin{align*} L = ∑_i^N ≤ft [||f(x_i^a) - f(x_i^p)||_2^2 - ||f(x_i^a) - f(x_i^n)||_2^2 + α \right ]_+ \end{align*}
Triplet Selection 为了较好的训练效果，挑选hard-positive 和 hard-negative 的人脸对，就是同一个人时选择两张差别最大的图像，不同人脸的时候， 挑选差别最小的两张图像。其中 \(\alpha\) 是 margin 。当然所有这些选择都是在一个 mini-batch 中，而不是整个训练样本中。
另外为了防止网络进入局部最优解或者训练崩溃（如 f(x) = 0），选择 semi-hard negative 样本，即满足 \[ ||f(x_i^a) - f(x_i^p)||_2^2 - ||f(x_i^a) - f(x_i^n)||_2^2 \]</description>
    </item>
    
    <item>
      <title>损失函数</title>
      <link>https://kylestones.github.io/blog/machinelearning/loss-function/</link>
      <pubDate>Fri, 17 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/machinelearning/loss-function/</guid>
      <description>为了度量算法关于某个数据集的性能，我们需要损失函数。当算法希望生成比真实值一个较小的数字，那么损失函数中应该体现出现，较 大的输出比较小的输出有更大的惩罚。
Loss: Used to evaluate and diagnose model optimization only. Metric: Used to evaluate and choose models in the context of the project. Mean Squared Error MSE 是经常被使用的损失函数，易于理解，且表现很好。
take the difference between your predictions and the ground truth square it average it out across the whole dataset def MSE(y_predicted, y): squared_error = (y_predicted, y) ** 2 sum_squared_error = np.sum(squared_error) mse = sum_squared_error / y.size return mse Cross Entropy Loss (Log Loss) 交叉熵损失经常用于分类问题。函数定义如下</description>
    </item>
    
    <item>
      <title>TensorFlow</title>
      <link>https://kylestones.github.io/blog/machinelearning/tensorflow/</link>
      <pubDate>Sun, 12 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/machinelearning/tensorflow/</guid>
      <description>架构 阅读大神的 《TensorFlow 内核剖析》 对 TensorFlow 的整个代码框架有了一些了解，以下是读书笔记。
Graph (计算图)是 TensorFlow 领域模型的核心。计算图就是节点与边的集合，是一个 DAG (有向无环图)图。
Node(节点)持有零条或多条输入/输出的边，分别使用 in_edges， out_edges 表示。
Edge(边) 持有前驱节点与后驱节点，从而实现了计算图的连接，也是计算图前向遍历，后向遍历的衔接点。边上的数据以 Tensor 的形 式传递。计算图中存在两类边：
普通边：用于承载 Tensor，常用实线表示； 控制依赖：控制节点的执行顺序，常用虚线表示。 TensorFlow 计算的单位是 OP，它表示了某种抽象计算。通过定义 OP 来构建 DAG 图。OP 拥有 0 个或多个「输入/输出」，及其 0 个 或多个「属性」。其中，输入/输出以Tensor 的形式存在。在系统实现中，OP 的元数据使用 Protobuf 格式的 OpDef 描述，实现前端与 后端的数据交换，及其领域模型的统一。OpDef 定义包括 OP 的名字，输入输出列表，属性列表，优化选项等。其中，属性常常用于描述 输入/输出的类型，大小，默认值，约束，及OP 的其他特性。
计算图的执行过程将按照 DAG 的拓扑排序，依次启动 OP 的运算。其中，如果存在多个入度为 0 的节点，TensorFlow 运行时可以实现 并发，同时执行多个 OP 的运算，提高执行效率。
架构设计 TensorFlow 遵循良好的分层架构：
front end ： 用户接口，负责构造计算图 runtime ： 实现计算图的拆分。提供本地运行模式和分布式运行模式，两者共享大部分设计和实现 计算层 ： 基于 Eigen 实现计算的逻辑实现；同时支持各种硬件的并行加速 通信层 ： 基于 gRPC 实现组件间的数据交换。同时支持 RDMA 设备层 ： 支持多种异构计算设备。实际执行计算的载体 前端系统 Client 是前端系统的主要组成部分，它是一个支持多语言的编程环境，且对 Python 和 C++ 的支持比较完善。实现时通过 Swig 完成对 后端 C++ 的调用。基于这些编程接口来构造计算图。</description>
    </item>
    
    <item>
      <title>Deep Learning</title>
      <link>https://kylestones.github.io/blog/machinelearning/deeplearning/</link>
      <pubDate>Thu, 12 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/machinelearning/deeplearning/</guid>
      <description>神经网络和深度学习 神经网络概论 结构化数据(structured data)：每个特征都有清晰、明确有意义的定义；比如房屋的面积，人的身高等 非结构化数据(unstructured data)：特征无法精确定义；比如图像的像素点，音频，文字 人类很擅长处理结构化的数据，但机器很不擅长。而归功于深度学习，使得机器在非结构化数据的处理有了明显的提高；但是现在比较挣 钱的仍然是让机器处理结构化数据，如广告投放、理解处理公司的海量数据并进行预测等。吴恩达希望设计的网络可以处理结构化数据也 可以处理非结构化的数据。
每个神经元类似一个乐高积木(Lego brick) ，将许多神经元堆叠在一起就形成了一个较大的神经网络。而且并不会人为决定每个神经元 的作用，而是由神经网络自己决定每个神经元的作用。如果给神经网络足够多的训练数据，其非常擅长计算从输入到输出的精确映射。神 经网络在监督学习中效果很好很强大。
神经网络有不同的种类，有用于处理图像的 CNN(Convolution Neural Network)、处理一维序列的 RNN(Recurrent Neural Network)、以 及自动驾驶中用于处理雷达数据的混合神经网络(Hybrid Neural Network)[对于复杂的问题，需要自行构建网络的架构；和机器学习中的 算法一样，针对具体的问题，需要去做具体的优化，而不是一成不变的使用基本的算法]
scale 使得神经网络在最近流行起来，这里的 scale 并不单单指神经网络的规模，还包括数据的规模。当训练样本不是很大的时候，神 经网络与传统的机器学习算法之间的优劣并不明显，此时主要取决有人为设计算法的技巧和能力以及算法处理的细节，可能一个设计良好 的 SVM 算法结果要优于一个神经网络的效果；但是随着样本量不断变大，传统的机器学习算法的性能会在达到一定的性能之后效果变无 法继续提升，而神经网络此时的效果将明显领先于传统的算法[需要很大的样本，且网络的规模越大，性能越好]。数据、计算能力、算法 都促使了深度学习的发展；算法的主要改进都在加快算法的速度，比如使用 ReLU 函数替代 sigmoid 函数就大大加快了算法的训练速度， 因为 sigmoid 函数在自变量趋向于正负无穷大的时候，导数趋向于 0，而使用梯度下降法，梯度的减小将使得参数的变化变得缓慢，从 而学习将变得缓慢；而 ReLU 函数右侧的斜率始终为 1，由于斜率不会逐渐趋向于 0，使得算法训练速度大大提高（ReLu: rectified linear unit ，修正线性单元；修正指的是取不小于 0 的值）。速度的提升使得我们可以训练大型的网络或者在一定的时间内完成网络 的训练。而且训练神经网络的过程一般是 idea - code - experiment - idea 不断循环，迭代的更快使得验证自己的想法更加快速得到 验证，将有机会取验证更多的想法，从而更有可能找到合适的结果。
1989 年 Robert Hecht-Nielsen 证明了万能逼近定理：对于任何闭区间的一个连续函数都可以用一个隐含层的 BP 网络来逼近（完成任 意m 维到 n 维的映射）。虽然如此，但是若要模拟复杂的函数可能需要特别特别多的隐层神经元，因此现代网络总是加大网络的深度， 以让每一层的函数尽量简单，而整个网络完成复杂的映射。</description>
    </item>
    
    <item>
      <title>Machine Learning</title>
      <link>https://kylestones.github.io/blog/machinelearning/machine-learning/</link>
      <pubDate>Fri, 22 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/machinelearning/machine-learning/</guid>
      <description>ML *机器学习* machine learning: Field of study that gives computers the ability to learn without being explicitly program.
机器学习并不是仅仅是若干算法的堆积，学会“十大算法”，熟练掌握具体算法算法的推导与编程实现，并不能让所有问题迎刃而解，因为 现实世界的问题千变万化。而应该像张无忌那样，忘记张三丰传授的太极剑法的具体招式，而只记住一些规则和套路，从而根据敌人的招 式去不断变化自己的招式，达到以不变应万变的效果。或者说用 Andrew Ng 的话，要成为一个 master carpenter （顶级木匠），可以 灵活使用工具来制造桌椅，只有手艺差的木匠才会抱怨工具不合适。因此必须把握算法背后的思想脉络，针对具体的任务特点，对现有套 路进行改造融通；要记住算法是 死 的，思想才是 活 的。
数据库提供数据管理技术，机器学习提供数据分析技术。
Supervised Learning 样本有标签的时候称为监督学习
Linear Regression 首先可以根据样本输入的维数来选择参数的个数（最终依据交叉验证的结果来选择模型和特征） \[ h_{\theta}(x) = \sum_{i=1}^{n} \theta_i x_i = \theta^{T}x \] 求取 \(\theta\) 的策略：在训练集上使得 \(h(x)\) 尽可能接近 y 。那么就涉及到距离的定义，距离通常定义为两者差的平方。因此 损失函数(cost function)定义为 \[ J(\theta) = \frac{1}{2} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)})^2 \]
认为数据服从高斯分布 \(y|x;\theta \thicksim (\mu, \sigma^2)\) ，即其前置概率估计是高斯分布。</description>
    </item>
    
    <item>
      <title>关于</title>
      <link>https://kylestones.github.io/about/</link>
      <pubDate>Thu, 21 Jun 2018 08:02:31 +0000</pubDate>
      
      <guid>https://kylestones.github.io/about/</guid>
      <description>书籍是人类进步的阶梯 &amp;ndash; 高尔基
多数人为了避免真正的思考，愿意做任何事情 &amp;ndash; 王兴
努力做一个开心的人，开心到别人见了你也会觉得幸福！ &amp;ndash; 佚名
未经审视的人生是不值得过的 &amp;ndash; 苏格拉底
生命比盖房更需要蓝图 &amp;ndash; 卡内基
抓住了这个主要矛盾，一切问题就迎刃而解了 &amp;ndash; 《毛泽东选集.第一卷》
生活中 10% 由发生在你身上的事情组成，而另外的 90% 则由你对所发生事情如何反应决定 &amp;ndash; 费斯汀格法则 Festinger
任何事物都需要创造两次，一次在大脑，一次在实践 &amp;ndash; 高效能人士的七个习惯
Fear is your friend. More often than not it shows you exactly what you should do. &amp;ndash; TED
The perceived complexity of a task will expand to fill the time you allow it. &amp;ndash; Parkinson&amp;rsquo;s Law
A muttonhead, a cretin, a dumbbell and an imbecile.</description>
    </item>
    
    <item>
      <title>C&#43;&#43;</title>
      <link>https://kylestones.github.io/blog/apue/cpp/</link>
      <pubDate>Wed, 30 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/apue/cpp/</guid>
      <description>头文件 #include &amp;lt;cctype&amp;gt; // C++ 头文件，开头的 C 表明来自 C 语言，但更符合 C++ 规范 #include &amp;lt;ctype.h&amp;gt; // C 语言头文件；不应该在 C++ 中使用 // 为啥 C++ 的头文件名都没有 .h 后缀呀？ // IO 库 #include &amp;lt;iostream&amp;gt; // 容器 #include &amp;lt;vector&amp;gt; #include &amp;lt;list&amp;gt; #include &amp;lt;deque&amp;gt; #include &amp;lt;bitset&amp;gt; #include &amp;lt;stack&amp;gt; #include &amp;lt;queue&amp;gt; #include &amp;lt;priority_queue&amp;gt; 命名空间 namespace kyle { // 嵌套命名空间 namespace sanshi { ... } } // 命名空间重命名 作用域 scope 全局作用域：定义在所有函数外部 局部作用域：定义在函数内部 语句作用域：for 循环中，C 语言中不可以在 for 循环语句中定义变量 局部变量 hide 全局变量。</description>
    </item>
    
    <item>
      <title>Funny of C</title>
      <link>https://kylestones.github.io/blog/apue/fun-of-c/</link>
      <pubDate>Tue, 29 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/apue/fun-of-c/</guid>
      <description>变量 所谓变量，其实是内存地址的一个抽像名字罢了;;
在静态编译的程序中，所有的变量名都会在编译时被转成内存地址。机器是不知道我们取的名字的，只知道地址。
不管结构体的实例是什么– 访问其成员其实就是加成员的偏移量。
宏定义 C 语言的宏实现对组成程序的字符进行变换的方式。宏既可以使一段看上去完全不合语法的代码成为一个有效的 C 程序，也能使一段看上去无害的代码成为一个可怕的怪物。比如可以阅读陈皓的 《6 个变态的 C 语言 Hello World 程序》
#define tString(x) #x // 传入参数两侧加上双引号&amp;#34; 。如果入参是变量，其定义必须在宏定义之前 #define toChar(x) #@x // 将参数两侧加上单引号&amp;#39; ， #define conn(x,y) x##y // 连接两个参数 字符串 数组 C 语言中只有一维数组，而且数组的大小必须在编译期就作为常数确定下来。但数组中的元素可以是任何类型 的对象，当然也就可以是另外一个数组 对一个数组只能做两件事：确定数组的大小；获得指向数组下标为 0 的元素的指针。 任何一个数组下标运算都等同于一个对应的指针运算。
对于数组 char s[10]来说，数组名 s 和 &amp;amp;s 都是一样的。
int a[10]; int i = 3; *a = 84; *(a+i) = 22; a[i] = 21; /* 表达式 *(a+i) 即数组 a 中下标为 i 的元素的引用；由于比较常用，所以被简记成 a[i] ； * 又由于 a+i 与 i+a 含义相同，因此 *(a+i) 与 *(i+a) 含义相同，所以 a[i] 与 i[a] 含义相同 */ i[a] = 12; //虽然如此，但强烈不建议这么写 数组的原地就是内容，长度为 0 的数组其并不占据内存。</description>
    </item>
    
    <item>
      <title>Computer Systems - A Programmer&#39;s Perspective</title>
      <link>https://kylestones.github.io/blog/apue/csapp/</link>
      <pubDate>Thu, 24 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/apue/csapp/</guid>
      <description>局部性 局部性良好的代码速度会大大提高
分为时间局部性和空间局部性；时间局部性表明一个变量在不远的将来会再次被访问；空间局部性表明一个变量周围的变量很快会被访问。
代码提升的关键在于充分利用高速缓存 cache (CPU 和主存读写速度之间的差距在不断增大）
程序的机器级表示 汇编代码以及机器码都不包含任何变量名字以及类型信息 。程序就仅仅是一些数字序列而已。
在代码中假如汇编的方法：
函数使用汇编代码实现，然后链接的回收使用 使用 GCC 提供的选项，直接在 C 函数中插入汇编代码 x86-64 16 个通用寄存器 Instruction pointer %rip Stack pointer %rsp Return value %rax Arguments passed in registers %rdi, %rsi, %rdx, %rcx, %r8, %r9 Callee-saved %rbx, %r12, %r13, %r14, %rbp, %rsp Caller-saved %rdi, %rsi, %rdx, %rcx, %r8, %r9, %rax, %r10, %r11 x86-64 惯例，操作 32 位寄存器，会将该寄存器的高 32 位设置为 0 。
栈 栈的地址向下增长，即每次压栈都会导致寄存器 %rsp 的值减小。大多数函数都需要栈帧（栈上的一段内存）来保存信息，栈帧的结构从 栈底到栈顶依次为 保存的 callee-saved 寄存器的值 局部变量 调用其他函数需要的 6 个参数寄存器以外的内存 返回地址 程序使用栈完成函数调用，此时主要由寄存器 %rip 以及 %rsp 完成</description>
    </item>
    
    <item>
      <title>Get Things Done</title>
      <link>https://kylestones.github.io/blog/book/gtd/</link>
      <pubDate>Thu, 24 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/book/gtd/</guid>
      <description>YOUR MIND IS FOR HAVING IDEAS, NOT HOLDING THEM. – David Allen
时间管理的本质，是管理我们的心智和行动！</description>
    </item>
    
    <item>
      <title>Git</title>
      <link>https://kylestones.github.io/blog/tools/git/</link>
      <pubDate>Thu, 24 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/tools/git/</guid>
      <description>简介 Git 是用 C 语言开发的分布式版本控制系统。
Git 保存的不是文件差异或者变化量，而只是一系列文件快照。而其他系统在每个版本中记录着各个文件的具体差 异。Git 并不保存这些前后变化的差异数据。实际上，Git 更像是把变化的文件作快照后，记录在一个微型的文件 系统中。每次提交更新时，它会纵览一遍所有文件的指纹信息并对文件作一快照，然后保存一个指向这次快照的索 引。为提高性能，若文件没有变化，Git 不会再次保存，而只对上次保存的快照作一链接。
在 Git 中提交时，会保存一个提交（commit）对象，该对象包含一个指向暂存内容快照的指针，包含本次 提交的作者等相关附属信息，包含零个或多个指向该提交对象的父对象指针。
Git 中的绝大多数操作都只需要访问本地文件和资源，不用连网。
保存到 Git 之前，所有数据都要进行内容的校验和（checksum）计算，并将此结果作为数据的唯一标识和索引。 这项特性作为 Git 的设计哲学，建在整体架构的最底层。所以如果文件在传输时变得不完整，或者磁盘损坏导致 文件数据缺失，Git 都能立即察觉。Git 使用 SHA-1 算法计算数据的校验和，通过对文件的内容或目录的结构计 算出一个 SHA-1 哈希值，作为指纹字符串。该字串由 40 个十六进制字符组成。Git 的工作完全依赖于这类指纹 字串，所以你会经常看到这样的哈希值。实际上，所有保存在 Git 数据库中的东西都是用此哈希值来作索引的， 而不是靠文件名。
对于任何一个文件，在 Git 内都只有三种状态：已提交（committed），已修改（modified）和已暂存（staged）
安装 $ apt-get install git 配置 命令 git config 专门用来配置或读取相应的工作环境变量，这些环境变量决定了 git 在各个环节的具体工作方 式和行为，存在在三个地方：
/etc/gitconfig 文件：系统中所有用户都普遍适用的配置；使用 git config 时用–system 选项，读写的就 是这个文件 ~/.gitconfig 文件：只适用该用户，使用 –global 选项时读写该文件；一般我们只配置该文件 .git/config 文件：进对该项目有效。每一级别的配置都会覆盖上层的相同配置 首先需要配置用户名称和电子邮件地址，这两条配置很重要，每次提交的时候都会随更新一起被纳入历史记录</description>
    </item>
    
    <item>
      <title>GNU Compiler Collection</title>
      <link>https://kylestones.github.io/blog/tools/gcc/</link>
      <pubDate>Thu, 24 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/tools/gcc/</guid>
      <description>GCC 由最初的 GNU C Compiler 逐渐演变为 GNU Compiler Collection ，支持 C, C++, Objective-C, Fortran, Ada, Go 语言的编译，当然也包含这些语言的库。不同的语言会有不同的版本， gcc 是 c 语言的编译器。
编译 编译过程通常包含 4 个步骤，可以指定选项来只完成某一步操作：
预处理或预编译 (Preprocessing)，进行宏替换、注释消除、找到库文件 : gcc -E test.c -o test.i 编译 (Compilation) ，编译成汇编代码 : gcc -S test.i -o test.s 汇编 (Assembly) ，生成机器代码（目标代码）: gcc -c test.s -o test.o 连接 (Linking) ，将目标文件和库文件连接起来，生成可执行文件 : gcc test.o -o test # 没有连接选项 多文件编译 # 方法一 gcc test1.c test2.c -o test # 方法二 gcc test1.</description>
    </item>
    
    <item>
      <title>GNU Project Debugger</title>
      <link>https://kylestones.github.io/blog/tools/gdb/</link>
      <pubDate>Thu, 24 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/tools/gdb/</guid>
      <description>GDB是GNU开源组织发布的一个强大的UNIX下的程序调试工具。
GDB主要帮忙你完成下面四个方面的功能：
启动你的程序，可以按照你的自定义的要求随心所欲的运行程序。 可让被调试的程序在你所指定的调置的断点处停住。 当程序被停住时，可以检查此时你的程序中所发生的事。 动态的改变你程序的执行环境。 预置条件：在编译连接的时候要指定 -g 选项，把调试信息加入到可执行文件。
启动 $ gdb program $ gdb --args program arg1 arg2 ... # 带着执行参数挂载 gdb $ gdb # 先进入 gdb 界面 (gdb) file program $ gdb program core # $ gdb program PID # 程序运行之后再挂载 $ gdb attach PID # attach 帮助 (gdb) help # 列出命令的所有种类 (gdb) help &amp;lt;class&amp;gt; # 查看该类的所有命令 (gdb) helo &amp;lt;command&amp;gt; # 查看该命令的帮助信息 在输入命令名称不完整的时候，gdb 可以自动配置命令，然后执行匹配到没有歧义的命令。若有歧义则提示用户输 入更多信息来批备命令。直接回车将执行上一次执行的命令。
查看源代码 (gdb) list # 查看程序代码，简写 l (gdb) l (gdb) list &amp;lt;first&amp;gt;, &amp;lt;last&amp;gt; # 显示从first行到last行之间的源代码 (gdb) show listsize # 查看 (gdb) set listsize 20 # 修改 (gdb) forward-search &amp;lt;regexp&amp;gt; # 向前搜索源代码；regexp 是正则表达式 (gdb) search &amp;lt;regexp&amp;gt; (gdb) reverse-search &amp;lt;regexp&amp;gt; # 向后搜索源代码 (gdb) info line # 查看源代码在内存中的地址 (gdb) disassemble func # 查看源程序的当前执行时的机器码，这个命令会把目前内存中的指令 dump 出来 程序运行上下文 (gdb) set args arg1 # 这里指可以设置运行参数，不能加程序名 (gdb) show args (gdb) path &amp;lt;dir&amp;gt; # 设定程序运行路径 (gdb) show path # 查看程序运行路径 (gdb) cd /root # (gdb) pwd (gdb) set environment USER=sanshi # 设置环境变量 (gdb) show environment (gdb) info terminal # 显示程序用到的终端模式 (gdb) run &amp;gt; outfile # 重定向程序输出 (gdb) tty /dev/tty1 # 设置输入输出使用的终端设备 (gdb) set logging on # 将命令的输出保存到默认的 gdb.</description>
    </item>
    
    <item>
      <title>LaTex</title>
      <link>https://kylestones.github.io/blog/tools/latex/</link>
      <pubDate>Thu, 24 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/tools/latex/</guid>
      <description>\(\LaTeX\) 是一个文档准备系统 (Document Preparing System)，它非常适用于生成高印刷质量的科技类和数学 类文档。它也能够生成所有其他种类的文档，小到简单的信件，大到完整的书籍。 \(\LaTeX\) 使用 \(\TeX\) 作 为它的排版引擎。
\(\TeX\) 是高德纳 (Donald E.Knuth)开发的、以排版文字和数学公式为目的的一个计算机软件。高德纳从 1977 年开始开发 \(\TeX\) ，以发掘当时开始用于出版工业的数字印刷设备的潜力。正在编写著作《计算机程序设计艺 术》的高德纳，意图扭转排版质量每况愈下的状况，以免影响他的出书。我们现在使用的 \(\TeX\) 排版引擎发布 于 1982 年，在 1989 年又稍加改进以更好地支持 8-bit 字符和多语言排版。\(\TeX\) 以其卓越的稳定性、跨平 台、几乎没有 Bug 而著称。1990 年推出 3.1 版, 并宣布不再更新 (只修正 bug)。\(\TeX\) 的版本号不断趋近 于 \(\pi\)，当前为 3.141592653。
\(\TeX\) 读作 &amp;#34;Tech&amp;#34; ，其中 &amp;#34;ch&amp;#34; 的发音类似于 &amp;#34;h&amp;#34; ，与汉字“泰赫”的发音类似。\(\TeX\) 的拼写来自希 腊词语 technique （技术） 的开头几个字母。在 ASCII 字符环境，\(\TeX\) 写作 TeX。\(\TeX\) 系统提供了 300 + 600 多条基本的排版命令。
\(\TeX\) 提供的命令都是一些很底层的命令, 普通用户使用起来不太方便；大牛们在 \(\TeX\) 基础上, 定义新的命令, 为普通用户排版提供方便</description>
    </item>
    
    <item>
      <title>make</title>
      <link>https://kylestones.github.io/blog/tools/make/</link>
      <pubDate>Thu, 24 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/tools/make/</guid>
      <description>make 可以自行决定一个大型程序中的哪些源文件需要重新编译，并根据指定的命令对其进行重新编译。make 不仅 可以用于 C 语言的编译，其可以用于任何可以使用 shell 命令进行编译的语言。而且 make 并不仅仅限于编译某 种语言，也可以使用 make 工具来做一些其它的事。例如，有这样的需求：当我们修改了某个或者某些文件后，需 要能够根据修改的文件来自动对相关文件进行重建或者更新。
需要编写 makefile 文件来使用 make 这个工具。makefile 用于告诉 make 需要编译哪些文件，以及如何编译。 makefile 文件中描述了文件之间的关系，并制定了一些命令用于编译更新。一般来说，一个可执行程序文件依赖 一些目标文件，而这些目标文件有某些源文件编译得到。一旦完成了一个有效的 makefile 文件，每次只需要执行 make 命令便可以完成增量编译。make 根据文件的最后修改时间来决定哪些文件需要被更新。
edit : main.o kbd.o command.o display.o \ insert.o search.o files.o utils.o cc -o edit main.o kbd.o command.o display.o \ insert.o search.o files.o utils.o main.o : main.c defs.h cc -c main.c kbd.o : kbd.c defs.h command.h cc -c kbd.c command.o : command.</description>
    </item>
    
    <item>
      <title>美句</title>
      <link>https://kylestones.github.io/blog/emotion/goodsentence/</link>
      <pubDate>Sun, 25 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/emotion/goodsentence/</guid>
      <description>杂记 人们宁愿去关心一个蹩脚的电影演员的吃喝拉撒和鸡毛蒜皮，而不愿了解一个普通人波涛汹涌的内心…… —路瑶《平凡的世界》
世界上有两样东西不能直视，一是太阳，而是人心 —东野奎吾《白夜行》
人的恶，连佛都度不了 —《西游记》
一个人可以被毁灭，但不能被打败。 —海明威《老人与海》
生活就像一座围城，城里的人想出去，城外的人想进来。 —《围城》
其实所有纠结做选择的人心里早就有了答案，咨询只是想得到心里内心所倾向的选择，最终的所谓命运，还是自己一步步走出来的。 —东野圭吾《解忧的杂货铺》
让你难过的事，总有一天你会笑着说出来。 —《肖申克的救赎》
孩子，我要求你读书用功，不是因为我要你跟别人比成绩，而是，我希望你将来拥有选择的权利，选择有意义，有时间的工作，而不是被 迫谋生。当你的工作在你心中有意义，你就有成就感。当你的工作给你时间，不剥夺你的生活，你就有尊严。成就感和尊严，给你快乐。 —《亲爱的安德鲁》
真正有气质的淑女，从不炫耀她所拥有的一切，她不告诉人她读过什么书，去过什么地方，有多少件衣服，买过什么珠宝，因为她没有自 卑感。 —《圆舞》
十年饮冰，难凉热血。 —梁启超《饮冰室合集》
每逢你想要批评任何人的时候，你就记住，这个世界上所有人，并不是个个都有过你拥有的那些优越条件。 —《了不起的盖茨比》
我什么都没忘，只是有些事适合收藏。 —史铁生《我与地坛》
这是一个最好的时代，这是一个最坏的时代；这是一个智慧的年代，这是一个愚蠢的年代；这是一个光明的季节，这是一个黑暗的季节 —狄更斯《双城记》
被真相伤害总比被谎言欺骗的好，得到了再失去，总是比从来就没有得到更伤人。 —《追风筝的人》
也许每一个男子都有过这样两个女人，至少两个。娶了红玫瑰，久而久之，红的变成了墙上的一抹蚊子血，白的还是床前明月光。娶了白 玫瑰，白的便是衣服上沾的一粒饭粘子，红的却是心口上一粒朱砂痣。 —张爱玲《红玫瑰与白玫瑰》
庭有枇杷树，吾妻死之年所手植也，今已亭亭如盖矣。 —《项脊轩志》
哪有人喜欢孤独，不过是不喜欢失望罢了。 —村上春树《挪威的森林》
钱钟书先生对杨降，从今往后，咱们只有死别，再无生离。 —《我们仨》
对世界而言，你是一个人；但对于某个人，你是他的整个世界。 —《飘》
我知道我长的丑，被仍石头无所谓，但让你害怕让我觉得很难过。 —《巴黎圣母院》
海底月是天上月，眼前人是心上人。 —《我不喜欢这世界，我只喜欢你》
这世上真话本就不多，一位女子的脸红胜过一大段对白。 —《骆驼祥子》
I love three things in this world, Sun, moon and you, Sun for morning, moon for night, and you forever. —《暮光之城》</description>
    </item>
    
    <item>
      <title>发现</title>
      <link>https://kylestones.github.io/blog/idea/something-find/</link>
      <pubDate>Sat, 24 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/idea/something-find/</guid>
      <description> https://36kr.com/p/5042735 Conway&amp;#39;s law [https://www.ruanyifeng.com/blog </description>
    </item>
    
    <item>
      <title>mpayer</title>
      <link>https://kylestones.github.io/blog/tools/mplayer/</link>
      <pubDate>Wed, 24 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/tools/mplayer/</guid>
      <description>拷贝路径 mplayer 参数说明
格式有点凌乱，待整理。 TODO
MPlayer 名称 概要 说明 一般注记 播放选项 ( 仅用于 MPLAYER) 分路器 / 媒体流选项 OSD/ 字幕选项 音频输出选项 ( 仅用于 MPLAYER) 视频输出选项 ( 仅用于 MPLAYER) 解码 / 滤镜选项 编码选项 ( 仅用于 MENCODER) 键盘控制 SLAVE 模式协议 文件 示例 BUGS 作者 标准声明 名称 mplayer − Linux下的电影播放器 mencoder − Linux下的电影编码器 概要 mplayer [选项] [ 文件 | URL | 播放列表 | - ] mplayer [全局选项] 文件1 [特定选项] [文件2] [特定选项] mplayer [全局选项] {一组文件和选项} [针对该组的特定选项] mplayer [dvd|vcd|cdda|cddb|tv]://title [选项] mplayer [mms[t]|http|http_proxy|rt[s]p]:// [用户名:密码@]URL[:端口] [选 项] mencoder [选项] [ 文件 | URL | - ] [−o 输出文件] gmplayer [选项] [−skin skin] 说明 mplayer 是一个LINUX下的电影播放器, (也能运行在许多其它的Unices 和 非x86 的CPU 上, 参看文档).</description>
    </item>
    
    <item>
      <title>Org-mode笔记</title>
      <link>https://kylestones.github.io/blog/tools/org-mode/</link>
      <pubDate>Wed, 27 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/tools/org-mode/</guid>
      <description>概要 以 # 号后加一空格开始的行表示注释，文件导出时这些内容不被导出，上面第一行就是。 以 #+ 符号开始的行用于设置文档参数或内容属性，比如文档的标题、作者，org-mode打开时文档的呈现状态等。 用 括起来的内容表示外部链接 用 &amp;lt;&amp;lt;&amp;gt;&amp;gt; 括起来的内容表示文档的内部链接 以 * 符号开始的行，表示该行为标题。 标题内容前的 TODO 标记是待办任务的标记符号。 标题行后面两个 : 符号间的内容表示标签（ TAG ）。 标题行如果标题文本前有 COMMENT 标记表示该标题下的所有内容为注释。 位于 #+BEGIN_XXX 和 #+END_XXX 之间的内容为特殊文档块，如代码块、例子、引用等。 C-c C-x C-h查看按键的帮助信息 元数据 文档元数据 具体包括： #+TITLE: the title to be shown (default is the buffer name) #+AUTHOR: the author (default taken from user-full-name) #+DATE: a date, an Org timestamp1, or a format string for format-time-string #+EMAIL: his/her email address (default from user-mail-address) #+DESCRIPTION: the page description, e.</description>
    </item>
    
    <item>
      <title>Unix Network Programming</title>
      <link>https://kylestones.github.io/blog/apue/unp/</link>
      <pubDate>Wed, 13 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/apue/unp/</guid>
      <description>问题 程序异常终止 此时所有打开的文件描述符将被关闭，TCP 连接发送一个 FIN 。然后呢？对端的确认报文是否收到无所谓？ 对端发送 FIN 报文，此时已经终止？无法恢复确认报文？对端会重复发送？
IP 分片(fragment) – TCP 分段 MTU : 最大传输单元 链路层对网络数据帧的一个限制：以太网限制 MTU 为 1500 字节。 尽管 IP 报文头中有 16 位表示数据报的长度（最大长度 2^16-1=65535） IP 报文的长度超过 MTU 就需要分片，IP 数据报的分片与重组都是在网络层完成的。 IP 头有 3 个标志位，一个标志位保留；一个标志位 DF 表示是否允许分片，为 0 表示允许分片，为 1 表示禁止 分片，当报文长度大于 MTU 则丢弃该报文，并向源主机发送 ICMP 报文；一个标志位 MF 表示之后是否还有分片 片偏移用于确定该片偏移原始数据报开始处的位置。 16 位标识用于确定相应的片是否属于同一个 IP 报文。 每个以太帧长度在 64 bytes ~ 1518 bytes，减去以太网帧头（DMAC 48bits=6Bytes + SMAC 48bits=6Bytes + Type 2Bytes + CRC 4Bytes）最大只能有 1500 bytes。链路层数据部分长度的要求是 46bytes ~ 1500bytes</description>
    </item>
    
    <item>
      <title>Advanced Programming in the UNIX Environment</title>
      <link>https://kylestones.github.io/blog/apue/apue/</link>
      <pubDate>Thu, 07 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/apue/apue/</guid>
      <description>I/O 文件 I/O 通过文件描述符标识。对于内核而言，所有打开文件都通过文件描述符引用。 其也是有缓冲的，只是其缓冲区在内核空间，不再用户空间。体现在延迟写，只在适当的时候才调用写文件操作， 减少不必要的写操作，增加性能。 函数 open() 打开文件时，指定模式必须有且仅有 O _RDONLY / O _WRONLY / O _RDWR 三者中的一个。 选项 O _APPEND 指定为追加。原来的 UNIX 系统不支持追加，只能先使用 lseek() 先设置文件的偏移量到文件的 结尾，然后再写。但是这在多个进程同时写的时候会出问题，因为调用了两个函数来追加，所以不是一个原子操作。 而追加选项确保设置偏移和写操作为原子操作。
管道读写 写管道的时候不用使用追加选项，内核会自动按写的顺序写入管道。读取后自动将相应的内容从管道清除。 读取一个写端关闭的管道，在读取完全部数据后，read 函数返回 0 ； 写一个读端关闭的管道，产生 SIGPIPE 信号，设置该信号处理函数或者忽略该信号，write 返回 -1，且 errno 设置为 EPIPE。 读管道，如果管道为空，调用线程阻塞，同进程内的其他线程不受影响。
套接字描述符 虽然套接字描述符本质上是一个文件描述符，但不是所有参数为文件描述符的函数都可以接受套接字描述符。 套接字不支持文件偏移量概念，不能使用 lseek 函数，也不可以使用 mmap 函数，不可调用 fchdir 函数。
shutdown 函数可以直接关闭一个套接字的读端或者写端，不管该套接字描述符复制了多少分； close 函数只有在关闭最后一个套接字的时候才会释放该套接字。
改变文件偏移量 lseek
标准库 I/O 标准 I/O 库的操作围绕流(stream)进行。利用指向 FILE 对象维护，该结构体包含了标准 I/O 库为了维护该流所 需要的信息：文件描述符，指向缓冲区的指针，缓冲区的长度，缓冲区中当前的字符数，出错标志等。不需要关心 FILE 结构的具体形式。 相对于文件 I/O，标准库的 I/O 都是带缓冲的，标准库维护了一个缓冲区，在适当的时候才调用 read、write 函 数，从而减少系统调用的开销。</description>
    </item>
    
    <item>
      <title>How to read a book</title>
      <link>https://kylestones.github.io/blog/book/how-to-read-a-book/</link>
      <pubDate>Thu, 07 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/book/how-to-read-a-book/</guid>
      <description>HOW TO READ A BOOK 第一篇 阅读的层次 第一章 阅读的活力与艺术 书是写给那些想要把读书的主要目的当作是增进理解能力的人而写。 “readers”：那些今天仍然习惯于从书写文字中汲取大量资讯，以增进对世界的了解的人，就和过去历史上每一个深有教养、智慧的人别无 二致。 许多资讯与知识是从口传或观察而得，或者收音机、电视、网络等，但这些是远远不够的，他们知道还得阅读，而他们也真的身体力行。 新时代的传播媒体是否真能增进我们对自己世界的了解？——我们为了理解一件事，并不需要知道和这件事相关的所有事情。太多的资 讯就如太少的资讯一样，都是一种对理解力的阻碍。换句话说，现代媒体正在以压倒性的泛滥资讯阻碍了我们的理解力。 现代媒体经过太精心的设计，直接将包装后的观点装进自己的脑海中，目的都在让人很容易整理出“自己”的思绪，使得思想形同没有需要 了；但事实并非如此。
主动的阅读 目标：
提醒读者，阅读可以是一件多么主动的事 阅读越主动，效果越好 听众或读者的“接收”，应该像棒球赛中的捕手一样，而不是类似被打了一拳，或者得到一项遗产。捕手的艺术就在能接住任何球的技巧： 快速球、曲线球、变化球、慢速球等等。阅读的艺术也在尽可能掌握住每一种讯息的技巧。 同样的书一个人比另一个人读的好：
这人读的更主动 他在阅读中的每一种活动都参与了更多的技巧 阅读是一个复杂的活动，就像写作一样，包含了大量不同的活动。要达成良好的阅读，这些活动都是不可或缺的。一个人越能运作这些活 动，阅读的效果就越好。
阅读的目标：为获得资讯而读以及为求得理解而读 真正的阅读：没有任何外力的帮助，你就是要读这本书。你什么都没有，只凭着内心的力量，玩味着眼前的字句，慢慢地提升自己，从只 有模糊的概念到更清楚地理解为止。 阅读的定义：这是一个凭借着头脑运作，除了玩味读物中的一些字句之外，不假任何外助，以一己之力来提升自我的过程。 凭着自己的心智活动努力阅读，从只有粗浅的了解推进到深入的体会，就像是自我的破茧而出。 这里的“学习”指的是理解更多的事情，而不是记住更多的资讯。 任何一个可以阅读的人，都有能力用这样的方式来阅读。只要我们努力运用这些技巧在有益的读物上，每个人都能读的更好，学的更多， 毫无例外。
阅读就是学习：指导型的学习，以及自我发现型的学习之间的差异 所谓吸收咨询，就是知道某件事发生了；想要被启发，就是要去理解，搞清楚这到底是怎么回事：为什么会发生，与其他的事实有什么关 联，有什么类似的情况，同类的差异在哪里等等。 要能被启发，除了知道作者所说的话之外，还要明白他的意思，懂得他为什么这么说。 蒙田说：初学者的物质在于未学，而学者的无知在于学后。——第二种无知是读错了许多书，总有一群书呆子读得太广，却读不通，被 称为“半瓶醋”（Sophomores） 自我发现型的学习是必要的—这是经由研究、调查或无人指导的状况下，自己深思熟虑的一种学习过程。 “辅导型的自我发现学习”：类似医生努力为病人做许多事，但最终的结论是这个病人必须自己好起来，变得健康起来；农夫为他的植物或 动物做了许多事，结果是这些植物必须长大，变得更好；老师用尽了方法来教学生，学生却必须自己能学习才行。当他学习到了，只是就 会在他的脑海中生根发芽。 无论那种学习方式，都不该没有活力，就像任何阅读都不该死气沉沉。 辅助型自我发现学习及非辅助型自我发现学习之间的差异—一个最基本的不同点就在学习者所使用的教材上。辅助型自我发现学习是阅 读自我或世界的学习；非辅助型学习是阅读一本书，包括倾听，从讲解中学习的一种艺术。 “思考”是指运用我们的头脑去增加知识或理解力。思考是在这两种学习中都会出现的东西，在阅读与倾听时我们必须思考，就像我们在研 究时一定要思考一样。 思考是主动阅读的一部分，还必须观察、记忆，在看不见的地方运用想象力。 阅读的艺术包括了所有非辅助型自我发现学习的技巧：敏锐的观察、灵敏可靠的记忆、想象的空间，再者就是训练有素的分析、省思能力。 因为阅读是一种经过帮助后的发现。
老师的出席与缺席 如果你问一本书一个问题，你就必须自己回答这个问题。只有在你自己作了思考与分析之后，才会在书本上找到答案。因此我们就要懂得 如何让书本来教导我们。
第二章 阅读的层次 读者要追求的目的决定会决定他阅读的技巧； 阅读的效果取决于他在阅读上花了多少努力与技巧。 基础阅读—elementary reading 这个阶段面对的主要问题：如何认出一页中的一个个字 不论我们身为有多精通这样的阅读技巧，我们在阅读的时候还是一直会碰上这个层次的阅读问题。 典型问题：这个句子在说什么？
检视阅读—inspectional reading 特点在强调时间：必须在规定的时间内完成阅读 检视阅读是系统化略读（skimming systematically）的一门艺术 典型问题：这本书在谈什么？ 许多优秀的读者都忽略了检视阅读的价值。</description>
    </item>
    
    <item>
      <title>friends</title>
      <link>https://kylestones.github.io/blog/emotion/friend/</link>
      <pubDate>Mon, 27 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/emotion/friend/</guid>
      <description>搞笑 3-3-05:25 Ross to Chandler: You never look. You just answer. It&amp;#39;s like a reflex. &amp;#34;Do I look fat?&amp;#34; &amp;#34;No.&amp;#34; &amp;#34;Is she prettier than I am?&amp;#34; &amp;#34;No.&amp;#34; &amp;#34;Does the size matter?&amp;#34; &amp;#34;No.&amp;#34; &amp;#34;And it works both ways.&amp;#34;
So you both just know this stuff?
You know, after about 30 or 40 fights, you kind of catch on.
3-12-10:43 It&amp;#39;s from Ross. It&amp;#39;s a love bug. Somebody wants people to know you have a boyfriend.</description>
    </item>
    
    <item>
      <title>他山之石</title>
      <link>https://kylestones.github.io/blog/emotion/experience/</link>
      <pubDate>Sat, 18 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/emotion/experience/</guid>
      <description>胜败 有时候，暂时的失利比暂时的胜利要好很多。 – 《毛.骗》
体育中总是胜者不变，败者变。失败者会努力思考现状，改变修正不足，查找出奇制胜奇招，最终打败对手。当你失败的时候，可能是为 了让你拥有更大的成功。人生不如意事常八九，可我们却没有很好的接受失败教育。 – 《白说》
First they ignore you. Then they laugh at you. Then they fight you. Then you win. 聪明 木秀于林，风必催之；堆出于岸，流比湍之；行高于人，人必非之。杨修和曹冲都是被自己的聪明害死的。
同时不叫的鹅被杀。应介乎才与不才之间
看破不说破；不痴不聋不做阿家翁；难得糊涂；等等都是古人的智慧，他们并非不知道，只是装糊涂，但绝不踩雷。这才是真聪明。
谁都不傻，但谁都有傻的时候。
比赛 比赛，骑自行车，总是许多人一起比赛时结果 优于 单独一个人去测试。
感觉这个可能和鲶鱼理论有某种程度上的关联。
量子纠缠 两个粒子相互纠缠，即使相距遥远，一个粒子的行为将会影响另一个粒子的状态。
策略 《三侠五义》义渠人偷东西。《毛骗》第二季第六集44:22。第二季第八集19：11-23：58,27：55-29：59 先让他失望，在给他希望，那样他就深信不疑。《毛骗》第一季，第二集,28:30 当一个卧底都不知道自己是卧底的时候，自然毫无破绽。 思考 ego cogito ergo sum – 我思故我在 A man is but the product of his thoughts what he thinks, he becomes. silence is very import. 可以让自己好好思考总结考虑 尘世的喧嚣比内心的不安更加不堪。有时需要拒绝接受外界的信息。 自信 Deeply believe in themselves.</description>
    </item>
    
    <item>
      <title>哈弗幸福公开课</title>
      <link>https://kylestones.github.io/blog/emotion/happiness/</link>
      <pubDate>Sat, 18 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/emotion/happiness/</guid>
      <description>内容比较凌乱，待系统学习，梳理脉络，整理提纲 TODO
并不是所有的心理学观点或者研究都适合你，选择合适自己的去实践很重要。。。
为什么要研究积极心理学？ 需要集中研究，这样会带来更大的收获 并不是治疗了抑郁等负面情绪，就会变得幸福快乐 积极心理学可以有效预防压抑、 Questions always create reality. 我关注什么就只会看到什么。游戏：限定时间内观察一幅画，问题画中有多少几何图形？时间到后， 大多数人只会数几何图像的个数。如果再问，图中钟表的时间？大巴车中有几个孩子？图像左侧主要的颜色？等这些问题时，几乎没有人 知道。
We see what we look for and we miss much of what we are not looking for even though it is there… Our experience of the world is heavily influenced by where we place our attention. — Stavros and Torres
resilience :: 即使面对糟糕的环境、恶劣的处境，仍旧保持乐观、对未来充满希望。不是那种盲目乐观，而是这次没有成功、总结经验， 相信下次定会成功。
认清长处 设立目标 榜样 role model 不单干，社会支持 欣赏优点、美好的东西也很重要。
appreciate：欣赏长处、成功。不应把其视为理所当然；成长，欣赏长处-采取行动-长处增长；
责任：为了让小组变得更加美好，我能做什么？而不是去抱怨那个人哪里做的不够好等等
No one is coming!</description>
    </item>
    
    <item>
      <title>情商</title>
      <link>https://kylestones.github.io/blog/emotion/emotion/</link>
      <pubDate>Sat, 18 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/emotion/emotion/</guid>
      <description>真正的情商高手，少不了对情绪的洞察 心里学家Kang和Shaver的研究发现，一个情绪管理能力越强，TA的情绪复杂度也就越高，情绪回路也越多。而情绪复杂度高的关键就在于， 他们习惯分析他人的情绪，这使得他们有了更多的机会，从中学习情绪的多种回路表达。 很多研究表明，人识别面部情绪表达是由特定的神经通路实现的，当这些结构损伤时，人识别情绪的能力就会发生障碍，这样的人往往在 人际交往中四处碰壁。因为如果一个人失去了，从别人身上学习情绪的机会，那他就只能依赖自己的情绪经验，然而每个人的体验都是有 限的。甚至大多时候，由于当局者迷，我们自身也缺少识别情绪的能力。这会让我们在情商提高这件事上，陷入瓶颈。通俗地说，读千卷 书，行万里路，都不如阅人无数。 情绪复杂度高的人，对他人的情绪产生的多样化原因，能有更加深度的认识，这使得他们能有更好的方法，更多的语言来解决别人的情感 冲突，引得他人的理解。情绪的识别面小了，我们太容易钻牛角尖，容易把人活活逼死。所以太史公司马迁说过：世间本无事，一切在人 心。
例子 朋友 案例 当我因为命运的一些戏剧变化，突然间生活状态远高于过去时，即便我最好的朋友，也难以对我发自内心的祝贺。哪怕我有下乡支教这种 比较硬派的高尚美德，嫉妒情绪也会妨碍她们由衷替我感到高兴。老朋友们之所以不高兴，是因为她们突然发现，似乎我的地位可能高过 了她们时，她们认为这是一种对她们尊严的冒犯。
回答 当别人嘲讽时，我回答：“别说，我恐怕混不到约炮的那天，就已经被人骂死了吧。我这种满身晦气的网红，哪有女生看得上。要想真正 赚到钱，怕是要我们一起才行哦！” 回答要点，在急着炫耀自己成就前，先用一些事实让对方觉得‘我并没有比你强’，从而消除焦虑。之后再提醒对方，对于我的改变，他们 可以有其他的角度去思考。所以，没必要急着批评朋友们出现的嫉妒情绪，这不过是一种‘我是否安全’的危险觉知。你需要充分察觉到对 方的情绪，引导他们走出负面的揣测。 亚当斯密把这种处境比喻为‘好运者的悲剧’，他认为这样的人，会走上放弃人际间感情链接，拼命努力的道路，然后有一天，会因为事业 的受挫，身边缺乏社会支持，精神备受打击，从而一蹶不振。
总结 我们之所以在这样的情景里，所有的解释都感到苍白无力，就是因为对方的情绪系统其实被堵住了，你需要先引导他把情绪管道里的某些 东西释放出来，他才可能接收到你的某些积极能量。你应该顺从他们的意思帮助他们缓解焦虑，因为焦虑往往会让人，只注意到眼前的威 胁，即我似乎比他们地位高，他们很难从更大的角度去思考一些问题，比如是否可以学习一点我的经验，从其他途径为自己积累资本。 所谓孤独不是身边没人，就是你面对的那个人，他的情绪和你的情绪不再同一个频率。当解决了焦虑这个情绪时，对方的情绪同类才会畅 通，阳光才能照进别人的心里。
女老师 案例 有位大学女老师吐槽：“我最近压力太大了，学校为了评文明单位，我每天晚上都工作到十一点，我有个朋友因为忙，孩子都流产了，真 是不想干下去了。但评上后，今年会多发一个月工资。” 这个时候要学会识别她的情绪，她此刻的情绪，比起‘焦虑’更像‘愤怒’，一种对领导安排工作不合理的愤怒。但内心确实又没法完全否定 这件事的意义，这使她矛盾。接着，你要学会和她的情绪对话，疏导它的情绪。
回答 “可能你们领导觉得，一个单位材料越多就越文明，就算他们对待员工的方式，不是很文明”
总结 这里，疏导的关键就在于，你要用一种高度总结，逻辑严密的句子，把对方想说又不知道说的东西表达出来。他的内心仿佛有一种堵着的 东西，被你一说，瞬间就通畅了。网络上很多意见领袖，就是靠这样的方式吸粉，可以说，这是一种很高级的提供情感价值的方式，叫做 帮你说出，你想说的话。
小乌龟 案例 有姑娘和我倾诉：“我曾经养过一只小乌龟，后来我去旅游，就把它托付给我爹照顾，谁知道回来的时候，我的小乌龟就被我爹养死了！” 这时候，安慰她或者说一个自己的悲伤故事都不好。其实，这位姑娘的情绪要更加综合而复杂，她的真是情绪是，即为小乌龟的逝世感到 难过，又不想指责她的父亲。
回答 “爸爸接下这个任务，说明他有男人的担当，但比起你来，缺乏一点女性的细致。看你这么伤心，天国的小乌龟不会责怪你的。” 这段话不仅给了这位姑娘情绪价值，也为我们内心构建了更为复杂的情绪回路。
总结 人的记忆，从来都不是一栋完整的房子，到处是回忆的瓦 。情绪就像雨滴，当它淋在记忆的废墟上时，你会看到一个海市蜃楼的家，在 那个家里，有画满涂鸦的床头柜，有你曾经遗弃的小熊。进家拿起小熊，那种幻觉一般的触感，会让你找回过去的幸福，这份感觉会让你 不再沉默寡言，更不会歇斯底里。
情绪识别 你的情绪别人一定会察觉到，且受到你的情绪的影响。 放松的状态，无欲无求的状态是最自在的状态，能够最大的发挥自己的优点。 体会自己的感受，思想，从而去体会别人的思想。 一定要从别人身上体会其情感，观察其面部变化，才能提升自己的情感回路。
女人喜欢在她们面前很自在的男人，而且有魅力的男人都是这种在女人面前很自在的男人。 我内心的平静让女人在我身边时，感觉到安全。 情绪没有味道，但确实可以闻的到，你闻到某个人情绪的时候，你的内心也会自然升起那个情绪。 你所匮乏的会变成你所恐惧的，你一旦开始恐惧自己所匮乏的事物不能满足自己，你就会开始充满欲望。 每一个欲望背后都藏着一个深深的恐惧。 当注意力高度集中的时候，情绪，焦虑，和欲望都会消退，有人说男人在高潮的时候最接近神，其实是因为性爱的过程会高度集中人的注 意力。 这个世界上有很多事情需要等，但爱情这件事无需再等。
聊天时：不要说教，不要比较，不要评论，尽量少提问，尽量让语境变得轻松愉快一点。女生倾向于分享感受，很多话他们说出来没有目 的，就是想说而已，想分享一下此时此刻心里想到的而已。所以能和女生共鸣的对谈，必要的成分是分享感受。</description>
    </item>
    
    <item>
      <title>自我完善</title>
      <link>https://kylestones.github.io/blog/emotion/self-improvement/</link>
      <pubDate>Sat, 18 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/emotion/self-improvement/</guid>
      <description>自信、乐观开朗、有活力、笑 自信打不死的心态活到老。 凡事都要有自信。展现在生活的方方面面。 不要低声下气。 对未来乐观、有热情。 It is not the strongest of the species that survives nor the most intelligent. It is the one that is the most adaptable to change. – Charles Darwin 要多笑，这么多年的牙可别白刷了 You got a dream, you gotta protect it. People can&amp;#39;t do something themeselves, they wanna tell you you can&amp;#39;t do it. If you want something, go get it. Period. – The pursuit of happiness 我父亲说，生活很简单的，如果你非常想得到什么，你就会得到。而失败只是表明，欲望还不够 – 理发师的情人 思考 ego cogito ergo sum – 我思故我在 必须要自己思考，形成自己的观点。 行成于思，毁于随。 技巧-策略 谋定而后动，知止而有得，凡事讲究策略；plan A 还要有 plan B</description>
    </item>
    
    <item>
      <title>C Traps and Pitfalls</title>
      <link>https://kylestones.github.io/blog/apue/c-traps-and-pitfalls/</link>
      <pubDate>Tue, 29 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/apue/c-traps-and-pitfalls/</guid>
      <description>导读 程序严格按照我们写明的程序来执行，但结果却并不是我们真正希望得到的。 程序设计错误实际上反应的是程序与程序员的“心智模式”两者的相异之处。（心智模式(mental model)解释为人 们深植心中，对于周遭世界如何运作的看法和行为；《列子》中记录有疑邻盗斧）
练习 0-1 返修率高 你是否愿意购买一个返修率很高的厂商所生产的汽车？如果厂家声明它已经做出了改进，你的态度是否会改变？用 户为你找出程序中的 Bug，你真正损失的是什么？
答：我们经常会依据厂商的信誉去购买其商品；会考虑其最近的高质量是真实的还是偶然的；会损失信誉。而信誉 一旦失去，就很难重新获得。
0-2 修建一个 100 英尺长的护栏，护栏的栏杆之间相距 10 英尺，共需要多少根栏杆？ 答：11 根。
0-3 菜刀 在烹饪时你是否失手用菜刀切伤过自己的手？怎样改进菜刀使得使用更安全？你是否愿意使用这样一把经过改良的 菜刀？
答：我们很容易想到办法让一个工具更安全，代价是原来简单的工具现在要变得复杂一些。食品加工机一般有连锁 装置，保护使用者不让手指受伤。但是菜刀却不同，给这样一个简单、灵活的工具附加保护手指避免受伤的装置， 只能让其失去简单灵活的特点。实际上，这样做最后得到的也许更像一台食品加工机，而不是一把菜刀。 使其难于做“傻事”常常会使其难于做“聪明事”，正所谓“弄巧成拙”。
第 1 章 词法陷阱 从较低层面考虑，程序是由 符号(token) 序列组成的，将程序分解成符号的过程称为“词法分析” 术语符号指的是程序的一个基本组成单元，其作用相当于一个句子中的单词，在不同的句子中用于相同的意义；但 是组成符号的字符序列就不同，同一组字符序列在某个上下文环境中属于一个符号，而在另一个上下文环境中可能 属于完全不同的另一个符号。 编译器中负责将程序分解为一个一个符号的部分称为“词法分析器” 在 C 语言中，符号之间的空白（包括空格符、制表符或换行符）将被忽略，因此 C 语言书写的格式可以很随意， 但这并不是好习惯。
1.1 = is not == C 语言中赋值符号被作为一种操作符对待，因而重复进行赋值操作可以很容易的书写(a=b=c) 不要误用比较运算和赋值运算 比较运算时，如果有常量，将常量方在左侧 如果确实需要赋值运算，明确的表示出来，如下： if(x = y) foo(); //应写成 if (0 != (x = y)) foo(); 不要指望靠编译器来提醒这些警告消息，毕竟警告可以被忽略，而且有些编译器不支持 1.</description>
    </item>
    
    <item>
      <title>秘密--吸引力法则</title>
      <link>https://kylestones.github.io/blog/emotion/secret/</link>
      <pubDate>Wed, 05 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/emotion/secret/</guid>
      <description>内在的声音和影像比外在的观点更深刻更清晰和明确时，这个时候你就掌握了自己的命运。
吸引力法则： 目前的生活状况是过去想法和行为造成的结果，如果你用目前的生活状况来定义自己，你很容易相信，未来的生活和现在没有任何区别。 你今天的想法和感觉，正在创造你的未来 你把注意力放在什么想法和感受上，你就把他们吸引到你的生活中来，不管是不是你期待的。 你跟家人一起担心这些事跟朋友一起讨论那些不好的事，你的态度会令你进一步处于却这少那的状态。 你无论认为自己行还是不行都是对的。 你的人生是你自己创造出来的。 如果你想改变自己的状况，那你需要先改变自己的想法。 不要把过多的经历放在不想要的事情上，把焦点放在自己想要的事物上。学会静下心来，把注意力从自己不要的事物上转移开，让所有情 绪沉淀下来，把注意力放在你想要的体验的事情上。精力会随着注意力流转。 如果你心情好，那么你正在创造理想的未来。 只要简单转换一下心情，这一天甚至这辈子都会变得不一样；只要你不让任何事情影响你的好心情。 感恩 如果你对已经拥有的事物感到满足和感恩，你也许会很快得到其他你更想要的东西。 你必须要有渴望，要专心的想拥有她们。你要有企图心，一旦有了企图心，热切盼望你想要的东西，宇宙就会把你一直等着，想拥有的一 切，送到你面前。好好注意身旁那些美好的事物，祝福她们，赞赏她们；在另一方面，对于那些你目前不如意的事，不要浪费精神去挑剔 或者抱怨，去拥抱一切你想要的事物，你就会得到更多。 你的愿望、想法和你内心的感觉都非常重要，因为这些都会在你的生命里实现。 如果你想要富足、成功，那么你就要把注意力放在富足和成功上。 人能得到心里想要得到的一切。 每个人都有能力改变自己的人际关系和自己的财务状况等一切东西。 你能拥有你想要拥有的一切，你能成为你想成为的人，能做到任何想做的事。 你是你自己命运的主宰 所有的力量都来自内心，也就是说被我们自己掌控着。 你内心的力量比整个世界的力量还大。 经常有人这么跟我说：我希望明年薪水可以增加，可是接下来我从他们的所有的行为里看不出来他们打算要让他们的目标实现。你知道他 们会转头说：我付不起。 愿景版：把自己想要的东西都贴上去。 你可以设计自己的命运，你是个作者，你在写自己的故事，而笔就我在你的手里。最后的结局其实是你自己选的。 灵感出现后，你要相信她，还要采取行动。 宇宙喜欢快速行动，不要拖延、不要猜测、不要怀疑，当机会出现的时候，当冲动来临的时候，当内在的直觉推动你的时候，你必须马上 行动，这就是你的任务，也是你唯一要做的事情。 世界永远处于供过于求，美好的事物与创意是取之不尽的，能量、爱和快乐也是取之不尽的；只要你在心里感觉到自己有着无限的力量， 那么这些就会出现在你的现实生活里。永远不用担心供不应求。
观想：视动行为复演法：你所观想的一切都会变成现实。 奥林匹克运动员，你的心智辨别不出你是在跑步还是在做心里练习。 你在生活中得到是你所感受的集合，而不是你思考的东西。 吸引力不仅靠我们心里图像或想法来产生，真正创造她的是我们的感觉。 没有产生那种热爱或快乐的感觉，那么你就无法从心里产生吸引的力量。 感觉那种喜爱和快乐。 制造一种自己已经坐那辆车里，而不是你将来一定要拥有那辆车。 让你的渴望显现的捷径是—把你想要的，看成是即成的事实。 观想的力量很大很大 想象力就是一切，她是未来生命的预览。 决定你要什么，相信自己可以得到，也能够得到她，并且相信你有可能实现她，接下来每天闭上眼睛几分钟，想象自己已经得到了她，感 觉一下得到她是什么滋味，然后脱离出来，想象自己已经拥有了她并心怀感激，你要真的心满意足。
爱自己： 在两性关系里，先了解在这关系里面的人是非常重要的事。指的是你自己。 如果你独处的时候都不快乐，那别人跟你在一起的时候，又怎么会快乐呢？ 你对待你自己的方式和期待别人对你的方式一样吗？ 直到我真正的爱上我自己，周围的人才开始爱上我。 你拥有很棒的特质，我研究自己27年了，我真的想亲我自己！ 你一定要学者爱自己。 你天生完美 不是要自以为是，至少要尊重自己。 你只有爱自己，才会懂得怎样去爱别人。 你该对自己好一点，多留点时间给自己，给自己充电。然后你就可以给予。 在人际关系方面，大家都习惯抱怨别人不好。如果你真想建立良好的关系，你就因该多多关注和欣赏别人的优点，而不是注意他们的缺点。 因为我们越是抱怨，那些事情就会不断的发生。 如果和某人关系很恶劣，也是可以改善的。在未来30天里，拿出一张纸来，每天坐下来，把这个人的优点一一写下来。想想你为什么会爱 那个人，因为他很幽默还是因为他给你不断的支持，最后你会发现，只要你专心去想，感激和确定这个人的优点，他就会更为你展现这一 面，你们之间的问题就会消失。 我们无法控制别人，无论我们再怎么努力都没有用。你常常会把那些让自己快乐的机会拱手让给了别人，他们又常常没有办法如你所愿。 为什么呢？因为事实上，唯一能够控制你的快乐还有幸福的人是你自己。就算是你的父母、你的孩子、你的配偶，那些你最亲近的人，他 们也没有办法控制你的幸福。快乐在你自己的心中。 内在的快乐是成功必须的燃料。 初期目标是感觉和体验快乐，接着开始去做所有能给我带来快乐的事。有一句格言叫做“不好玩的事不做”。如果你坐着冥想一个小时能让 你快乐，那就去冥想，如果你吃香肠三明治的时候感到快乐，那就去买，当我摸的猫咪就会很开心，走进大自然就会很舒服，所以我想让 自己经常的处于这种状态。我所要做的就是专心把注意力放在我想要的，我希望在生活中显现的事。 生命太不平凡了，他是一段精彩的旅程。 你只要尽力做好你自己。 你经历的每一件事，你所经历的每一刻，其实都是为了现在这一刻所做的准备。 你得先追求内心的快乐与平和，内心现有这样的景象，然后外在事物才会一一呈现。</description>
    </item>
    
    <item>
      <title>基础心理学</title>
      <link>https://kylestones.github.io/blog/emotion/basicpsychlolgy/</link>
      <pubDate>Mon, 03 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/emotion/basicpsychlolgy/</guid>
      <description>心理暗示 一种主观意识上被肯定的假设，不一定有根据，但是由于主观上已经肯定了他的存在，所以在心理上便竭力倾向于这项内容。
心理战术 先上她失望，然后再给她希望，那样她就会深信不疑。
偷东西 绿林人抛砖引玉的方法，《三侠五义》
胜败 有时候暂时的失利比暂时的胜利要好很多。
谁都不傻，但谁都有傻的时候。
木秀于林，风比摧之；堆出于岸，流必湍之；行高于人，人必非之。
自信 self-confidence The ability or the belief to believe in yourself, to accomplish any task, no matter the odds, no matter the difficulty, no matter the adversity. The belief that you can accomplish it. Thoughts influence actions. Self-confidence people interpret feedback the way they choose to. No one will believe in you unless you do. 方法：
Repetition. Repetition. Repetition. Write a letter to myself, it was my own brag sheet.</description>
    </item>
    
    <item>
      <title>常见心理学效应</title>
      <link>https://kylestones.github.io/blog/emotion/effect/</link>
      <pubDate>Thu, 08 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/emotion/effect/</guid>
      <description>羊群效应 “羊群效应”也叫“从众效应”：是个人的观念或行为由于真实的或想象的群体的影响或压力，而向与多数人相一致的方向变化的现象。 无论意识到与否，群体观点的影响足以动摇任何抱怀疑态度的人。群体力量很明显使理性判断失去作用。
羊群是一种很散乱的组织，平时在一起也是盲目地左冲右撞，但一旦有一只头羊动起来，其他的羊也会不假思索地一哄而上，全然不顾前 面可能有狼或者不远处有更好的草。因此，“羊群效应”就是比喻人都有一种从众心理，从众心理很容易导致盲从，而盲从往往会陷入骗局 或遭到失败。
羊群效应的出现一般在一个竞争非常激烈的行业上，而且这个行业上有一个领先者（领头羊）占据了主要的注意力，那么整个羊群就会不 断模仿这个领头羊的一举一动，领头羊到哪里去“吃草”，其它的羊也去哪里“淘金”。
一则幽默 一位石油大亨到天堂去参加会议，一进会议室发现已经座无虚席，没有地方落座，于是他灵机一动，喊了一声：“地狱里发现石油了！”这 一喊不要紧，天堂里的石油大亨们纷纷向地狱跑去，很快，天堂里就只剩下那位后来的了。这时，这位大亨心想，大家都跑了过去，莫非 地狱里真的发现石油了？于是，他也急匆匆地向地狱跑去。 松毛虫实验 法国科学家让约翰·法伯曾经做过一个松毛虫实验。他把若干松毛虫放在一只花盆的边缘，使其首尾相接成一圈，在花盆的不远处，又撒 了一些松毛虫喜欢吃的松叶，松毛虫开始一个跟一个绕着花盆一圈又一圈地走。这一走就是七天七夜，饥饿劳累的松毛虫尽数死去。而可 悲的是，只要其中任何一只稍微改变路线就能吃到嘴边的松叶。
镜子效应 镜子效应来源与现实的生活，主要用于人际交往。当你照镜子时，镜子里的你会随着你的喜怒哀乐而变化。这是最狭义的镜子效应！同样 的，在人际交往中，你对别人好，别人也会对你好。相反你对别人不好，别人也对你不好！这就是镜子效应的真谛！ 在现实中的应用。 当你和你不喜欢的人相处时，或许他也不怎么喜欢你，但是只要你试着慢慢的喜欢他，逐渐的他也会开始喜欢和你相处！人都是相互的！
多看效应 对越熟悉的东西越喜欢的现象，心理学上称为“多看效应”。多看效应不仅仅是在心理学实验中才出现，在生活中，我们也常常能发现这种 现象。 有些人善于制造双方接触的机会，从而提高彼此间的熟悉度，互相产生更强的吸引力。在我们新认识的人中，有时会有相貌不佳的人，最 初，我们可能会觉得这个人难看，可是在多次见到此人之后，逐渐就不觉得他难看了，有时甚至会觉得他在某些方面很有魅力。 实验 20世纪60年代，心理学家查荣茨做过这样一个实验：他向参加实验的人出示一些人的照片，让他们观看。有些照片出现了二十几次，有的 出现十几次，而有的则只出现了一两次。之后，请看照片的人评价他们对照片的喜爱程度。结果发现，参加实验的人看到某张照片的次数 越多，就越喜欢这张照片。他们更喜欢那些看过二十几次的熟悉照片，而不是只看过几次的新鲜照片。也就是说，看的次数增加了喜欢的 程度。 另一个实验：在一所大学的女生宿舍楼里，心理学家随机找了几个寝室，发给她们不同口味的饮料，然后要求这几个寝室的女生，可以以 品尝饮料为理由，在这些寝室间互相走动，但见面时不得交谈。一段时间后，心理学家评估她们之间的熟悉和喜欢的程度，结果发现：见 面的次数越多，相互喜欢的程度越大；而见面的次数越少或根本没有，相互喜欢的程度就较低。 作用 可见，若想增强人际吸引，就要留心提高自己在别人面前的熟悉度，这样可以增加别人喜欢你的程度。因此，一个自我封闭的人，或是一 个面对他人就逃避和退缩的人，由于不易让人亲近而令人费解，也就是不太让人喜欢了。想想看，你周围有没有常在你面前“露脸”的人。 如果想给别人留下不错的印象，常出现在他面前就是一个简单有效的好方法。 所以，作为大学生，如果你想改善自己的人缘，不妨多在寝室间走动一下，即使只是露个脸，借瓶开水，换本书。在这些细节的来来往往 中，就无形中提高了自己的人际吸引力，也获得了你所期待的群众基础。作为职场人士，埋头苦干也并非明智之举。自我封闭，不与人交 往，遇事逃避退缩，只会让你的职业发展之路止步不前。 当然，“多看效应”并非万能钥匙。它发挥作用的前提是，首因效应要好，即给人的第一印象不是很差，如果第一印象就很差，那么见面越 多就越惹人讨厌，“多看”反而让人“多厌”，那就得不偿失了。所以，一切的前提，还是认清自己，理顺了，再拿给别人看。
标签效应 当一个人被一种词语名称贴上标签时，他就会作出自我印象管理，使自己的行为与所贴的标签内容相一致。这种现象是由于贴上标签后面 引起的，故称为“标签效应”。 心理学认为，之所以会出现“标签效应”，主要是因为“标签”具有定性导向的作用，无论是“好”是“坏”，它对一个人的“个性意识的自我认 同”都有强烈的影响作用。给一个人“贴标签”的结果，往往是使其向“标签”所喻示的方向发展。
鸟笼效应 鸟笼效应：假如一个人买了一只空鸟笼放在家里，那么一段时间后，他一般会为了用这只笼子再买一只鸟回来养而不会把笼子丢掉，也就 是这个人反而被笼子给异化掉了，成为笼子的俘虏。 鸟笼效应是一个著名的心理现象，又称“鸟笼逻辑”，是人类难以摆脱的十大心理之一 ，其发现者是近代杰出的心理学家詹姆斯。“鸟 笼效应”是一个很有意思的规律，人们会在偶然获得一件原本不需要的物品的基础上，自觉不自觉的继续添加更多自己不需要的东西。
禁果效应 禁果效应也叫做“亚当与夏娃效应”，越是禁止的东西，人们越要得到手。越希望掩盖某个信息不让别人知道，却越勾起别人的好奇心和探 求欲，反而促使别人试图利用一切渠道来获取被掩盖的信息 。这种由于单方面的禁止和掩饰而造成的逆反现象，即心理学上的“禁果 效应” 。这与人们的好奇心与逆反心理有关。有一句谚语：“禁果格外甜”，就是这个道理。
刺猬效应 刺猬效应（Hedgehog Effect），是指刺猬在天冷时彼此靠拢取暖，但保持一定距离，以免互相刺伤的现象。这个比喻来自叔本华的哲学 著作，它强调的是人际交往中的“心理距离效应”。刺猬效应的理论可应用于多种领域。在管理实践中，就是领导者如要搞好工作，应该与 下属保持“亲密有间”的关系，即为一种不远不近的恰当合作关系。在教育学中，教育者与受教育者日常相处只有保持适当的距离，才能取 得良好的教育效果。
淬火效应 淬火效应，原意指金属工件加热到一定温度后，浸入冷却剂（油、水等）中，经过冷却处理，工件的性能更好、更稳定。心理学把这定义 为“淬火效应”。教育上也会有类似的现象，被称之为“冷处理”。 在心理学与教育学中衍生出的含义为长期受表扬头脑有些发热的学生，不妨设置一点小小的障碍，施以“挫折教育”，几经锻炼，其心理会 更趋成熟，心理承受能力会更强；对于麻烦事或者已经激化的矛盾，不妨采用“冷处理”，放一段时间，思考会更周全，办法会更稳妥。</description>
    </item>
    
    <item>
      <title>话题</title>
      <link>https://kylestones.github.io/blog/emotion/topic/</link>
      <pubDate>Mon, 05 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/emotion/topic/</guid>
      <description> 社交网络 人们总是倾向于在网络上展现理想的自我，这与真实的形象一定存在落差。很多人把这归结为欺骗和背叛。 社交网络给人太多幻想，让人们以为自己在任何情况下都会被关注。这种被关注的压力是双向的，一方面人们会为了在他人眼中表现 的更完美，而强迫自己去做很多不必要的事。在被动消费的情况下，看到更多其他人精心展现的形象之后，下意识地以更高的标准来 评估和改造自己；另一方面又会占据情感高点，看不上那些在社交媒体上不如自己有吸引力的朋友。 社交网络让每个人都更加完美了，但也更加孤独了。 友谊 相比于美丽闪耀的时刻，脆弱和尴尬才是催化友谊的重要契机，因为暴露弱点这一行为体现了信任，而彼此信任是友谊的基础。 读书 傲慢的人，最容易被收买—只要吹捧几句。傲慢是因为受尊重不足。 大仲马在《基督山伯爵》一文的最后告诉人们：人类的全部智慧都包含在两个词里面—等待与希望。 自信 从小缺乏自信的孩子有什么表现：
走路很慢，低头，遇人就缩在路里边 说话声音很小 不敢看人的脸 别人分东西时每次拿少的那份 害怕别人看到自己优秀 害怕别人看到自己出丑 害怕被人反问 在天黑没有人没有灯光没有星星和月亮的晚上特别的开心和激动，有时会偷偷地笑或者哭 试卷做完了不交，等很多人都交了才交 排队从来不敢站最前面 人多的时候尽量不动，不说话，不笑 但心里一直在渴望成为那个大家都喜欢的人 情商 真正的情商高手少不了对情绪的洞察 </description>
    </item>
    
    <item>
      <title>情话</title>
      <link>https://kylestones.github.io/blog/emotion/love/</link>
      <pubDate>Sun, 21 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/emotion/love/</guid>
      <description>摘抄的关于爱情的美句</description>
    </item>
    
    <item>
      <title>Learning GNU Emacs</title>
      <link>https://kylestones.github.io/blog/tools/emacs/</link>
      <pubDate>Mon, 02 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/tools/emacs/</guid>
      <description>1. 第一章 1.1. 输入命令 如果不知道一个命令对应的按键或者需要输入命令的时候，首先输入&amp;rsquo;ESC x&amp;rsquo;或者&amp;rsquo;META-x&amp;rsquo;，然后输入命令，接着回车即可。
1.2. 缓冲区 buffer 编辑器并不会对文件本身进行编辑，会首先将文件的内容放到一个临时性的缓冲区里（文件的一个副本），然后再对缓冲区中的东西进行编辑。缓冲区的名字通常就是正在编辑的文件的名字。 scratch是一个临时性的辅助性的缓冲区，类似草稿簿。 help是将帮助信息显示的地方
1.3. 编辑模式 Emacs有很多个主模式，且一个编辑缓冲区只能处于一个主模式，退出某个主模式的办法就是进入另一个主模式。 Emacs会依据文件名后缀或者文件的内容来判断文件的类型，从而尝试进入正确的模式。如果无法判断，会转入基本编辑模式。
模式 功能 基本模式（fundamental mode） 默认模式，无特殊行为 文本模式（text mode） 书写文字材料 （2） 邮件模式（mail mode） 书写电子邮件消息（6） RMAIL mode 阅读和组织电子邮件（6） 只读模式（view mode） 查看文件，但不进行编辑（5） shell mode 在Emacs里运行一个UNIX shell（5） FTP模式（ange-ftp mode） 下载或者查看远程系统上的文件（7） telnet mode 登陆到远程系统（7） 大纲模式（outline mode） 书写大纲（8） 缩进文本模式（indented text mode） 自动缩进文本（8） 图形模式（pictures mode） 绘制简单的线条图形（8） nroff mode 按nroff的要求对文件进行排版（9） TEX mode 按TEX的要求对文件进行排版（9） LATEX mode 按LATEX的要求对文件进行排版（9） C mode 书写C语言程序（12） C++ mode 书写C++程序（12） FORTRAN mode 书写FORTRAN程序（12） Emacs LISP mode 书写Emacs LISP程序（12） LISP mode 书写LISP程序（12） LISP 互动模式（LISP interaction mode） 书写和求值LISP表达式（12） 在主模式之外还有一些副模式（minor mode），用于定义Emacs的某些特定的行为，可以在主模式里打开或者关闭。比如自动换行模式（auto-fill mode）会使文件在一个适当的位置自动插入一个换行符。</description>
    </item>
    
    <item>
      <title>markdown 语法</title>
      <link>https://kylestones.github.io/blog/tools/markdown/</link>
      <pubDate>Thu, 02 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/tools/markdown/</guid>
      <description>如何使用Markdown 作者：三石 上面的两个等同于一级和二级标题即：# 和##
三个*, -, _均可以用于分割线
1. 基本用法 1.1 怎样使用斜体 命令： 在需要斜体的文字两边加上*或者_即可
操作系统：精髓与设计原理
深入理解操作系统
1.2 怎样使用粗体 命令：在需要斜体的文字两边加上两个*或者_即可
代码大全2
UML用户指南第二版
1.3 怎样使用粗斜体 命令：在需要斜体的文字两边加上三个*或者_即可
构建之法
编程之美
1.4 文字有背景 鸟哥的Linux私房菜
1.5 文字中间有删除线效果 Effective C++
1.6 需要阅读的书籍 设计模式 C语言程序设计 1.7 有序和无序列表 算法步骤 算法原理 实验结果 检测人脸 人脸对齐 超分重建 人脸识别 后面的即使没有按正确的顺序书写，或者使用无序的都可以（这里无序的好像不支持） 1.8 前面的符号和文字之间有5个以上的文字，字体效果会有变化 可以嵌套使用，如果想解除嵌套，需要分段 没有充分的空格的效果
1.9 链接、图片、邮箱 github上面有大量的优秀的开源代码
stackoverflow是一个优秀的程序员论坛
插入图片： 邮箱：kyleemail@163.com
1.10 代码语法高亮 #include &amp;lt;iostream&amp;gt; using namespce std; int main(){ int a = 10; unsigned int b = -5; cout &amp;lt;&amp;lt; a + b &amp;lt;&amp;lt; endl; return 0; } #!</description>
    </item>
    
    <item>
      <title>Vim summary</title>
      <link>https://kylestones.github.io/blog/tools/vim/</link>
      <pubDate>Sun, 02 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/blog/tools/vim/</guid>
      <description>1. vim的三种模式 模式可以理解为状态，不同的模式下适合处理不同的工作。vim有6种基本模式和5种派生模式，初学者只需要掌握后面三种模式即可，普通模式，编辑模式，命令行模式。 普通模式可以进行复制，删除，移动屏幕、光标，恢复、撤销等操作。有些人打开vim后敲代码，一阵忙碌之后发现似乎什么都没有打上去，或者根本跟自己想的不一样，因此变得畏惧这个编辑器，其实它只是并不处于编辑模式而已；编辑模式就是大部分编辑器的状态，可以书写程序。这个状态下vim的状态和通常的编辑器一样。命令行模式可以设置，替换，保存，退出等操作。 三种模式之间可以相互转化，打开vim后默认进入普通模式，普通模式和命令行模式可以认为是在相同的情况下状态，两者之间不需要进行转化，键入a、i、r、s、o（A、I、R、S、O）可以进入编辑模式，当然不同的字母有不同的效果(vim命令区分大小写)。编辑模式时按esc键转换到普通模式。
2. 插入文本 命令 作用 a 是append的缩写，变成编辑状态，在当前位置之后输入字符 A 在当前的行的末尾追加字符 i 是insert的缩写，从当前位置开始插入文字 I 从当前行的行首开始插入字符 s 删除当前字符并变成编辑状态 S 删除当前行并变成编辑状态 o 从当前行的下一行开始编辑 O 从当前行的上一行开始编辑 3. 移动光标 感觉学习vim最重要的是要先学会移动光标（普通模式），学会移动光标才会使这个编辑器听你的话。
3.1 最基本的当然是h、j、k、l，分别可以将光标向左、下、上、右移动一个位置。 3.2 在一行内移动光标 命令 作用 0 将光标移动到行首 ^ 移动光标到行首第一个非空字符上去 $ 移动光标到行尾 f[字符] 将光标直接定位到指定的字符，该命令只可以让光标在当前行移动。移动光标到当前位置右侧出现的第一个指定字符 F[字符] 将光标直接定位到指定的字符，该命令只可以让光标在当前行移动。移动光标到当前位置左侧出现的第一个指定字符 t[字符] 将光标直接定位到指定的字符的前面一个字符，该命令只可以让光标在当前行移动。移动光标到当前位置右侧出现的第一个指定字符前面的一个字符 T[字符] 将光标直接定位到指定的字符的前面一个字符，该命令只可以让光标在当前行移动。移动光标到当前位置左侧出现的第一个指定字符后面的一个字符 ; 配合f、F、t、T使用，重复上一个命令 , 配合f、F、t、T使用，反方向重复上一个命令 n+&amp;lt;sapce&amp;gt; 向后移动n个字符 3.3. 以一个单词的步长来移动光标 命令 作用 w 光标向右移动一个单词，光标移动到右侧或者下一行单词的第一个字母上。W以空格来区分一个单词 b 与w相反，将光标向左移动一个单词。B以空格来区分一个单词 e 光标向右移动一个单词，并且光标定位到单词的最后一个字母上。E以空格来区分一个单词 ge 与e相反，需要连续输入两个字母g和e。将光标往左移动一个字符 3.4 以&amp;rsquo;行&amp;rsquo;为步长来移动光标 命令 作用 + 移动到下一行的行首 - 移动到上一行的行首 :[行号] 光标跳转到指定行号的行首 [行号]gg 光标跳转到指定行号的行首 [行号]G 光标跳转到指定行号的行首 G 光标跳转到最后一行的行首 Ctrl+e 编辑窗口中的文件内容整体上移一行。 Ctrl+y 编辑窗口中的文件内容整体下移一行。 H 将光标移动到屏幕最上面的一行 M 将光标移动到屏幕中间的一行 L 将光标移动到屏幕最下面的一行 Z+return 将当前行（光标所在的行）移动到屏幕的第一行 Z.</description>
    </item>
    
  </channel>
</rss>
