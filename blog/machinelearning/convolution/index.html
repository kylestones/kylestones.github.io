<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>convolution | Org Mode</title><meta name=keywords content="卷积,,深度学习"><meta name=description content="cs231n 斯坦福大学的课程 CS231n: Convolutional Neural Networks for Visual Recognition 。发现写的很好，后悔没有早点看。可惜只看了 卷积这一部分。有中文翻译的部分可以只看中文总结，后面的英文原文更方便理解。
卷积 卷积神经网络明确假设输入是 images ，根据这个假设可以大量的减少参数的个数。同时有利于更 efficient 的实现。普通的神经网 络采用全连接，用于图像中会产生太多的参数，导致 overfitting 。卷积神经网络的 layers 拥有神经元的维数为 (width,height,channels) 。
A ConvNet is made up of Layers. Every Layer has a simple API: It transforms an input 3D volume to an output 3D volume with some differentiable function that may or may not have parameters.
卷积操作的本质就是滤波器和输入的部分区域做点积。卷积的反向传播也是卷积，只是做了转置。Note that the convolution operation essentially performs dot products between the filters and local regions of the input."><meta name=author content="Kyle Three Stones"><link rel=canonical href=https://kylestones.github.io/blog/machinelearning/convolution/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://kylestones.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://kylestones.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://kylestones.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://kylestones.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://kylestones.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="convolution"><meta property="og:description" content="cs231n 斯坦福大学的课程 CS231n: Convolutional Neural Networks for Visual Recognition 。发现写的很好，后悔没有早点看。可惜只看了 卷积这一部分。有中文翻译的部分可以只看中文总结，后面的英文原文更方便理解。
卷积 卷积神经网络明确假设输入是 images ，根据这个假设可以大量的减少参数的个数。同时有利于更 efficient 的实现。普通的神经网 络采用全连接，用于图像中会产生太多的参数，导致 overfitting 。卷积神经网络的 layers 拥有神经元的维数为 (width,height,channels) 。
A ConvNet is made up of Layers. Every Layer has a simple API: It transforms an input 3D volume to an output 3D volume with some differentiable function that may or may not have parameters.
卷积操作的本质就是滤波器和输入的部分区域做点积。卷积的反向传播也是卷积，只是做了转置。Note that the convolution operation essentially performs dot products between the filters and local regions of the input."><meta property="og:type" content="article"><meta property="og:url" content="https://kylestones.github.io/blog/machinelearning/convolution/"><meta property="og:image" content="https://kylestones.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2018-12-16T00:00:00+00:00"><meta property="article:modified_time" content="2018-12-16T00:00:00+00:00"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://kylestones.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="convolution"><meta name=twitter:description content="cs231n 斯坦福大学的课程 CS231n: Convolutional Neural Networks for Visual Recognition 。发现写的很好，后悔没有早点看。可惜只看了 卷积这一部分。有中文翻译的部分可以只看中文总结，后面的英文原文更方便理解。
卷积 卷积神经网络明确假设输入是 images ，根据这个假设可以大量的减少参数的个数。同时有利于更 efficient 的实现。普通的神经网 络采用全连接，用于图像中会产生太多的参数，导致 overfitting 。卷积神经网络的 layers 拥有神经元的维数为 (width,height,channels) 。
A ConvNet is made up of Layers. Every Layer has a simple API: It transforms an input 3D volume to an output 3D volume with some differentiable function that may or may not have parameters.
卷积操作的本质就是滤波器和输入的部分区域做点积。卷积的反向传播也是卷积，只是做了转置。Note that the convolution operation essentially performs dot products between the filters and local regions of the input."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Blogs","item":"https://kylestones.github.io/blog/"},{"@type":"ListItem","position":3,"name":"convolution","item":"https://kylestones.github.io/blog/machinelearning/convolution/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"convolution","name":"convolution","description":"cs231n 斯坦福大学的课程 CS231n: Convolutional Neural Networks for Visual Recognition 。发现写的很好，后悔没有早点看。可惜只看了 卷积这一部分。有中文翻译的部分可以只看中文总结，后面的英文原文更方便理解。\n卷积 卷积神经网络明确假设输入是 images ，根据这个假设可以大量的减少参数的个数。同时有利于更 efficient 的实现。普通的神经网 络采用全连接，用于图像中会产生太多的参数，导致 overfitting 。卷积神经网络的 layers 拥有神经元的维数为 (width,height,channels) 。\nA ConvNet is made up of Layers. Every Layer has a simple API: It transforms an input 3D volume to an output 3D volume with some differentiable function that may or may not have parameters.\n卷积操作的本质就是滤波器和输入的部分区域做点积。卷积的反向传播也是卷积，只是做了转置。Note that the convolution operation essentially performs dot products between the filters and local regions of the input.","keywords":["卷积,","深度学习"],"articleBody":" cs231n 斯坦福大学的课程 CS231n: Convolutional Neural Networks for Visual Recognition 。发现写的很好，后悔没有早点看。可惜只看了 卷积这一部分。有中文翻译的部分可以只看中文总结，后面的英文原文更方便理解。\n卷积 卷积神经网络明确假设输入是 images ，根据这个假设可以大量的减少参数的个数。同时有利于更 efficient 的实现。普通的神经网 络采用全连接，用于图像中会产生太多的参数，导致 overfitting 。卷积神经网络的 layers 拥有神经元的维数为 (width,height,channels) 。\nA ConvNet is made up of Layers. Every Layer has a simple API: It transforms an input 3D volume to an output 3D volume with some differentiable function that may or may not have parameters.\n卷积操作的本质就是滤波器和输入的部分区域做点积。卷积的反向传播也是卷积，只是做了转置。Note that the convolution operation essentially performs dot products between the filters and local regions of the input. The backward pass for a convolution operation (for both the data and the weights) is also a convolution (but with spatially-flipped filters).\n卷积输出的每个元素都是输入的一个区域和滤波器逐元素相乘并求和，然后再加上一个 bias 得到的。The visualization below iterates over the output activations (green), and shows that each element is computed by elementwise multiplying the highlighted input (blue) with the filter (red), summing it up, and then offsetting the result by the bias.\n卷积层特性： 输入维度 \\(W_1 * H_1 * D_1\\) 四个超参： filters number K , fileter spatial size F , stride S , padding P 输出维度 \\(W_2 * H_2 * D_2\\) , 其中 \\(W_2=(W_1-F+2P)/S+1, \\ D_2=K\\) 由于参数共享，每个 filter 有 \\(F \\cdot F \\cdot D_1\\) 个权重，整个卷积层有 \\((F \\cdot F \\cdot D_1) \\cdot K\\) 个权重和 \\(K\\) 个 biases 由 d-th 个 filter 使用 stride S 和 d-th 个 bias 生成输出的 d-th 通道（ size \\(W_2*H_2\\) ） Converting FC layers to CONV layers 卷积层和全连接层可以相互转化。It is worth noting that the only difference between FC and CONV layers is that the neurons in the CONV layer are connected only to a local region in the input, and that many of the neurons in a CONV volume share parameters. However, the neurons in both layers still compute dot products, so their functional form is identical. Therefore, it turns out that it’s possible to convert between FC and CONV layers:\nFor any CONV layer there is an FC layer that implements the same forward function. The weight matrix would be a large matrix that is mostly zero except for at certain blocks (due to local connectivity) where the weights in many of the blocks are equal (due to parameter sharing). Conversely, any FC layer can be converted to a CONV layer. For example, an FC layer with K=4096 that is looking at some input volume of size 7×7×512 can be equivalently expressed as a CONV layer with F=7,P=0,S=1,K=4096. In other words, we are setting the filter size to be exactly the size of the input volume, and hence the output will simply be 1×1×4096 since only a single depth column “fits” across the input volume, giving identical result as the initial FC layer. Evaluating the original ConvNet (with FC layers) independently across 224x224 crops of the 384x384 image in strides of 32 pixels gives an identical result to forwarding the converted ConvNet one time.\n将图片放大，然后输入网络，相当于 crop 原图像大小的输入，然后在相应的位置求平均，来得到更好的结果，这样比 crop 多次，然后 在卷积多次要快的多，因为这样会有很多重复的计算：Naturally, forwarding the converted ConvNet a single time is much more efficient than iterating the original ConvNet over all those 36 locations, since the 36 evaluations share computation. This trick is often used in practice to get better performance, where for example, it is common to resize an image to make it bigger, use a converted ConvNet to evaluate the class scores at many spatial positions and then average the class scores.\n没懂下面的内容：通过执行两次卷积，以原图像和移位 16 个像素的图像分别作为输入，可以将本来缩小比例为 32 的网络，求解出缩小 比例为 16 的网络？ Lastly, what if we wanted to efficiently apply the original ConvNet over the image but at a stride smaller than 32 pixels? We could achieve this with multiple forward passes. For example, note that if we wanted to use a stride of 16 pixels we could do so by combining the volumes received by forwarding the converted ConvNet twice: First over the original image and second over the image but with the image shifted spatially by 16 pixels along both width and height.\nPadding 使用 padding 保持卷积之后，图像的尺寸不变，网络的表现会更好。valid 卷积无法充分获取图像边界的信息。 Why use padding? In addition to the aforementioned benefit of keeping the spatial sizes constant after CONV, doing this actually improves performance. If the CONV layers were to not zero-pad the inputs and only perform valid convolutions, then the size of the volumes would reduce by a small amount after each CONV, and the information at the borders would be “washed away” too quickly.\n使用矩阵乘法实现卷积 caffe 中的 im2col 实现图像卷积展开成矩阵。由于同一个值需要存储在矩阵的不同位置，会消耗大量的内存。但可以利用现有的矩阵乘 法库（BLAS）来加速计算。\nImplementation as Matrix Multiplication. Note that the convolution operation essentially performs dot products between the filters and local regions of the input. A common implementation pattern of the CONV layer is to take advantage of this fact and formulate the forward pass of a convolutional layer as one big matrix multiply as follows:\nThe local regions in the input image are stretched out into columns in an operation commonly called im2col. For example, if the input is [227x227x3] and it is to be convolved with 11x11x3 filters at stride 4, then we would take [11x11x3] blocks of pixels in the input and stretch each block into a column vector of size 11*11*3 = 363. Iterating this process in the input at stride of 4 gives (227-11)/4+1 = 55 locations along both width and height, leading to an output matrix X_col of im2col of size [363 x 3025], where every column is a stretched out receptive field and there are 55*55 = 3025 of them in total. Note that since the receptive fields overlap, every number in the input volume may be duplicated in multiple distinct columns. The weights of the CONV layer are similarly stretched out into rows. For example, if there are 96 filters of size [11x11x3] this would give a matrix W_row of size [96 x 363]. The result of a convolution is now equivalent to performing one large matrix multiply np.dot(W_row, X_col), which evaluates the dot product between every filter and every receptive field location. In our example, the output of this operation would be [96 x 3025], giving the output of the dot product of each filter at each location. The result must finally be reshaped back to its proper output dimension [55x55x96]. This approach has the downside that it can use a lot of memory, since some values in the input volume are replicated multiple times in X_col. However, the benefit is that there are many very efficient implementations of Matrix Multiplication that we can take advantage of (for example, in the commonly used BLAS API). Moreover, the same im2col idea can be reused to perform the pooling operation, which we discuss next.\nPooling Layer Pooling 可以大大较少参数的个数，防止过拟合。 It is common to periodically insert a Pooling layer in-between successive Conv layers in a ConvNet architecture. Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network, and hence to also control overfitting.\nThe Pooling Layer operates independently on every depth slice of the input and resizes it spatially, using the MAX operation. 特性：\n两个超参： spatial extent F, stride S zero padding 反向传播 前向传播的时候记录 max value 的索引，方便反向传播计算。 max pooling 反向传播时，先将 feature maps 上采样， 将梯度值传入 max value 索引的地方，其他位置的值均赋值为 0 。avera pooling 则将梯度值除以扩大的倍数，并将平均值 分别填入每一个位置。总之确保 loss 值反向传播中大小和保持不变。 摒弃 pooling 在 variational autoencoders (VAEs) or generative adversarial networks (GANs) 中不使用 pooling 对于网络 的训练很重要。可以使用 stride 较大的卷积替代。 Pooling sizes with larger receptive fields are too destructive.\nPooling 可以使用任意函数，只是 max 使用比较多。 In addition to max pooling, the pooling units can also perform other functions, such as average pooling or even L2-norm pooling.\nAverage pooling 表现不如 max pooling 而逐渐被抛弃。 Average pooling was often used historically but has recently fallen out of favor compared to the max pooling operation, which has been shown to work better in practice.\nLayer Patterns INPUT -\u003e [[CONV -\u003e RELU]*N -\u003e POOL?]*M -\u003e [FC -\u003e RELU]*K -\u003e FC\nHere we see two CONV layers stacked before every POOL layer. This is generally a good idea for larger and deeper networks, because multiple stacked CONV layers can develop more complex features of the input volume before the destructive pooling operation.\n堆叠小尺寸的卷积核，相比于直接只用一个大尺寸的卷积核，可以得到更好的特征，且只需要更好的参数。但是需要更多的内存来存储中 间的卷积层。 Intuitively, stacking CONV layers with tiny filters as opposed to having one CONV layer with big filters allows us to express more powerful features of the input, and with fewer parameters. As a practical disadvantage, we might need more memory to hold all the intermediate CONV layer results if we plan to do backpropagation.\n从输入到输出只有一单条线路的卷积神经网络，正在被更复杂的网络结构所替代 GoogLeNet, ResNet, DenseNet 等。It should be noted that the conventional paradigm of a linear list of layers has recently been challenged, in Google’s Inception architectures and also in current (state of the art) Residual Networks from Microsoft Research Asia. Both of these (see details below in case studies section) feature more intricate and different connectivity structures.\n尽量使用已有的网络模型，并在自己的数据集上 finetune ，而不是从头开始设计。 In practice: use whatever works best on ImageNet. If you’re feeling a bit of a fatigue in thinking about the architectural decisions, you’ll be pleased to know that in 90% or more of applications you should not have to worry about these. I like to summarize this point as “don’t be a hero”: Instead of rolling your own architecture for a problem, you should look at whatever architecture currently works best on ImageNet, download a pretrained model and finetune it on your data. You should rarely ever have to train a ConvNet from scratch or design one from scratch. I also made this point at the Deep Learning school.\nLayer Sizing Patterns 输入图像的大小最好可以多次被 2 整除。 The input layer (that contains the image) should be divisible by 2 many times. Common numbers include 32 (e.g. CIFAR-10), 64, 96 (e.g. STL-10), or 224 (e.g. common ImageNet ConvNets), 384, and 512.\n使用 3*3 的卷积可以有效减小参数的个数，需要增大感受野可以堆叠多个 3*3 卷积。 The conv layers should be using small filters (e.g. 3x3 or at most 5x5), using a stride of S=1, and crucially, padding the input volume with zeros in such way that the conv layer does not alter the spatial dimensions of the input. That is, when F=3, then using P=1 will retain the original size of the input. When F=5, P=2. For a general F, it can be seen that P=(F−1)/2 preserves the input size. If you must use bigger filter sizes (such as 7x7 or so), it is only common to see this on the very first conv layer that is looking at the input image.\nPooling 层有较大的破坏性，一般不会使用较大的尺寸。The pool layers are in charge of downsampling the spatial dimensions of the input. The most common setting is to use max-pooling with 2x2 receptive fields (i.e. F=2), and with a stride of 2 (i.e. S=2). Note that this discards exactly 75% of the activations in an input volume (due to downsampling by 2 in both width and height). Another slightly less common setting is to use 3x3 receptive fields with a stride of 2, but this makes. It is very uncommon to see receptive field sizes for max pooling that are larger than 3 because the pooling is then too lossy and aggressive. This usually leads to worse performance.\n卷积层只改变通道数，不修改图像的大小。POOL 负责减小图像的大小。 Reducing sizing headaches. The scheme presented above is pleasing because all the CONV layers preserve the spatial size of their input, while the POOL layers alone are in charge of down-sampling the volumes spatially. In an alternative scheme where we use strides greater than 1 or don’t zero-pad the input in CONV layers, we would have to very carefully keep track of the input volumes throughout the CNN architecture and make sure that all strides and filters “work out”, and that the ConvNet architecture is nicely and symmetrically wired.\n卷积步长选择为 1 效果较好。 Why use stride of 1 in CONV? Smaller strides work better in practice. Additionally, as already mentioned stride 1 allows us to leave all spatial down-sampling to the POOL layers, with the CONV layers only transforming the input volume depth-wise.\n由于内存的限制，会首先大幅度减小图像的尺寸。Compromising based on memory constraints. In some cases (especially early in the ConvNet architectures), the amount of memory can build up very quickly with the rules of thumb presented above. For example, filtering a 224x224x3 image with three 3x3 CONV layers with 64 filters each and padding 1 would create three activation volumes of size [224x224x64]. This amounts to a total of about 10 million activations, or 72MB of memory (per image, for both activations and gradients). Since GPUs are often bottlenecked by memory, it may be necessary to compromise. In practice, people prefer to make the compromise at only the first CONV layer of the network. For example, one compromise might be to use a first CONV layer with filter sizes of 7x7 and stride of 2 (as seen in a ZF net). As another example, an AlexNet uses filter sizes of 11x11 and stride of 4.\n论文的部分翻译，github 地址 ，由于不是很理解转置卷积和空洞卷积，所以看了看这篇文章，翻译一下以备忘。相比于斯坦福大学的课 程，感觉这篇论文有点故弄玄虚，不符合大道至简呀【微吐槽】。\nIntroduction CNNs 在深度学习中处于核心角色，但通常第一次使用 CNNs 却是一场噩梦。因为一个卷积层输出的大小会由输入的大小、卷积核的尺寸、 padding 的大小、stride 共同决定，而且相互之间的关心有点复杂。相对而言，全连接层的输出的大小和输入的大小并没有直接关系。 同时 CNNs 经常会引入 pooling层，进一步增加了复杂性。另外转置卷积也在被更频繁的使用。\n这个手册有两个目的：\n解释卷积和转置卷积的相互关系 对卷积层、pooling 层、转置卷积层中 input shape、kernel shape、zero padding、strides、output shape 的相互关系有一个直 观的理解 Discrete convolutions 神经网络的必备元素是仿射变换 ： 输入是一个向量，乘以一个矩阵，在加上一个偏移得到输出。这适用于任何形式的输入（图片、音频 片段、无序特征；不管维数是多少都可以在转换前变成一个向量）\n图片、音频片段等数据都有内在结构，一般来说，他们有以下重要的属性：\n使用多维数组存储。 [They are stored as multi-dimensional arrays.] 特征在一个维度或者多个维度的存储顺序有影响。 [They feature one or more axes for which ordering matters (e.g., width and height axes for an image, time axis for a sound clip).] channel 维度表示的是数据的不同 view 。[One axis, called the channel axis, is used to access different views of the data (e.g., the red, green and blue channels of a color image, or the left and right channels of a stereo audio track).] 当使用仿射变换的时候，这些结构信息将无法被有效利用。因为所有轴上的数据都被统一对待，根本没有考虑数据的拓扑信息（直接将数 据转换成了一个向量）。而在计算机视觉和语音识别中最好保留这些数据的结构，因此有了离散卷积 (discrete convolution)。\n离散卷积是一个线性变换，保留了数据的顺序，采用稀疏连接和权值共享。\n卷积核扫描输入，在每一个位置上，卷积核和其覆盖的输入区域逐元素相乘并求和得到该位置的输出。可以重复执行这个过程来得到多个 输出，这些生成的输出称为 feature maps 。如果输入是多个 feature maps ，卷积核需要是三维的，卷积核 channel 的个数应该等于 输入 feature maps channel 的个数，输入的每一个 feature map 都需要一个特定的卷积核来扫描，得到的多个结果（不同的 channel ）逐元素相加得到输出的一个 feature map 。\nstrides 表示的是 kernel 移动的步长，也可以看做是保留步长为 1 卷积输出的某些部分。比如步长为 2 的卷积输出可以看做，步长为 1 的卷积得到的输出中只保留奇数位元素（从 1 开始计数）得到的结果。\npooling pooling 是 CNNs 的另一个关键部件。用于减小 feature maps 的尺寸，通常有 average pooling 和 max pooling 两种。 Pooling 和 卷积很像（我感觉就是一种特殊的卷积呀！），同样会扫描输入，但是将 kernel 换成了 pooling function 。\nConvolution arithmetic 下面的分析依赖于卷积不会跨越不同轴的特性；[i.e., the choice of kernel size, stride and zero padding along axis j only affects the output size of axis j.]\nHalf (same) padding 保持输出和输入的尺寸不变时使用，需要填充的个数 \\( p = \\lfloor k / 2 \\rfloor \\) 。\nFull padding 卷积通常会较小输出的尺寸，但有时候需要增大输出的尺寸（这个不是转置卷积的任务吗），此时可以用 full padding 。此时填充的个 数 \\( p = k - 1 \\) ，这里 full padding 的意思是让输入的每个像素点对 kernel 都有相同的地位。\nZero padding, non-unit strides 卷积输出大小的通用公式 \\[ o = \\lfloor \\frac{i + 2p - k}{s} \\rfloor + 1 \\]\n这里向下取整，是因为如果剩余的区域（含 padding ）不足一个 kernel 的长度时，不再继续执行卷积。\nPooling arithmetic pooling 层使得网络对输入的微小偏移不敏感。max pooling 使用较频繁： 将输入分割成 patches （通常不会重叠） ，并输出每个 patch 内的最大值；另外还有 average pooling ，输出每个 patch 内的平均值。\n计算卷积输出大小的公式同样适用于 pooling ，但是 Pooling 并不会 padding ，所以 Pooling 的通用输出公式为\n\\[ o = \\lfloor \\frac{i - k}{s} \\rfloor + 1 \\]\nTransposed convolution arithmetic 转置卷积一般用于常规卷积的反方向操作，将常规卷积的输出转换到输入，同时保持和常规卷积的兼容性。例如，可以用于 autoencoder 中的 decoding layer 或者将 feature map 变换到更高维的空间中。全连接层比较简单，只需要利用权重矩阵的转置就可以实现反向操 作。卷积层会比全连接层复杂很多，不过可以将卷积层归结为高效实现的矩阵操作，洞悉全连接层将有助于解决卷积。如同卷积操作不会 跨越不同的坐标轴（不会跨越 channel），转置卷积也有相同的属性，这将简化转置卷积算法。\n下文重点关注;\n2-D 转置卷积 (N=2) square inputs \\((i_1 = i_2 = i)\\) square kernel size \\((k_1 = k_2 = k)\\) 所有方向使用相同的 strides \\((s_1 = s_2 = s)\\) 所有 axes 使用相同大小的 padding \\((p_1 = p_2 = p)\\) 这些结果都可以推广到 N-D 和 non-square 情形\n转置卷积等效的卷积输出大小同样可以用上面卷积中的通用公式 \\[ o = \\lfloor \\frac{i + 2p - k}{s} \\rfloor + 1 \\] 只是这里的 参数需要正确对应。\n卷积视为矩阵操作 当 \\(i = 4, k = 3, s = 1, p = 0\\) 时，如果将 input 和 output 从左到右，由上到下展开成向量，卷积可以表示成一个稀疏的矩阵 C ，其中非 0 的元素是 kernel 中相应的元素，其中 \\(i\\) 和 \\(j\\) 分别表示 kernel 的行和列\n\\begin{equation*} ≤ft( \\begin{array}{cccccccccccccccc} w_{0,0} \u0026 w_{0,1} \u0026 w_{0,2} \u0026 0 \u0026 w_{1,0} \u0026 w_{1,1} \u0026 w_{1,2} \u0026 0 \u0026 w_{2,0} \u0026 w_{2,1} \u0026 w_{2,2} \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \\\\ 0 \u0026 w_{0,0} \u0026 w_{0,1} \u0026 w_{0,2} \u0026 0 \u0026 w_{1,0} \u0026 w_{1,1} \u0026 w_{1,2} \u0026 0 \u0026 w_{2,0} \u0026 w_{2,1} \u0026 w_{2,2} \u0026 0 \u0026 0 \u0026 0 \u0026 0 \\\\ 0 \u0026 0 \u0026 0 \u0026 0 \u0026 w_{0,0} \u0026 w_{0,1} \u0026 w_{0,2} \u0026 0 \u0026 w_{1,0} \u0026 w_{1,1} \u0026 w_{1,2} \u0026 0 \u0026 w_{2,0} \u0026 w_{2,1} \u0026 w_{2,2} \u0026 0 \\\\ 0 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u0026 w_{0,0} \u0026 w_{0,1} \u0026 w_{0,2} \u0026 0 \u0026 w_{1,0} \u0026 w_{1,1} \u0026 w_{1,2} \u0026 0 \u0026 w_{2,0} \u0026 w_{2,1} \u0026 w_{2,2} \\end{array} \\right) \\end{equation*}\n这种线性操作将输入矩阵展开成 16 维的向量，同时生成一个 4 维的向量，然后 reshape 成 2x2 的输出矩阵。\n使用这种方法表示时，反向传播可以很容易通过矩阵 C 的转置矩阵实现，即 loss 乘以 \\(C^T\\) 来反向传播误差。将一个 4 维的向量 作为输入，得到一个 16 维的输出，并且其连接模式（感觉应该是输入输出矩阵展开成向量的方式）和矩阵 C 兼容。\n值得注意的是， kernel W 同时定义了用于前向传播矩阵 C ，也定义了反向传播矩阵 \\(C^T\\) 。\n转置卷积 转置卷积 (transposed convolution) 也叫做小数步长卷积 (fractionally strided convolutions) 或 deconvolutions [学术上可能使 用这个名称，但是在数学上 deconvolution 表示逆矩阵，和矩阵的转置并不一样] 。kernel 定义了矩阵 W ，可以认为是常规卷积，在 前向传播时乘以矩阵 C ，反向传播时乘以矩阵 \\(C^T\\) ；也可以看做转置卷积，在前向传播时乘以矩阵 \\(C^T\\) ，反向传播时乘以 \\((C^T)^T = C\\) ，所以 一个转置卷积总是可以对应到一个常规卷积，但需要对输入增加许多无效的 0 值（额外的值全为 0 的行和 列），导致效率低下 。\n实际代码实现时将转置卷积看做，将梯度作为输入的常规卷积？？？ TODO [The transposed convolution operation can be thought of as the gradient of some convolution with respect to its input, which is usually how transposed convolutions are implemented in practice.]\nNo zero padding, unit strides, transposed 思考转置卷积最简单的方法是，将输入看做常规卷积的某一层输出的 feature map ，通过转置卷积来恢复该层的输入（由于转置卷积并 不是卷积的逆，无法原值恢复输入，仅仅可以恢复相同宽和高的输入）。\n\\(i=4, k=3, s=1, p=0\\) 的常规卷积得到一个 2x2 的输出，转置卷积将用 2x2 的输入得到 4x4 的输出。\n另一种得到转置卷积输出的方法（第一种方法在哪里？）：利用常规卷积来实现： \\(i'=2, k'=k, s'=1, p'=2\\) 。可以看到卷积核和步 长保持不变，但是增加了 padding 。这是一个低效的方法，这里只是为了阐述方便，实际实现中不会计算那些无效的 0 值。\n通过卷积的连接方式（connectivity pattern : 输入、输出的大小），理解转置卷积的等效常规卷积需要增加 padding 的背后逻辑，并 用其来指导设计等效的卷积。如，输入左上角的 pixel 只会影响输出的左上角，输入的右上角的像素只会作用于输出的右上角，其他的 位置类似。\n为了保持相同的连接方式，需要 zero pad ，使得第一个卷积（最左上角的卷积）只与输入的最左上角的像素相乘（kernel 其他位置与 padding 的 0 值相乘），也就是让 \\(p = k -1 \\) 。\n\\(s=1, p=0, k\\) 卷积的转置，等效于卷积 \\(k'=k, s'=s, p'=k-1\\) ，且其输出为 \\[ o' = i' + (k - 1) \\] 有趣的是，这里等效的常规卷积是 full padding 、单位步长的卷积。\ni'=2, k'=3, s'=1, p'=2 动图\nZero padding, unit strides, transposed 既然知道无 padding 的卷积的转置等效于让 input 增加 padding 的常规卷积，当 input 有 padding 的时候，等效的转置卷积输入会有较 少个数的 padding 。因为常规卷积输入 padding 的值并非真正有效的值（但是参与了运算），而当前面的输入需要作为转置卷积的输出 的时候，输出的大小的等效值变小了（padding 并不需要输出），从而转置卷积的输入也就需要较小的 padding 。\n\\(i=5, k=4, p=2\\) 卷积的转置，等效于 \\(k'=k, s'=s, p'=k-p-1\\) 的卷积，且输出大小为 \\[o' = i' + (k-1) -2p\\] Half(same) padding, transposed 由于 same padding 的常规卷积，输出的大小等于输入的大小；因此 same padding 常规卷积的转置的等效常规卷积是其本身，也是 一个 same padding 的卷积。即 \\(k=2n+1, s=1, p= \\lfloor k/2 \\rfloor = n\\) 卷积的转置等效于 \\(k'=k, s'=s, p'=p\\) ，且其输 出大小为\n\\begin{align*} o' \u0026= i' + (k-1) -2p \u0026= i' + 2n -2n \u0026= i' \\end{align*}\n动图\nFull padding, transposed non-padded 卷积的转置等效于一个 full padding 的卷积，理所当然，一个 full padding 卷积的转置等效于一个 non-padded 卷积。 两者互为输入输出关系。\n\\(s=1, k, p=k-1\\) 卷积的转置等效于 \\(k'=k, s'=s, p'=0\\) 的卷积，且输出大小为 \\begin{align*} o' \u0026= i' + (k-1) -2p \u0026= i' - (k-1) \\end{align*}\n动图\nNo zero padding, non-unit strides, transposed 步长 \\(s \u003e 1\\) 卷积的转置，等效的卷积步长 \\(s \u003c 1\\) ，这也是转置卷积被称为小数步长卷积 fractionally strided convolutions 的原因。当卷积的步长 \\(s\u003e1\\) 时，其转置等效的卷积需要在 input 的每两个 pixel 之间插入 \\(s-1\\) 的 0 值，从而使得 kernel 的移动步伐慢于 unit strides 的卷积。当然这里只是为了便于理解，实际代码实现肯定不会去让那些无效的 0 值去做乘法。\n\\(p=0, k, s\\) 且 \\(i-k\\) 可以整除 \\(s\\) 卷积的转置的等效卷积为 \\(\\tilde{i}', k'=k, s'=1, p'=k-1\\) ，其中 \\(\\tilde{i}'\\) 表示在 input 的两个 pixel 之间插入 \\(s-1\\) 个 0 值，且其输出为 \\[o' = s(i'-1) + k\\]\n动图\nZero padding, non-unit strides, transposed \\(k, s, p\\) 且 \\(i+2p-k\\) 可以整除 \\(s\\) 卷积的转置，等效的卷积为 \\(\\tilde{i}', k'=k, s'=1, p'=k-p-1\\) ，其中 \\(\\tilde{i}'\\) 表明在 input 的每两个 pixel 之间插入 \\(s-1\\) 个 0 值，且其输出为 \\[o'=s(i'-1)+k-2p\\]\n动图\n\\(k,s,p\\) 卷积的转置，等效的卷积为 \\(a, \\tilde{i}', k'=k, s'=1, p'=k-p-1\\) 其中 \\(\\tilde{i}'\\) 表示在 input 的每两个 pixel 之间插入 \\(s-1\\) 个 0 值，\\(a=(i+2p-k) \\ mod \\ s \\) 表示在 input 的底部和右侧需要添加的 0 值的个数，且其输出为 \\[o'=s(i'-1)+a+k-2p\\] 动图\n转置卷积等效卷积参数的通用规则 \\begin{align*} p' \u0026= k -p -1 s' \u0026= 1 k' \u0026= k \u0026 \\text{adding s-1 zeros between each input unit} \\end{align*}\nMiscellaneous convolutions Dilated convolutions Dilated convolutions 通过在 kernel 相邻的两个元素之间插入空隙来扩充 kernel 。扩大比例由超参 d 来决定，即在 kernel 的相邻 元素之间插入 d-1 个 0 值后，执行常规卷积操作。\nDilated convolutions 是不增大卷积核的尺寸，同时可以扩大感受野的有效方法，特别是堆叠使用多层的 dilated convolutions 的时 候。\n扩充后的卷积核的尺寸为 \\[ \\hat{k} = k + (k-1)(d-1) \\]\n参考常规卷积输出的通用公式，利用扩充后卷积核的大小，可以得到 Dilated convolutions 输出的大小 \\[ o = \\lfloor \\frac{i + 2p - k - (k-1)(d-1) }{s} \\rfloor + 1 \\]\n这个将导致输出的 size 减小\n读后感 论文中解释了一些自己原来不清楚的知识点，如 full padding 的含义、 感觉论文没有很清楚的解释，只是单纯的列举出了各种情况下卷积和转置卷积，以及相互对应关系 我还是不清楚实际中是怎样实现转置卷积以及空洞卷积的，当时就是看 DarkNet 的代码没看懂才来阅读的！！！转置卷积是通过将输 入输出展开成矩阵，然后使用矩阵乘法来实现？还是转换成等效的卷积实现？ 读完之后没有那种豁然开朗的感觉 Deformable Convolutions NetWorks 正常的 3x3 卷积，首先使用一个规则的区域 R 采集输入的 feature maps X ，然后乘以权重 w 。\n\\[ \\mathit{R} = \\{(-1,-1),(-1,0), \\ldots, (0,1), (1,1) \\]\n\\[ y(P_0) = \\sum_{P_n \\in R} W \\cdot X(P_0 + P_n) \\]\n而可变形卷积就是将输入进行添加 offset \\(\\{\\triangle P_n | n = 1,\\ldots, N \\} \\ where \\ N = | R |\\) ，相应的卷积公式变成\n\\[ y(P_0) = \\sum_{P_n \\ in R} W \\cdot X(P_0 + P_n + \\triangle P_n) \\]\n因为每个输入的位移都是二维的，所以可以在二维的 feature map 上任意方向上移动。\n又由于增加 offset 的点可能不对应整数坐标，此时通过双线性差值得到偏移后的输入值。\nMxNet 有可变形卷积的接口，不过记得可变形卷积官方代码说 MxNet 实现的有问题？\nmxnet.ndarray.contrib.DeformableConvolution ","wordCount":"3189","inLanguage":"en","datePublished":"2018-12-16T00:00:00Z","dateModified":"2018-12-16T00:00:00Z","author":{"@type":"Person","name":"Kyle Three Stones"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://kylestones.github.io/blog/machinelearning/convolution/"},"publisher":{"@type":"Organization","name":"Org Mode","logo":{"@type":"ImageObject","url":"https://kylestones.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://kylestones.github.io accesskey=h title="Home (Alt + H)"><img src=https://kylestones.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://kylestones.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://kylestones.github.io/tags/ title=tags><span>tags</span></a></li><li><a href=https://example.org title=example.org><span>example.org</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://kylestones.github.io>Home</a>&nbsp;»&nbsp;<a href=https://kylestones.github.io/blog/>Blogs</a></div><h1 class=post-title>convolution</h1><div class=post-meta><span title='2018-12-16 00:00:00 +0000 UTC'>December 16, 2018</span>&nbsp;·&nbsp;15 min&nbsp;·&nbsp;3189 words&nbsp;·&nbsp;Kyle Three Stones&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/blog/machinelearning/convolution.org rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><div id=outline-container-headline-1 class=outline-3><h3 id=headline-1><a href=http://cs231n.github.io/convolutional-networks/>cs231n</a></h3><div id=outline-text-headline-1 class=outline-text-3><p>斯坦福大学的课程 <a href=http://cs231n.stanford.edu/>CS231n: Convolutional Neural Networks for Visual Recognition</a> 。发现写的很好，后悔没有早点看。可惜只看了
卷积这一部分。有中文翻译的部分可以只看中文总结，后面的英文原文更方便理解。</p></div></div><div id=outline-container-headline-2 class=outline-3><h3 id=headline-2>卷积</h3><div id=outline-text-headline-2 class=outline-text-3><p>卷积神经网络明确假设输入是 <strong>images</strong> ，根据这个假设可以大量的减少参数的个数。同时有利于更 efficient 的实现。普通的神经网
络采用全连接，用于图像中会产生太多的参数，导致 <strong>overfitting</strong> 。卷积神经网络的 layers 拥有神经元的维数为
(width,height,channels) 。</p><p>A ConvNet is made up of Layers. Every Layer has a simple API: It transforms an input 3D volume to an output 3D volume
with some differentiable function that may or may not have parameters.</p><p>卷积操作的本质就是滤波器和输入的部分区域做点积。卷积的反向传播也是卷积，只是做了转置。Note that the convolution
operation essentially performs dot products between the filters and local regions of the input. The backward pass for a
convolution operation (for both the data and the weights) is also a convolution (but with spatially-flipped filters).</p><p>卷积输出的每个元素都是输入的一个区域和滤波器逐元素相乘并求和，然后再加上一个 bias 得到的。The visualization below
iterates over the output activations (green), and shows that each element is computed by elementwise multiplying the
highlighted input (blue) with the filter (red), summing it up, and then offsetting the result by the bias.</p><div id=outline-container-headline-3 class=outline-4><h4 id=headline-3>卷积层特性：</h4><div id=outline-text-headline-3 class=outline-text-4><ol><li>输入维度 \(W_1 * H_1 * D_1\)</li><li>四个超参： filters number K , fileter spatial size F , stride S , padding P</li><li>输出维度 \(W_2 * H_2 * D_2\) , 其中 \(W_2=(W_1-F+2P)/S+1, \ D_2=K\)</li><li>由于参数共享，每个 filter 有 \(F \cdot F \cdot D_1\) 个权重，整个卷积层有 \((F \cdot F \cdot D_1) \cdot K\) 个权重和
\(K\) 个 biases</li><li>由 d-th 个 filter 使用 stride S 和 d-th 个 bias 生成输出的 d-th 通道（ size \(W_2*H_2\) ）</li></ol></div></div><div id=outline-container-headline-4 class=outline-4><h4 id=headline-4>Converting FC layers to CONV layers</h4><div id=outline-text-headline-4 class=outline-text-4><p>卷积层和全连接层可以相互转化。It is worth noting that the only difference between FC and CONV layers is that the neurons
in the CONV layer are connected only to a local region in the input, and that many of the neurons in a CONV volume share
parameters. However, the neurons in both layers still compute dot products, so their functional form is identical.
Therefore, it turns out that it’s possible to convert between FC and CONV layers:</p><ol><li>For any CONV layer there is an FC layer that implements the same forward function. The weight matrix would be a large
matrix that is mostly zero except for at certain blocks (due to local connectivity) where the weights in many of the
blocks are equal (due to parameter sharing).</li><li>Conversely, any FC layer can be converted to a CONV layer. For example, an FC layer with K=4096 that is looking at
some input volume of size 7×7×512 can be equivalently expressed as a CONV layer with F=7,P=0,S=1,K=4096. In other
words, we are setting the filter size to be exactly the size of the input volume, and hence the output will simply be
1×1×4096 since only a single depth column “fits” across the input volume, giving identical result as the initial FC
layer.</li></ol><p>Evaluating the original ConvNet (with FC layers) independently across 224x224 crops of the 384x384 image in strides of
32 pixels gives an identical result to forwarding the converted ConvNet one time.</p><p>将图片放大，然后输入网络，相当于 crop 原图像大小的输入，然后在相应的位置求平均，来得到更好的结果，这样比 crop 多次，然后
在卷积多次要快的多，因为这样会有很多重复的计算：Naturally, forwarding the converted ConvNet a single time is much more
efficient than iterating the original ConvNet over all those 36 locations, since the 36 evaluations share computation.
This trick is often used in practice to get better performance, where for example, it is common to resize an image to
make it bigger, use a converted ConvNet to evaluate the class scores at many spatial positions and then average the
class scores.</p><p>没懂下面的内容：通过执行两次卷积，以原图像和移位 16 个像素的图像分别作为输入，可以将本来缩小比例为 32 的网络，求解出缩小
比例为 16 的网络？ Lastly, what if we wanted to efficiently apply the original ConvNet over the image but at a stride
smaller than 32 pixels? We could achieve this with multiple forward passes. For example, note that if we wanted to use a
stride of 16 pixels we could do so by combining the volumes received by forwarding the converted ConvNet twice: First
over the original image and second over the image but with the image shifted spatially by 16 pixels along both width and
height.</p><div id=outline-container-headline-5 class=outline-5><h5 id=headline-5>Padding</h5><div id=outline-text-headline-5 class=outline-text-5><p>使用 padding 保持卷积之后，图像的尺寸不变，网络的表现会更好。valid 卷积无法充分获取图像边界的信息。 Why use padding? In
addition to the aforementioned benefit of keeping the spatial sizes constant after CONV, doing this actually improves
performance. If the CONV layers were to not zero-pad the inputs and only perform valid convolutions, then the size of
the volumes would reduce by a small amount after each CONV, and the information at the borders would be “washed away”
too quickly.</p></div></div></div></div><div id=outline-container-headline-6 class=outline-4><h4 id=headline-6>使用矩阵乘法实现卷积</h4><div id=outline-text-headline-6 class=outline-text-4><p>caffe 中的 im2col 实现图像卷积展开成矩阵。由于同一个值需要存储在矩阵的不同位置，会消耗大量的内存。但可以利用现有的矩阵乘
法库（BLAS）来加速计算。</p><p>Implementation as Matrix Multiplication. Note that the convolution operation essentially performs dot products between
the filters and local regions of the input. A common implementation pattern of the CONV layer is to take advantage of
this fact and formulate the forward pass of a convolutional layer as one big matrix multiply as follows:</p><ol><li>The local regions in the input image are stretched out into columns in an operation commonly called im2col. For
example, if the input is [227x227x3] and it is to be convolved with 11x11x3 filters at stride 4, then we would take
[11x11x3] blocks of pixels in the input and stretch each block into a column vector of size 11*11*3 = 363. Iterating
this process in the input at stride of 4 gives (227-11)/4+1 = 55 locations along both width and height, leading to an
output matrix X_col of im2col of size [363 x 3025], where every column is a stretched out receptive field and there
are 55*55 = 3025 of them in total. Note that since the receptive fields overlap, every number in the input volume may
be duplicated in multiple distinct columns.</li><li>The weights of the CONV layer are similarly stretched out into rows. For example, if there are 96 filters of size
[11x11x3] this would give a matrix W_row of size [96 x 363].</li><li>The result of a convolution is now equivalent to performing one large matrix multiply np.dot(W_row, X_col), which
evaluates the dot product between every filter and every receptive field location. In our example, the output of this
operation would be [96 x 3025], giving the output of the dot product of each filter at each location.</li><li>The result must finally be reshaped back to its proper output dimension [55x55x96].</li></ol><p>This approach has the downside that it can use a lot of memory, since some values in the input volume are replicated
multiple times in X_col. However, the benefit is that there are many very efficient implementations of Matrix
Multiplication that we can take advantage of (for example, in the commonly used BLAS API). Moreover, the same im2col
idea can be reused to perform the pooling operation, which we discuss next.</p></div></div></div></div><div id=outline-container-headline-7 class=outline-3><h3 id=headline-7>Pooling Layer</h3><div id=outline-text-headline-7 class=outline-text-3><p>Pooling 可以大大较少参数的个数，防止过拟合。 It is common to periodically insert a Pooling layer in-between successive
Conv layers in a ConvNet architecture. Its function is to progressively <strong>reduce the spatial size of the representation
to reduce the amount of parameters and computation in the network, and hence to also control overfitting.</strong></p><p>The Pooling Layer operates independently on every depth slice of the input and resizes it spatially, using the MAX
operation.</p><p>特性：</p><ol><li>两个超参： spatial extent F, stride S</li><li>zero padding</li></ol><dl><dt>反向传播</dt><dd>前向传播的时候记录 max value 的索引，方便反向传播计算。 max pooling 反向传播时，先将 feature maps 上采样，
将梯度值传入 max value 索引的地方，其他位置的值均赋值为 0 。avera pooling 则将梯度值除以扩大的倍数，并将平均值
分别填入每一个位置。总之确保 loss 值反向传播中大小和保持不变。</dd><dt>摒弃 pooling</dt><dd>在 variational autoencoders (VAEs) or generative adversarial networks (GANs) 中不使用 pooling 对于网络
的训练很重要。可以使用 stride 较大的卷积替代。</dd></dl><p>Pooling sizes with larger receptive fields are too destructive.</p><p>Pooling 可以使用任意函数，只是 max 使用比较多。 In addition to max pooling, the pooling units can also perform other
functions, such as average pooling or even L2-norm pooling.</p><p>Average pooling 表现不如 max pooling 而逐渐被抛弃。 Average pooling was often used historically but has recently fallen
out of favor compared to the max pooling operation, which has been shown to work better in practice.</p></div></div><div id=outline-container-headline-8 class=outline-3><h3 id=headline-8>Layer Patterns</h3><div id=outline-text-headline-8 class=outline-text-3><p>INPUT -> [[CONV -> RELU]*N -> POOL?]*M -> [FC -> RELU]*K -> FC</p><p>Here we see two CONV layers stacked before every POOL layer. This is generally a good idea for larger and deeper
networks, because multiple stacked CONV layers can develop more complex features of the input volume before the
destructive pooling operation.</p><p>堆叠小尺寸的卷积核，相比于直接只用一个大尺寸的卷积核，可以得到更好的特征，且只需要更好的参数。但是需要更多的内存来存储中
间的卷积层。 Intuitively, stacking CONV layers with tiny filters as opposed to having one CONV layer with big filters
allows us to express more powerful features of the input, and with fewer parameters. As a practical disadvantage, we
might need more memory to hold all the intermediate CONV layer results if we plan to do backpropagation.</p><p>从输入到输出只有一单条线路的卷积神经网络，正在被更复杂的网络结构所替代 GoogLeNet, ResNet, DenseNet 等。It should be
noted that the conventional paradigm of a linear list of layers has recently been challenged, in Google’s Inception
architectures and also in current (state of the art) Residual Networks from Microsoft Research Asia. Both of these (see
details below in case studies section) feature more intricate and different connectivity structures.</p><p>尽量使用已有的网络模型，并在自己的数据集上 finetune ，而不是从头开始设计。 In practice: use whatever works best on
ImageNet. If you’re feeling a bit of a fatigue in thinking about the architectural decisions, you’ll be pleased to know
that in 90% or more of applications you should not have to worry about these. I like to summarize this point as “don’t
be a hero”: Instead of rolling your own architecture for a problem, you should look at whatever architecture currently
works best on ImageNet, download a pretrained model and finetune it on your data. You should rarely ever have to train a
ConvNet from scratch or design one from scratch. I also made this point at the Deep Learning school.</p></div></div><div id=outline-container-headline-9 class=outline-3><h3 id=headline-9>Layer Sizing Patterns</h3><div id=outline-text-headline-9 class=outline-text-3><p>输入图像的大小最好可以多次被 2 整除。 The input layer (that contains the image) should be divisible by 2 many times.
Common numbers include 32 (e.g. CIFAR-10), 64, 96 (e.g. STL-10), or 224 (e.g. common ImageNet ConvNets), 384, and 512.</p><p>使用 3*3 的卷积可以有效减小参数的个数，需要增大感受野可以堆叠多个 3*3 卷积。 The conv layers should be using small
filters (e.g. 3x3 or at most 5x5), using a stride of S=1, and crucially, padding the input volume with zeros in such way
that the conv layer does not alter the spatial dimensions of the input. That is, when F=3, then using P=1 will retain
the original size of the input. When F=5, P=2. For a general F, it can be seen that P=(F−1)/2 preserves the input size.
If you must use bigger filter sizes (such as 7x7 or so), it is only common to see this on the very first conv layer that
is looking at the input image.</p><p>Pooling 层有较大的破坏性，一般不会使用较大的尺寸。The pool layers are in charge of downsampling the spatial dimensions
of the input. The most common setting is to use max-pooling with 2x2 receptive fields (i.e. F=2), and with a stride of 2
(i.e. S=2). Note that this discards exactly 75% of the activations in an input volume (due to downsampling by 2 in both
width and height). Another slightly less common setting is to use 3x3 receptive fields with a stride of 2, but this
makes. It is very uncommon to see receptive field sizes for max pooling that are larger than 3 because the pooling is
then too lossy and aggressive. This usually leads to worse performance.</p><p>卷积层只改变通道数，不修改图像的大小。POOL 负责减小图像的大小。 Reducing sizing headaches. The scheme presented above is
pleasing because all the CONV layers preserve the spatial size of their input, while the POOL layers alone are in charge
of down-sampling the volumes spatially. In an alternative scheme where we use strides greater than 1 or don’t zero-pad
the input in CONV layers, we would have to very carefully keep track of the input volumes throughout the CNN
architecture and make sure that all strides and filters “work out”, and that the ConvNet architecture is nicely and
symmetrically wired.</p><p>卷积步长选择为 1 效果较好。 Why use stride of 1 in CONV? Smaller strides work better in practice. Additionally, as
already mentioned stride 1 allows us to leave all spatial down-sampling to the POOL layers, with the CONV layers only
transforming the input volume depth-wise.</p><p>由于内存的限制，会首先大幅度减小图像的尺寸。Compromising based on memory constraints. In some cases (especially early in
the ConvNet architectures), the amount of memory can build up very quickly with the rules of thumb presented above. For
example, filtering a 224x224x3 image with three 3x3 CONV layers with 64 filters each and padding 1 would create three
activation volumes of size [224x224x64]. This amounts to a total of about 10 million activations, or 72MB of memory (per
image, for both activations and gradients). Since GPUs are often bottlenecked by memory, it may be necessary to
compromise. In practice, people prefer to make the compromise at only the first CONV layer of the network. For example,
one compromise might be to use a first CONV layer with filter sizes of 7x7 and stride of 2 (as seen in a ZF net). As
another example, an AlexNet uses filter sizes of 11x11 and stride of 4.</p></div></div><div id=outline-container-headline-10 class=outline-3><h3 id=headline-10><a href=https://arxiv.org/abs/1603.07285>&lt;A guide to convolution arithmetic for deep learning></a></h3><div id=outline-text-headline-10 class=outline-text-3><p>论文的部分翻译，<a href=https://github.com/vdumoulin/conv_arithmetic>github 地址</a> ，由于不是很理解转置卷积和空洞卷积，所以看了看这篇文章，翻译一下以备忘。相比于斯坦福大学的课
程，感觉这篇论文有点故弄玄虚，不符合大道至简呀【微吐槽】。</p></div></div><div id=outline-container-headline-11 class=outline-3><h3 id=headline-11>Introduction</h3><div id=outline-text-headline-11 class=outline-text-3><p>CNNs 在深度学习中处于核心角色，但通常第一次使用 CNNs 却是一场噩梦。因为一个卷积层输出的大小会由输入的大小、卷积核的尺寸、
padding 的大小、stride 共同决定，而且相互之间的关心有点复杂。相对而言，全连接层的输出的大小和输入的大小并没有直接关系。
同时 CNNs 经常会引入 pooling层，进一步增加了复杂性。另外转置卷积也在被更频繁的使用。</p><p>这个手册有两个目的：</p><ol><li>解释卷积和转置卷积的相互关系</li><li>对卷积层、pooling 层、转置卷积层中 input shape、kernel shape、zero padding、strides、output shape 的相互关系有一个直
观的理解</li></ol><div id=outline-container-headline-12 class=outline-4><h4 id=headline-12>Discrete convolutions</h4><div id=outline-text-headline-12 class=outline-text-4><p>神经网络的必备元素是仿射变换 ： 输入是一个向量，乘以一个矩阵，在加上一个偏移得到输出。这适用于任何形式的输入（图片、音频
片段、无序特征；不管维数是多少都可以在转换前变成一个向量）</p><p>图片、音频片段等数据都有内在结构，一般来说，他们有以下重要的属性：</p><ol><li>使用多维数组存储。 [They are stored as multi-dimensional arrays.]</li><li>特征在一个维度或者多个维度的存储顺序有影响。 [They feature one or more axes for which ordering matters (e.g., width
and height axes for an image, time axis for a sound clip).]</li><li>channel 维度表示的是数据的不同 view 。[One axis, called the channel axis, is used to access different views of the
data (e.g., the red, green and blue channels of a color image, or the left and right channels of a stereo audio
track).]</li></ol><p>当使用仿射变换的时候，这些结构信息将无法被有效利用。因为所有轴上的数据都被统一对待，根本没有考虑数据的拓扑信息（直接将数
据转换成了一个向量）。而在计算机视觉和语音识别中最好保留这些数据的结构，因此有了离散卷积 (discrete convolution)。</p><p>离散卷积是一个线性变换，保留了数据的顺序，采用稀疏连接和权值共享。</p><p>卷积核扫描输入，在每一个位置上，卷积核和其覆盖的输入区域逐元素相乘并求和得到该位置的输出。可以重复执行这个过程来得到多个
输出，这些生成的输出称为 feature maps 。如果输入是多个 feature maps ，卷积核需要是三维的，卷积核 channel 的个数应该等于
输入 feature maps channel 的个数，输入的每一个 feature map 都需要一个特定的卷积核来扫描，得到的多个结果（不同的 channel
）逐元素相加得到输出的一个 feature map 。</p><p>strides 表示的是 kernel 移动的步长，也可以看做是保留步长为 1 卷积输出的某些部分。比如步长为 2 的卷积输出可以看做，步长为
1 的卷积得到的输出中只保留奇数位元素（从 1 开始计数）得到的结果。</p></div></div><div id=outline-container-headline-13 class=outline-4><h4 id=headline-13>pooling</h4><div id=outline-text-headline-13 class=outline-text-4><p>pooling 是 CNNs 的另一个关键部件。用于减小 feature maps 的尺寸，通常有 average pooling 和 max pooling 两种。 Pooling 和
卷积很像（我感觉就是一种特殊的卷积呀！），同样会扫描输入，但是将 kernel 换成了 pooling function 。</p></div></div></div></div><div id=outline-container-headline-14 class=outline-3><h3 id=headline-14>Convolution arithmetic</h3><div id=outline-text-headline-14 class=outline-text-3><p>下面的分析依赖于卷积不会跨越不同轴的特性；[i.e., the choice of kernel size, stride and zero padding along axis j only
affects the output size of axis j.]</p><div id=outline-container-headline-15 class=outline-4><h4 id=headline-15>Half (same) padding</h4><div id=outline-text-headline-15 class=outline-text-4><p>保持输出和输入的尺寸不变时使用，需要填充的个数 \( p = \lfloor k / 2 \rfloor \) 。</p></div></div><div id=outline-container-headline-16 class=outline-4><h4 id=headline-16>Full padding</h4><div id=outline-text-headline-16 class=outline-text-4><p>卷积通常会较小输出的尺寸，但有时候需要增大输出的尺寸（这个不是转置卷积的任务吗），此时可以用 full padding 。此时填充的个
数 \( p = k - 1 \) ，这里 full padding 的意思是让输入的每个像素点对 kernel 都有相同的地位。</p></div></div><div id=outline-container-headline-17 class=outline-4><h4 id=headline-17>Zero padding, non-unit strides</h4><div id=outline-text-headline-17 class=outline-text-4><p>卷积输出大小的通用公式 \[ o = \lfloor \frac{i + 2p - k}{s} \rfloor + 1 \]</p><p>这里向下取整，是因为如果剩余的区域（含 padding ）不足一个 kernel 的长度时，不再继续执行卷积。</p></div></div></div></div><div id=outline-container-headline-18 class=outline-3><h3 id=headline-18>Pooling arithmetic</h3><div id=outline-text-headline-18 class=outline-text-3><p>pooling 层使得网络对输入的微小偏移不敏感。max pooling 使用较频繁： 将输入分割成 patches （通常不会重叠） ，并输出每个
patch 内的最大值；另外还有 average pooling ，输出每个 patch 内的平均值。</p><p>计算卷积输出大小的公式同样适用于 pooling ，但是 Pooling 并不会 padding ，所以 Pooling 的通用输出公式为</p><p>\[ o = \lfloor \frac{i - k}{s} \rfloor + 1 \]</p></div></div><div id=outline-container-headline-19 class=outline-3><h3 id=headline-19>Transposed convolution arithmetic</h3><div id=outline-text-headline-19 class=outline-text-3><p>转置卷积一般用于常规卷积的反方向操作，将常规卷积的输出转换到输入，同时保持和常规卷积的兼容性。例如，可以用于 autoencoder
中的 decoding layer 或者将 feature map 变换到更高维的空间中。全连接层比较简单，只需要利用权重矩阵的转置就可以实现反向操
作。卷积层会比全连接层复杂很多，不过可以将卷积层归结为高效实现的矩阵操作，洞悉全连接层将有助于解决卷积。如同卷积操作不会
跨越不同的坐标轴（不会跨越 channel），转置卷积也有相同的属性，这将简化转置卷积算法。</p><p>下文重点关注;</p><ol><li>2-D 转置卷积 (N=2)</li><li>square inputs \((i_1 = i_2 = i)\)</li><li>square kernel size \((k_1 = k_2 = k)\)</li><li>所有方向使用相同的 strides \((s_1 = s_2 = s)\)</li><li>所有 axes 使用相同大小的 padding \((p_1 = p_2 = p)\)</li></ol><p>这些结果都可以推广到 N-D 和 non-square 情形</p><p>转置卷积等效的卷积输出大小同样可以用上面卷积中的通用公式 \[ o = \lfloor \frac{i + 2p - k}{s} \rfloor + 1 \] 只是这里的
参数需要正确对应。</p><div id=outline-container-headline-20 class=outline-4><h4 id=headline-20>卷积视为矩阵操作</h4><div id=outline-text-headline-20 class=outline-text-4><p>当 \(i = 4, k = 3, s = 1, p = 0\) 时，如果将 input 和 output 从左到右，由上到下展开成向量，卷积可以表示成一个稀疏的矩阵
C ，其中非 0 的元素是 kernel 中相应的元素，其中 \(i\) 和 \(j\) 分别表示 kernel 的行和列</p><p>\begin{equation*}
≤ft(
\begin{array}{cccccccccccccccc}
w_{0,0} & w_{0,1} & w_{0,2} & 0 & w_{1,0} & w_{1,1} & w_{1,2} & 0 & w_{2,0} & w_{2,1} & w_{2,2} & 0 & 0 & 0 & 0 & 0 \\
0 & w_{0,0} & w_{0,1} & w_{0,2} & 0 & w_{1,0} & w_{1,1} & w_{1,2} & 0 & w_{2,0} & w_{2,1} & w_{2,2} & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & w_{0,0} & w_{0,1} & w_{0,2} & 0 & w_{1,0} & w_{1,1} & w_{1,2} & 0 & w_{2,0} & w_{2,1} & w_{2,2} & 0 \\
0 & 0 & 0 & 0 & 0 & w_{0,0} & w_{0,1} & w_{0,2} & 0 & w_{1,0} & w_{1,1} & w_{1,2} & 0 & w_{2,0} & w_{2,1} & w_{2,2}
\end{array}
\right)
\end{equation*}</p><p>这种线性操作将输入矩阵展开成 16 维的向量，同时生成一个 4 维的向量，然后 reshape 成 2x2 的输出矩阵。</p><p>使用这种方法表示时，反向传播可以很容易通过矩阵 C 的转置矩阵实现，即 loss 乘以 \(C^T\) 来反向传播误差。将一个 4 维的向量
作为输入，得到一个 16 维的输出，并且其连接模式（感觉应该是输入输出矩阵展开成向量的方式）和矩阵 C 兼容。</p><p>值得注意的是， kernel W 同时定义了用于前向传播矩阵 C ，也定义了反向传播矩阵 \(C^T\) 。</p></div></div><div id=outline-container-headline-21 class=outline-4><h4 id=headline-21>转置卷积</h4><div id=outline-text-headline-21 class=outline-text-4><p>转置卷积 (transposed convolution) 也叫做小数步长卷积 (fractionally strided convolutions) 或 deconvolutions [学术上可能使
用这个名称，但是在数学上 deconvolution 表示逆矩阵，和矩阵的转置并不一样] 。kernel 定义了矩阵 W ，可以认为是常规卷积，在
前向传播时乘以矩阵 C ，反向传播时乘以矩阵 \(C^T\) ；也可以看做转置卷积，在前向传播时乘以矩阵 \(C^T\) ，反向传播时乘以
\((C^T)^T = C\) ，所以 <strong>一个转置卷积总是可以对应到一个常规卷积，但需要对输入增加许多无效的 0 值（额外的值全为 0 的行和
列），导致效率低下</strong> 。</p><p>实际代码实现时将转置卷积看做，将梯度作为输入的常规卷积？？？ TODO [The transposed convolution operation can be thought of as
the gradient of some convolution with respect to its input, which is usually how transposed convolutions are implemented
in practice.]</p></div></div><div id=outline-container-headline-22 class=outline-4><h4 id=headline-22>No zero padding, unit strides, transposed</h4><div id=outline-text-headline-22 class=outline-text-4><p>思考转置卷积最简单的方法是，将输入看做常规卷积的某一层输出的 feature map ，通过转置卷积来恢复该层的输入（由于转置卷积并
不是卷积的逆，无法原值恢复输入，仅仅可以恢复相同宽和高的输入）。</p><p>\(i=4, k=3, s=1, p=0\) 的常规卷积得到一个 2x2 的输出，转置卷积将用 2x2 的输入得到 4x4 的输出。</p><p>另一种得到转置卷积输出的方法（第一种方法在哪里？）：利用常规卷积来实现： \(i'=2, k'=k, s'=1, p'=2\) 。可以看到卷积核和步
长保持不变，但是增加了 padding 。这是一个低效的方法，这里只是为了阐述方便，实际实现中不会计算那些无效的 0 值。</p><p>通过卷积的连接方式（connectivity pattern : 输入、输出的大小），理解转置卷积的等效常规卷积需要增加 padding 的背后逻辑，并
用其来指导设计等效的卷积。如，输入左上角的 pixel 只会影响输出的左上角，输入的右上角的像素只会作用于输出的右上角，其他的
位置类似。</p><p>为了保持相同的连接方式，需要 zero pad ，使得第一个卷积（最左上角的卷积）只与输入的最左上角的像素相乘（kernel 其他位置与
padding 的 0 值相乘），也就是让 \(p = k -1 \) 。</p><p>\(s=1, p=0, k\) 卷积的转置，等效于卷积 \(k'=k, s'=s, p'=k-1\) ，且其输出为 \[ o' = i' + (k - 1) \]</p><p>有趣的是，这里等效的常规卷积是 full padding 、单位步长的卷积。</p><p><a href=https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/no_padding_no_strides_transposed.gif>i'=2, k'=3, s'=1, p'=2 动图</a></p></div></div><div id=outline-container-headline-23 class=outline-4><h4 id=headline-23>Zero padding, unit strides, transposed</h4><div id=outline-text-headline-23 class=outline-text-4><p>既然知道无 padding 的卷积的转置等效于让 input 增加 padding 的常规卷积，当 input 有 padding 的时候，等效的转置卷积输入会有较
少个数的 padding 。因为常规卷积输入 padding 的值并非真正有效的值（但是参与了运算），而当前面的输入需要作为转置卷积的输出
的时候，输出的大小的等效值变小了（padding 并不需要输出），从而转置卷积的输入也就需要较小的 padding 。</p><p>\(i=5, k=4, p=2\) 卷积的转置，等效于 \(k'=k, s'=s, p'=k-p-1\) 的卷积，且输出大小为 \[o' = i' + (k-1) -2p\]</p><div id=outline-container-headline-24 class=outline-5><h5 id=headline-24>Half(same) padding, transposed</h5><div id=outline-text-headline-24 class=outline-text-5><p>由于 same padding 的常规卷积，输出的大小等于输入的大小；因此 same padding 常规卷积的转置的等效常规卷积是其本身，也是
一个 same padding 的卷积。即 \(k=2n+1, s=1, p= \lfloor k/2 \rfloor = n\) 卷积的转置等效于 \(k'=k, s'=s, p'=p\) ，且其输
出大小为</p><p>\begin{align*}
o' &= i' + (k-1) -2p<br>&= i' + 2n -2n<br>&= i'
\end{align*}</p><p><a href=https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/same_padding_no_strides_transposed.gif>动图</a></p></div></div><div id=outline-container-headline-25 class=outline-5><h5 id=headline-25>Full padding, transposed</h5><div id=outline-text-headline-25 class=outline-text-5><p>non-padded 卷积的转置等效于一个 full padding 的卷积，理所当然，一个 full padding 卷积的转置等效于一个 non-padded 卷积。
两者互为输入输出关系。</p><p>\(s=1, k, p=k-1\) 卷积的转置等效于 \(k'=k, s'=s, p'=0\) 的卷积，且输出大小为</p><p>\begin{align*}
o' &= i' + (k-1) -2p<br>&= i' - (k-1)
\end{align*}</p><p><a href=https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/full_padding_no_strides_transposed.gif>动图</a></p></div></div></div></div><div id=outline-container-headline-26 class=outline-4><h4 id=headline-26>No zero padding, non-unit strides, transposed</h4><div id=outline-text-headline-26 class=outline-text-4><p>步长 \(s > 1\) 卷积的转置，等效的卷积步长 \(s &lt; 1\) ，这也是转置卷积被称为小数步长卷积 fractionally strided convolutions
的原因。当卷积的步长 \(s>1\) 时，其转置等效的卷积需要在 input 的每两个 pixel 之间插入 \(s-1\) 的 0 值，从而使得 kernel
的移动步伐慢于 unit strides 的卷积。当然这里只是为了便于理解，实际代码实现肯定不会去让那些无效的 0 值去做乘法。</p><p>\(p=0, k, s\) 且 \(i-k\) 可以整除 \(s\) 卷积的转置的等效卷积为 \(\tilde{i}', k'=k, s'=1, p'=k-1\) ，其中 \(\tilde{i}'\)
表示在 input 的两个 pixel 之间插入 \(s-1\) 个 0 值，且其输出为 \[o' = s(i'-1) + k\]</p><p><a href=https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/no_padding_strides_transposed.gif>动图</a></p></div></div><div id=outline-container-headline-27 class=outline-4><h4 id=headline-27>Zero padding, non-unit strides, transposed</h4><div id=outline-text-headline-27 class=outline-text-4><p>\(k, s, p\) 且 \(i+2p-k\) 可以整除 \(s\) 卷积的转置，等效的卷积为 \(\tilde{i}', k'=k, s'=1, p'=k-p-1\) ，其中
\(\tilde{i}'\) 表明在 input 的每两个 pixel 之间插入 \(s-1\) 个 0 值，且其输出为 \[o'=s(i'-1)+k-2p\]</p><p><a href=https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/padding_strides_transposed.gif>动图</a></p><p>\(k,s,p\) 卷积的转置，等效的卷积为 \(a, \tilde{i}', k'=k, s'=1, p'=k-p-1\) 其中 \(\tilde{i}'\) 表示在 input 的每两个
pixel 之间插入 \(s-1\) 个 0 值，\(a=(i+2p-k) \ mod \ s \) 表示在 input 的底部和右侧需要添加的 0 值的个数，且其输出为
\[o'=s(i'-1)+a+k-2p\]</p><p><a href=https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/padding_strides_odd_transposed.gif>动图</a></p></div></div><div id=outline-container-headline-28 class=outline-4><h4 id=headline-28>转置卷积等效卷积参数的通用规则</h4><div id=outline-text-headline-28 class=outline-text-4><p>\begin{align*}
p' &= k -p -1<br>s' &= 1<br>k' &= k<br>& \text{adding s-1 zeros between each input unit}
\end{align*}</p></div></div></div></div><div id=outline-container-headline-29 class=outline-3><h3 id=headline-29>Miscellaneous convolutions</h3><div id=outline-text-headline-29 class=outline-text-3><div id=outline-container-headline-30 class=outline-4><h4 id=headline-30>Dilated convolutions</h4><div id=outline-text-headline-30 class=outline-text-4><p>Dilated convolutions 通过在 kernel 相邻的两个元素之间插入空隙来扩充 kernel 。扩大比例由超参 d 来决定，即在 kernel 的相邻
元素之间插入 d-1 个 0 值后，执行常规卷积操作。</p><p>Dilated convolutions 是不增大卷积核的尺寸，同时可以扩大感受野的有效方法，特别是堆叠使用多层的 dilated convolutions 的时
候。</p><p>扩充后的卷积核的尺寸为 \[ \hat{k} = k + (k-1)(d-1) \]</p><p>参考常规卷积输出的通用公式，利用扩充后卷积核的大小，可以得到 Dilated convolutions 输出的大小</p><p>\[ o = \lfloor \frac{i + 2p - k - (k-1)(d-1) }{s} \rfloor + 1 \]</p><p>这个将导致输出的 size 减小</p></div></div></div></div><div id=outline-container-headline-31 class=outline-3><h3 id=headline-31>读后感</h3><div id=outline-text-headline-31 class=outline-text-3><ol><li>论文中解释了一些自己原来不清楚的知识点，如 full padding 的含义、</li><li>感觉论文没有很清楚的解释，只是单纯的列举出了各种情况下卷积和转置卷积，以及相互对应关系</li><li>我还是不清楚实际中是怎样实现转置卷积以及空洞卷积的，当时就是看 DarkNet 的代码没看懂才来阅读的！！！转置卷积是通过将输
入输出展开成矩阵，然后使用矩阵乘法来实现？还是转换成等效的卷积实现？</li><li>读完之后没有那种豁然开朗的感觉</li></ol></div></div><div id=outline-container-headline-32 class=outline-3><h3 id=headline-32>Deformable Convolutions NetWorks</h3><div id=outline-text-headline-32 class=outline-text-3><p>正常的 3x3 卷积，首先使用一个规则的区域 R 采集输入的 feature maps X ，然后乘以权重 w 。</p><p>\[ \mathit{R} = \{(-1,-1),(-1,0), \ldots, (0,1), (1,1) \]</p><p>\[ y(P_0) = \sum_{P_n \in R} W \cdot X(P_0 + P_n) \]</p><p>而可变形卷积就是将输入进行添加 offset \(\{\triangle P_n | n = 1,\ldots, N \} \ where \ N = | R |\) ，相应的卷积公式变成</p><p>\[ y(P_0) = \sum_{P_n \ in R} W \cdot X(P_0 + P_n + \triangle P_n) \]</p><p>因为每个输入的位移都是二维的，所以可以在二维的 feature map 上任意方向上移动。</p><p>又由于增加 offset 的点可能不对应整数坐标，此时通过双线性差值得到偏移后的输入值。</p><p>MxNet 有可变形卷积的接口，不过记得可变形卷积官方代码说 MxNet 实现的有问题？</p><div class="src src-python"><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>mxnet</span><span class=o>.</span><span class=n>ndarray</span><span class=o>.</span><span class=n>contrib</span><span class=o>.</span><span class=n>DeformableConvolution</span></span></span></code></pre></div></div></div></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://kylestones.github.io/tags/%E5%8D%B7%E7%A7%AF/>卷积,</a></li><li><a href=https://kylestones.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>深度学习</a></li></ul><nav class=paginav><a class=prev href=https://kylestones.github.io/blog/emotion/joke/><span class=title>« Prev</span><br><span>笑话集锦</span></a>
<a class=next href=https://kylestones.github.io/blog/machinelearning/gpu-mxnet-install/><span class=title>Next »</span><br><span>ubuntu18.04 install gpu mxnet</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share convolution on twitter" href="https://twitter.com/intent/tweet/?text=convolution&url=https%3a%2f%2fkylestones.github.io%2fblog%2fmachinelearning%2fconvolution%2f&hashtags=%e5%8d%b7%e7%a7%af%2c%2c%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share convolution on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fkylestones.github.io%2fblog%2fmachinelearning%2fconvolution%2f&title=convolution&summary=convolution&source=https%3a%2f%2fkylestones.github.io%2fblog%2fmachinelearning%2fconvolution%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share convolution on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fkylestones.github.io%2fblog%2fmachinelearning%2fconvolution%2f&title=convolution"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share convolution on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fkylestones.github.io%2fblog%2fmachinelearning%2fconvolution%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share convolution on whatsapp" href="https://api.whatsapp.com/send?text=convolution%20-%20https%3a%2f%2fkylestones.github.io%2fblog%2fmachinelearning%2fconvolution%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share convolution on telegram" href="https://telegram.me/share/url?text=convolution&url=https%3a%2f%2fkylestones.github.io%2fblog%2fmachinelearning%2fconvolution%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://kylestones.github.io>Org Mode</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>