<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>目标检测 | Org Mode</title><meta name=keywords content="目标检测,,深度学习"><meta name=description content="YOLO 非常敬佩作者 Joseph Redmon ，没有使用开源框架，而是自己使用 C 和 cuda 另外写了一套框架 DarkNet ，并且将其开源。而且 license 写的非常有意思，有点狂放不羁，可能这就是大牛该有的样子
Darknet is public domain. Do whatever you want with it. Stop emailing me about it! YOLO 是 You Only Look Once 的简写。当然 YOLO 很可能让人想起另一句话 You only live once, but if you do it right, once is enough. – Mae West ，个人感觉作者可能有点故意让两者混淆。
区别于 RNN 系列的文章，需要先查找 region proposals ，然后在之上进行目标检测。YOLO 只需要运行一遍卷积神经网络就可以完成目 标检测，所以其最主要的优点就是 速度 。而且 mAP (mean Average Precision) 随着算法的改进也表现的相当不错。"><meta name=author content="Kyle Three Stones"><link rel=canonical href=https://kylestones.github.io/blog/machinelearning/objectdetection/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://kylestones.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://kylestones.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://kylestones.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://kylestones.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://kylestones.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="目标检测"><meta property="og:description" content="YOLO 非常敬佩作者 Joseph Redmon ，没有使用开源框架，而是自己使用 C 和 cuda 另外写了一套框架 DarkNet ，并且将其开源。而且 license 写的非常有意思，有点狂放不羁，可能这就是大牛该有的样子
Darknet is public domain. Do whatever you want with it. Stop emailing me about it! YOLO 是 You Only Look Once 的简写。当然 YOLO 很可能让人想起另一句话 You only live once, but if you do it right, once is enough. – Mae West ，个人感觉作者可能有点故意让两者混淆。
区别于 RNN 系列的文章，需要先查找 region proposals ，然后在之上进行目标检测。YOLO 只需要运行一遍卷积神经网络就可以完成目 标检测，所以其最主要的优点就是 速度 。而且 mAP (mean Average Precision) 随着算法的改进也表现的相当不错。"><meta property="og:type" content="article"><meta property="og:url" content="https://kylestones.github.io/blog/machinelearning/objectdetection/"><meta property="og:image" content="https://kylestones.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2018-08-24T00:00:00+00:00"><meta property="article:modified_time" content="2018-08-24T00:00:00+00:00"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://kylestones.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="目标检测"><meta name=twitter:description content="YOLO 非常敬佩作者 Joseph Redmon ，没有使用开源框架，而是自己使用 C 和 cuda 另外写了一套框架 DarkNet ，并且将其开源。而且 license 写的非常有意思，有点狂放不羁，可能这就是大牛该有的样子
Darknet is public domain. Do whatever you want with it. Stop emailing me about it! YOLO 是 You Only Look Once 的简写。当然 YOLO 很可能让人想起另一句话 You only live once, but if you do it right, once is enough. – Mae West ，个人感觉作者可能有点故意让两者混淆。
区别于 RNN 系列的文章，需要先查找 region proposals ，然后在之上进行目标检测。YOLO 只需要运行一遍卷积神经网络就可以完成目 标检测，所以其最主要的优点就是 速度 。而且 mAP (mean Average Precision) 随着算法的改进也表现的相当不错。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Blogs","item":"https://kylestones.github.io/blog/"},{"@type":"ListItem","position":3,"name":"目标检测","item":"https://kylestones.github.io/blog/machinelearning/objectdetection/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"目标检测","name":"目标检测","description":"YOLO 非常敬佩作者 Joseph Redmon ，没有使用开源框架，而是自己使用 C 和 cuda 另外写了一套框架 DarkNet ，并且将其开源。而且 license 写的非常有意思，有点狂放不羁，可能这就是大牛该有的样子\nDarknet is public domain. Do whatever you want with it. Stop emailing me about it! YOLO 是 You Only Look Once 的简写。当然 YOLO 很可能让人想起另一句话 You only live once, but if you do it right, once is enough. – Mae West ，个人感觉作者可能有点故意让两者混淆。\n区别于 RNN 系列的文章，需要先查找 region proposals ，然后在之上进行目标检测。YOLO 只需要运行一遍卷积神经网络就可以完成目 标检测，所以其最主要的优点就是 速度 。而且 mAP (mean Average Precision) 随着算法的改进也表现的相当不错。","keywords":["目标检测,","深度学习"],"articleBody":" YOLO 非常敬佩作者 Joseph Redmon ，没有使用开源框架，而是自己使用 C 和 cuda 另外写了一套框架 DarkNet ，并且将其开源。而且 license 写的非常有意思，有点狂放不羁，可能这就是大牛该有的样子\nDarknet is public domain. Do whatever you want with it. Stop emailing me about it! YOLO 是 You Only Look Once 的简写。当然 YOLO 很可能让人想起另一句话 You only live once, but if you do it right, once is enough. – Mae West ，个人感觉作者可能有点故意让两者混淆。\n区别于 RNN 系列的文章，需要先查找 region proposals ，然后在之上进行目标检测。YOLO 只需要运行一遍卷积神经网络就可以完成目 标检测，所以其最主要的优点就是 速度 。而且 mAP (mean Average Precision) 随着算法的改进也表现的相当不错。\n而且此时是 R-CNN two-stage 方法盛行的时候，先找到一些候选区域（代替窗口扫描），然后只在这些候选区域上进行检测。多么好的 主意。一般人肯定都会在思考怎样更加快速有效的提取这些候选区域（Faster R-CNN），或者加快提取特征的速度（Fast R-CNN）。作者 居然可以完全另辟蹊径，直接进行检测输出 one-stage 完成，根本无需候选区域，的确大牛。由此也可以看到有些时候一些非常好的思 想同样会束缚住我们。\n先放三张 YOLOv3 的检测结果： 算法 首先将图像分成 s*s grid cells ，在每个 grid cell 上都输出 B 个 bouding box ，每个 bouding box 都包含 5 个值 (tx,ty,tw,th,to) ，其中 (tx,ty) 表示目标的中心点，(tw,th) 表示宽度和高度，(to) 表示 bouding box 中有 object 的概率。另外 每个 grid cell 会输出 C 个不同类别的使用 Softmax 求解到的概率。这样每个 grid cell 输出向量的维数是 (B*5 + C) ，网络总的 输出向量维数是 (s*s * (B*5 +c)) 。目标的中心点落在哪个 grid cell 中，则由这个 grid cell 进行检测。\n其中 (tx,ty) 是相对于 grid cell 左上角 (cx,cy) 的偏移 offset ，通过 sigmoid 函数使得 offset 在 0~1 之间（认为每个 grid cell 的宽度和高度都是 1 ）。而 (tw,th) 利用与 Anchor Boxes 的宽和高的相对关系来求取。当然 YOLOv1 中并没有使用 Anchor box， 这是在 YOLOv2 中的改进。相比于直接使用坐标，使用 offset 或者相对关系可以是模型更加稳定。因为通过随机初始化，网络需要很长 的训练时间才能找到目标的位置，而找到相对偏移量则会容易很多。\n由于 sum-squared error 比较容易优化，作者使用均方误差计算检测误差。即计算 grid cell 的四个坐标值与 ground truth 值差值的 平方和。\n另外由于大多数的 grid cell 中没有 object ，只有少数的 grid cell 中会有目标，所以作者为这两种不同的误差赋予了不同的权重， 以阻止预测值趋向于 0 。具体 \\(\\lambda_{coord}=5, \\ \\lambda_{noobj}=0.5\\) 。\n同时由于均方误差会让较大的 bouding box 相对于较小的 bouding box 产生更多的误差，作者将宽和高开方之后再计算误差。\n\\begin{align*} λcoord ∑i=0s^2 ∑j=0B {\\mathit{1}}ijobj ≤ft[ ( x_i - \\hat{x}_i )^2 + ( y_i - \\hat{y}_i )^2 + ( \\sqrt{w_i} - \\sqrt{ \\hat{w}_i } )^2 ( \\sqrt{h_i} - \\sqrt{ \\hat{h}_i } )^2 \\right] \\end{align*}\n此时每个 grid cell 只能检测一个目标，如果多个目标则中心点在同一个 grid cell 则无法同时检测，且对较小的目标检测效果不太好。\n将最后一个激活函数修改成线性激活函数，应该是去掉激活函数一样，使得 feature 有更大的可学习空间。\nYOLOv2 作者借鉴吸收了许多他人的方法，结合自己的思考来提升算法。YOLO 有明显的定位错误和较低的 recall\nBatch Normalization 大大加速了算法的收敛速度，且是一个很好的正则化的方法。此时作者已经放弃使用 Dropout ，只使用 BN 来达到正则化的效果。 Anchor Boxes 为 bouding Boxes 引入先验。在训练样本上使用 k-means 聚类来找到 Bouding Boxes 的 good prior ，而不像 R-CNN中那样人为凭经验设定 Anchor Boxes 的大小。假如使用欧氏距离来计算 k-means 的误差，larger boxes 会 比 smaller boxes 产生更大的误差，而此时真正关心的是 prior 和 ground truth Boxes 的 IOU (intersection over union) ，所以作者将距离度量的方法由欧式距离变为 \\(d = 1 - IOU(box,centroid)\\) ，作者最终选择了 5 个 anchor boxes ，是 tradeoff model complexity 和 high recall 的结果。使用 Anchor Boxes 后，不再是每个 grid cell 只检测一个目标，而是每个 Anchor Boxes 检测一个目标，也就需要为每一个 Anchor Boxes 输出所有类 别的概率。tx,ty 使用相对于 grid cell 左上角的 offset 而非坐标，tw,th 使用 box prior 的相对大小，使得模 型更加容易训练；另外作者移除了全连接层，使用 global average pooling 代替；去掉了一个 max-pooling 层使 得输出的 feature map 有更大的分辨率；grid cell 的个数也由 7*7 变成 13*13，作者将图像的输入调整为 416x416 ，经过网络或缩小 32 倍，变成 13x13 。奇数可以保证只有一个中心点，而较大的物体中心点一般在图像 的中心，有偶数个中心点则不太好归类。 Fine-Grained Feature YOLOv2 最终得到的 feature map 的大小是 13x13 ，为了在多个分辨率上进行检测，作者从 26x26 feature map 串接到 13x13 feature map 。The passthrough layer concatenates the higher resolution features with the low resolution features by stacking adja-cent features into different channels instead of spatial lo-cations, similar to the identity mappings in ResNet. This turns the 26×26×512 feature map into a 13×13×2048 feature map, which can be concatenated with the original features. Our detector runs on top of this expanded feature map so that it has access to fine grained features. ResNet 中的 skip connect 是逐 channel 逐 element 想加的，这里是什么意思？ Multi-Scale Training 由于 YOLOv2 中只有卷积层和池化层（去掉了全连接层），所以网络可以接受任何维数的输入。作者使用间 隔为 32 的从 320 到 608 {320,352,…,608} 这些不同分辨率的图像来训练网络。每 10 patches 随机选择输入图像的大小，强 制网络在不同的分辨率上表现都不错。 High Resolution Classifier 当网络需要同时在输入图像的尺寸和目标（由分类变成检测）都改变的时候，逐一进行 fine tune 。所有 state-of-art 的检测方法都会先使用 ImageNet 进行预训练，此时输入的大小为 224x224 ，先使用 448x448 的输入在 ImageNet 上进行 fine tune ，运行 10 epochs 。然后在使用检测的代价函数去 fine tune 。 Darknet-19 效仿 VGG 只使用 3x3 卷积，并且在 polling 之后将 channel 加倍；学习 NIN 在 3x3 卷积之间使用 1x1 conv 来压 缩特征（减小 channel 的个数），Global average pooling 代替全连接；BN 加速训练与正则化。 YOLO9000 作者提出了一种检测和分类的联合训练方法。有检测 label 的样本用于训练检测的 Bouding Boxes ，而用于分类的样本可以扩充检测类 别的个数。\nImageNet 依据 WordNet 来标记，而 WordNet 是一个有向图，而不是树，因为同一个节点可能有两个父节点。作者将其改造成树 WordTree。首先将所有只有一条 path 的添加到树中，剩余的节点按照增加最少边数来添加。从根节点到某节点的 path 所有节点条件概 率的乘积即为该节点的分类概率，是一个 multi-label model。而且可以利用 WordTree 结合不同的数据集\n另外使用相同级别的同义词为一个单位来计算 Softmax ，而不是所有的类别统一来计算 Softmax 。因为使用 Softmax 要求不同的类别 相互独立，而这里显示并不符合。\n作者利用 WordTree 结合 COCO 和 ImageNet 组成训练样本来训练网络，由于 ImageNet 比 COCO 大很多，通过 oversampling 来使两者 的比例为 4:1 。构造了一个包含 9000 种类别的样本，此时每个 Anchor Boxes 都需要输出 9000 中类别的概率？？？\n使用 detection image 样本训练时，使用 YOLOv2 损失函数来计算并反向传播，使用分类样本则值修正分类错误，修改的范围是这个类 别集其上层类别。\nYOLOv3 objectness score 使用 logistic regression 来求取 objectness score 。先找到概率最大的 box prior ，然后抑制那些与该 box IOU 大于一定值的其他 box 。 class prediction multilablel classification 使用独立的 logistic classifier 来分类，各个类别并不相互独立，而是相互有 重叠。 prediction across scales 由于 high level feature maps 有更强的语义信息，而 low level feature maps 有更强的空间位置 信息，结合两者可以更好的预测目标的位置。参考 feature pyramid network (FPN) ，使用网络多层的 feature map 来组成不同 scale 的特征金字塔来检测。FPN代替原来的图像金字塔，使用卷积后的不同尺寸的 feature maps 组成特征金字塔。论文中用 stage 表明 feature map size 相同的层，每个 stage 的最后一层 feature maps 用于生成特征金字塔。因为并不是直接使用这些 特征 feature maps 组成特征金字塔，而是让顶层（靠近输出层）的 feature maps 进行上采样 upsample ，得到和下层 feature maps 相同的 size，下层的 feature maps 进行 1x1 卷积以减少 channel 的个数，然后逐元素相加来组成该层的 feature maps 。由于要执行 element-wise 相加，作者让特征金字塔的每一层的 feature maps channel 固定为 256 个，首先将最顶层的 feature maps 使用 1x1 卷积将 channel 的个数降低为 256 个，组成特征金字塔的顶层。下面层也都会先使用 1x1 卷积将 channel 个数减小为 256 ，然后再与上层的特征逐元素相加得到新的一层。最终得到特征金字塔。在每一层上都独立进行目标检测。 Darknet-53 借鉴 ResNet 和 vgg ，使用 shortcut connecttions 、3x3 CONV 、1x1 CONV 。 other standard stuff multi-scale training、lots of data augmentation、batch normalization R-CNN R-CNN 使用选择搜索 selective search 的方法得到很多可能含有目标的矩形框（region proposal）；然后将得到的不同大小的 region proposal 统一 resize 到某个固定的大小，并送入卷积神经网络提取固定长度的语义特征；之后使用每个类别的 SVM 分类器来识别目标的种类；并且利用 bounding-Box Regression 对 region proposal 进行调节，只是简单的学习四个 参数来调节 x,y,w,h，以更好的匹配目标。 Fast R-CNN 仍然使用 selective search 来得到 region proposal ；利用一个卷积神经网络同时进行分类和 bounding box 回归， 即使用卷积神经网络得到 feature maps，将 region proposal 对应到该 feature maps 上，然后将 feature maps 上 的每一个 region proposal 输入两个全连接层，一个用来分类，另一个用来进行 bbox 回归。而且使用 RoI max pooling 的方法，将 feature maps 上的每个 region proposal 分割成 WxH 固定数量的 bins （每个 channel 独立 进行，不改变 channel 的个数），每个 bins 内执行 max pooling 得到最大值，这样无论 feature maps 上的 region proposal 尺寸的大小，都统一变成 WxH 后送入之后的全连接层。 Faster R-CNN 使用卷积神经网络来提取 region proposal 。作者设计 Region Proposal Network (RPN) ，先将任意尺寸的图片经 过一些卷积操作，然后在某层 feature maps 上使用固定大小的滑动窗口（文中使用 3x3 ）扫描 feature maps ， 每个窗口位置上提取固定长度的 feature （论文中提取 256d 的特征），然后将得到的所有特征经过两个全连接分 支，一个用于分类是前景还是背景，另一个用于输出 region proposal 的位置和大小。这就是 RPN 网络。当然由于 作者只使用固定的窗口而且只扫描一遍，为了得到较好的效果，作者提出了 anchor boxes 的概念，就是要让一个窗 口对应多个不同大小的矩形框。具体作者使用了 128^2, 256^2, 512^2 和 1:1, 1:2, 2:1 组合成的 k=9 种不同大 小的 anchor boxes （作者将输入图像都 rescale 到 1000*600，这些 anchor boxes 对应的都是输入图像上的矩形 框）。The design of multi-scale anchors is a key component for sharing features without extra cost for addressing scales. 这样 RPN 中的每个窗口 ： 都对应了 k 个anchor boxes ，分类分支生成 2k 个输出，回归分 支生成 4k 个输出。根据卷积层尺寸的不同最终得到输出的维数也不同，假如卷积层大小为 WxH ，那么分类层最终 将有 WxHx2k个输出，回归层有 WxHx4k 个输出。并且滑动窗口可以使用 3x3 的卷积实现，后面的两个分类和回归分 支可以使用1x1 卷积实现。训练 RPN 网络时使用分类和回归两者的共同误差来训练网络。检测网络使用 Fast R-CNN 的方式实现，每个 RoI 输出 C+1 个类别概率以及 4C 个物体边框（C 为物体的种类）。注意，每一个 feature maps 上的 region proposal 都需要独立经过 RoI max pooling （各个 channel 独立进行 max pooling ，即保持 channel 个数不变，只是空间上分成了许多 bins） 然后输入之后的全连接，这里并没有实现共享。另外作者让 RPN 网络和 Fast R-CNN 网络共享大部分卷积操作，具体作者采用 4 步训练法来训练网络\n使用 ImageNet 进行预训练，然后使用 RPN 网络进行 fine-tune 同样使用 ImageNet 上预训练的网络和上一步训练得到的 region proposal 来训练 Fast R-CNN 网络 利用第 2 步中得到的参数来初始化 RPN 网络中共享的卷积层参数，并固定这些卷积层的参数，只 fine-tune RPN 独有的参数 固定共享层的参数，值训练检测网络的参数 MASK R-CNN Mask R-CNN 最主要的共享在于图像分割，放在这里作为目标检测似乎有点不妥。\n效果图：\n在 Faster R-CNN 的基础上再增加一个用于分割的 branch ，采用 FCN 全卷积神经网络来对目标进行分割。采用逐元素分类的方法，对 每个 RoI 输出 kxmxm 个输出，而不是将其整合称一个向量（会损失空间位置信息），其中 k 表示目标的类别的个数，m 是经过 RoIAlign 之后得到的固定大小。使用二分类来判定每个像素点是否属于某个类别的目标。\n由于采用 pixel-to-pixel 的形式，故需要让 RoI 和原图精准对应，作者在 RoI pooling 的基础上进行上进行了改进，不再对 RoI 的 边界坐标和 bins 的大小进行量化（当求取的是浮点数时进行取整），而是保留这些浮点值；并在每一个 bins 内使用双线性差值得到采 样个数个（论文中使用的是 4 ，即在每一个 bins 内使用双线性差值求得 4 个点的值，4 个点将一个 bin 分成大小相等的 9 份； We note that the results are not sensitive to the exact sampling locations, or how many points are sampled, as long as no quantization is performed.）点的值，然后使用 max 或者 average pooling （作者表明两者影响不大，并且论文中采用了 average pooling）得到该 bin 的输出。No quantization is performed on any coordinates involved in the RoI, its bins, or the sampling points.\n损失采用分类边框回归和分割误差三者的和表示。 \\(L = L_{cls} + L_{box} + L_{mask}\\)\nThe mask branch has a Km 2 - dimensional output for each RoI, which encodes K binary masks of resolution m × m, one for each of the K classes. To this we apply a per-pixel sigmoid, and define L mask as the average binary cross-entropy loss. For an RoI associated with ground-truth class k, L mask is only defined on the k-th mask (other mask outputs do not contribute to the loss).\nMASK R-CNN 使用了特征金字塔 FPN\nSSD FPN feature pyramid network : 由于 high level feature maps 有更强的语义信息，而 low level feature maps 有更强的空间位置信息， 结合两者可以更好的预测目标的位置。使用网络多层的 feature map 来组成不同 scale 的特征金字塔来检测。FPN 代替原来的图像金字 塔，使用卷积后的不同尺寸的 feature maps 组成特征金字塔。论文中用 stage 表明 feature map size 相同的层，每个 stage 的最后 一层 feature maps 用于生成特征金字塔。因为并不是直接使用这些特征 feature maps 组成特征金字塔，而是让顶层（靠近输出层）的 feature maps 进行上采样 upsample ，得到和下层 feature maps 相同的 size，下层的 feature maps 进行 1x1 卷积以减少 channel 的个数，然后逐元素相加来组成该层的 feature maps 。由于要执行 element-wise 相加，作者让特征金字塔的每一层的 feature maps channel 固定为 256 个，首先将最顶层的 feature maps 使用 1x1 卷积将 channel 的个数降低为 256 个，组成特征金字塔的顶层。下 面层也都会先使用 1x1 卷积将 channel 个数减小为 256 ，然后再与上层的特征逐元素相加得到新的一层。最终得到特征金字塔。在每 一层上都独立进行目标检测。\nRPN 采用 FPN ： 在一个特征层使用一个固定的大小的 anchor （不过仍然有三种比例）\n","wordCount":"1248","inLanguage":"en","datePublished":"2018-08-24T00:00:00Z","dateModified":"2018-08-24T00:00:00Z","author":{"@type":"Person","name":"Kyle Three Stones"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://kylestones.github.io/blog/machinelearning/objectdetection/"},"publisher":{"@type":"Organization","name":"Org Mode","logo":{"@type":"ImageObject","url":"https://kylestones.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://kylestones.github.io accesskey=h title="Home (Alt + H)"><img src=https://kylestones.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://kylestones.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://kylestones.github.io/tags/ title=tags><span>tags</span></a></li><li><a href=https://example.org title=example.org><span>example.org</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://kylestones.github.io>Home</a>&nbsp;»&nbsp;<a href=https://kylestones.github.io/blog/>Blogs</a></div><h1 class=post-title>目标检测</h1><div class=post-meta><span title='2018-08-24 00:00:00 +0000 UTC'>August 24, 2018</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;1248 words&nbsp;·&nbsp;Kyle Three Stones&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/blog/machinelearning/objectdetection.org rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><div id=outline-container-headline-1 class=outline-3><h3 id=headline-1>YOLO</h3><div id=outline-text-headline-1 class=outline-text-3><p>非常敬佩作者 Joseph Redmon ，没有使用开源框架，而是自己使用 C 和 cuda 另外写了一套框架 DarkNet ，并且将其开源。而且
license 写的非常有意思，有点狂放不羁，可能这就是大牛该有的样子</p><ol><li>Darknet is public domain.</li><li>Do whatever you want with it.</li><li>Stop emailing me about it!</li></ol><p>YOLO 是 You Only Look Once 的简写。当然 YOLO 很可能让人想起另一句话 You only live once, but if you do it right, once is
enough. – Mae West ，个人感觉作者可能有点故意让两者混淆。</p><p>区别于 RNN 系列的文章，需要先查找 region proposals ，然后在之上进行目标检测。YOLO 只需要运行一遍卷积神经网络就可以完成目
标检测，所以其最主要的优点就是 <strong>速度</strong> 。而且 mAP (mean Average Precision) 随着算法的改进也表现的相当不错。</p><p>而且此时是 R-CNN two-stage 方法盛行的时候，先找到一些候选区域（代替窗口扫描），然后只在这些候选区域上进行检测。多么好的
主意。一般人肯定都会在思考怎样更加快速有效的提取这些候选区域（Faster R-CNN），或者加快提取特征的速度（Fast R-CNN）。作者
居然可以完全另辟蹊径，直接进行检测输出 one-stage 完成，根本无需候选区域，的确大牛。由此也可以看到有些时候一些非常好的思
想同样会束缚住我们。</p><p>先放三张 YOLOv3 的检测结果：</p><p><img src=./bottle.jpg alt=./bottle.jpg title=./bottle.jpg></p><p><img src=./person.jpg alt=./person.jpg title=./person.jpg></p><p><img src=./monkeyking.jpg alt=./monkeyking.jpg title=./monkeyking.jpg></p><div id=outline-container-headline-2 class=outline-4><h4 id=headline-2>算法</h4><div id=outline-text-headline-2 class=outline-text-4><p>首先将图像分成 s*s grid cells ，在每个 grid cell 上都输出 B 个 bouding box ，每个 bouding box 都包含 5 个值
(tx,ty,tw,th,to) ，其中 (tx,ty) 表示目标的中心点，(tw,th) 表示宽度和高度，(to) 表示 bouding box 中有 object 的概率。另外
每个 grid cell 会输出 C 个不同类别的使用 Softmax 求解到的概率。这样每个 grid cell 输出向量的维数是 (B*5 + C) ，网络总的
输出向量维数是 (s*s * (B*5 +c)) 。目标的中心点落在哪个 grid cell 中，则由这个 grid cell 进行检测。</p><p>其中 (tx,ty) 是相对于 grid cell 左上角 (cx,cy) 的偏移 offset ，通过 sigmoid 函数使得 offset 在 0~1 之间（认为每个 grid
cell 的宽度和高度都是 1 ）。而 (tw,th) 利用与 Anchor Boxes 的宽和高的相对关系来求取。当然 YOLOv1 中并没有使用 Anchor box，
这是在 YOLOv2 中的改进。相比于直接使用坐标，使用 offset 或者相对关系可以是模型更加稳定。因为通过随机初始化，网络需要很长
的训练时间才能找到目标的位置，而找到相对偏移量则会容易很多。</p><p>由于 sum-squared error 比较容易优化，作者使用均方误差计算检测误差。即计算 grid cell 的四个坐标值与 ground truth 值差值的
平方和。</p><p>另外由于大多数的 grid cell 中没有 object ，只有少数的 grid cell 中会有目标，所以作者为这两种不同的误差赋予了不同的权重，
以阻止预测值趋向于 0 。具体 \(\lambda_{coord}=5, \ \lambda_{noobj}=0.5\) 。</p><p>同时由于均方误差会让较大的 bouding box 相对于较小的 bouding box 产生更多的误差，作者将宽和高开方之后再计算误差。</p><p>\begin{align*}
λ<sub>coord</sub> ∑<sub>i=0</sub><sup>s^2</sup> ∑<sub>j=0</sub><sup>B</sup> {\mathit{1}}<sub>ij</sub><sup>obj</sup>
≤ft[ ( x_i - \hat{x}_i )^2 + ( y_i - \hat{y}_i )^2 + ( \sqrt{w_i} - \sqrt{ \hat{w}_i } )^2</p><ul><li>( \sqrt{h_i} - \sqrt{ \hat{h}_i } )^2 \right]</li></ul><p>\end{align*}</p><p>此时每个 grid cell 只能检测一个目标，如果多个目标则中心点在同一个 grid cell 则无法同时检测，且对较小的目标检测效果不太好。</p><p>将最后一个激活函数修改成线性激活函数，应该是去掉激活函数一样，使得 feature 有更大的可学习空间。</p><div id=outline-container-headline-3 class=outline-5><h5 id=headline-3>YOLOv2</h5><div id=outline-text-headline-3 class=outline-text-5><p>作者借鉴吸收了许多他人的方法，结合自己的思考来提升算法。YOLO 有明显的定位错误和较低的 recall</p><dl><dt>Batch Normalization</dt><dd>大大加速了算法的收敛速度，且是一个很好的正则化的方法。此时作者已经放弃使用 Dropout ，只使用 BN
来达到正则化的效果。</dd><dt>Anchor Boxes</dt><dd>为 bouding Boxes 引入先验。在训练样本上使用 k-means 聚类来找到 Bouding Boxes 的 good prior ，而不像
R-CNN中那样人为凭经验设定 Anchor Boxes 的大小。假如使用欧氏距离来计算 k-means 的误差，larger boxes 会
比 smaller boxes 产生更大的误差，而此时真正关心的是 prior 和 ground truth Boxes 的 IOU (intersection
over union) ，所以作者将距离度量的方法由欧式距离变为 \(d = 1 - IOU(box,centroid)\) ，作者最终选择了 5
个 anchor boxes ，是 tradeoff model complexity 和 high recall 的结果。使用 Anchor Boxes 后，不再是每个
grid cell 只检测一个目标，而是每个 Anchor Boxes 检测一个目标，也就需要为每一个 Anchor Boxes 输出所有类
别的概率。tx,ty 使用相对于 grid cell 左上角的 offset 而非坐标，tw,th 使用 box prior 的相对大小，使得模
型更加容易训练；另外作者移除了全连接层，使用 global average pooling 代替；去掉了一个 max-pooling 层使
得输出的 feature map 有更大的分辨率；grid cell 的个数也由 7*7 变成 13*13，作者将图像的输入调整为
416x416 ，经过网络或缩小 32 倍，变成 13x13 。奇数可以保证只有一个中心点，而较大的物体中心点一般在图像
的中心，有偶数个中心点则不太好归类。</dd><dt>Fine-Grained Feature</dt><dd>YOLOv2 最终得到的 feature map 的大小是 13x13 ，为了在多个分辨率上进行检测，作者从 26x26
feature map 串接到 13x13 feature map 。The passthrough layer concatenates the higher resolution features with the
low resolution features by stacking adja-cent features into different channels instead of spatial lo-cations,
similar to the identity mappings in ResNet. This turns the 26×26×512 feature map into a 13×13×2048 feature map,
which can be concatenated with the original features. Our detector runs on top of this expanded feature map so that
it has access to fine grained features. ResNet 中的 skip connect 是逐 channel 逐 element 想加的，这里是什么意思？</dd><dt>Multi-Scale Training</dt><dd>由于 YOLOv2 中只有卷积层和池化层（去掉了全连接层），所以网络可以接受任何维数的输入。作者使用间
隔为 32 的从 320 到 608 {320,352,…,608} 这些不同分辨率的图像来训练网络。每 10 patches 随机选择输入图像的大小，强
制网络在不同的分辨率上表现都不错。</dd><dt>High Resolution Classifier</dt><dd>当网络需要同时在输入图像的尺寸和目标（由分类变成检测）都改变的时候，逐一进行 fine tune
。所有 state-of-art 的检测方法都会先使用 ImageNet 进行预训练，此时输入的大小为 224x224 ，先使用 448x448 的输入在
ImageNet 上进行 fine tune ，运行 10 epochs 。然后在使用检测的代价函数去 fine tune 。</dd><dt>Darknet-19</dt><dd>效仿 VGG 只使用 3x3 卷积，并且在 polling 之后将 channel 加倍；学习 NIN 在 3x3 卷积之间使用 1x1 conv 来压
缩特征（减小 channel 的个数），Global average pooling 代替全连接；BN 加速训练与正则化。</dd></dl></div></div><div id=outline-container-headline-4 class=outline-5><h5 id=headline-4>YOLO9000</h5><div id=outline-text-headline-4 class=outline-text-5><p>作者提出了一种检测和分类的联合训练方法。有检测 label 的样本用于训练检测的 Bouding Boxes ，而用于分类的样本可以扩充检测类
别的个数。</p><p>ImageNet 依据 WordNet 来标记，而 WordNet 是一个有向图，而不是树，因为同一个节点可能有两个父节点。作者将其改造成树
WordTree。首先将所有只有一条 path 的添加到树中，剩余的节点按照增加最少边数来添加。从根节点到某节点的 path 所有节点条件概
率的乘积即为该节点的分类概率，是一个 multi-label model。而且可以利用 WordTree 结合不同的数据集</p><p>另外使用相同级别的同义词为一个单位来计算 Softmax ，而不是所有的类别统一来计算 Softmax 。因为使用 Softmax 要求不同的类别
相互独立，而这里显示并不符合。</p><p>作者利用 WordTree 结合 COCO 和 ImageNet 组成训练样本来训练网络，由于 ImageNet 比 COCO 大很多，通过 oversampling 来使两者
的比例为 4:1 。构造了一个包含 9000 种类别的样本，此时每个 Anchor Boxes 都需要输出 9000 中类别的概率？？？</p><p>使用 detection image 样本训练时，使用 YOLOv2 损失函数来计算并反向传播，使用分类样本则值修正分类错误，修改的范围是这个类
别集其上层类别。</p></div></div><div id=outline-container-headline-5 class=outline-5><h5 id=headline-5>YOLOv3</h5><div id=outline-text-headline-5 class=outline-text-5><dl><dt>objectness score</dt><dd>使用 logistic regression 来求取 objectness score 。先找到概率最大的 box prior ，然后抑制那些与该
box IOU 大于一定值的其他 box 。</dd><dt>class prediction</dt><dd>multilablel classification 使用独立的 logistic classifier 来分类，各个类别并不相互独立，而是相互有
重叠。</dd><dt>prediction across scales</dt><dd>由于 high level feature maps 有更强的语义信息，而 low level feature maps 有更强的空间位置
信息，结合两者可以更好的预测目标的位置。参考 feature pyramid network (FPN) ，使用网络多层的 feature map 来组成不同
scale 的特征金字塔来检测。FPN代替原来的图像金字塔，使用卷积后的不同尺寸的 feature maps 组成特征金字塔。论文中用
stage 表明 feature map size 相同的层，每个 stage 的最后一层 feature maps 用于生成特征金字塔。因为并不是直接使用这些
特征 feature maps 组成特征金字塔，而是让顶层（靠近输出层）的 feature maps 进行上采样 upsample ，得到和下层 feature
maps 相同的 size，下层的 feature maps 进行 1x1 卷积以减少 channel 的个数，然后逐元素相加来组成该层的 feature maps
。由于要执行 element-wise 相加，作者让特征金字塔的每一层的 feature maps channel 固定为 256 个，首先将最顶层的
feature maps 使用 1x1 卷积将 channel 的个数降低为 256 个，组成特征金字塔的顶层。下面层也都会先使用 1x1 卷积将
channel 个数减小为 256 ，然后再与上层的特征逐元素相加得到新的一层。最终得到特征金字塔。在每一层上都独立进行目标检测。</dd><dt>Darknet-53</dt><dd>借鉴 ResNet 和 vgg ，使用 shortcut connecttions 、3x3 CONV 、1x1 CONV 。</dd><dt>other standard stuff</dt><dd>multi-scale training、lots of data augmentation、batch normalization</dd></dl></div></div></div></div></div></div><div id=outline-container-headline-6 class=outline-3><h3 id=headline-6>R-CNN</h3><div id=outline-text-headline-6 class=outline-text-3><dl><dt>R-CNN</dt><dd>使用选择搜索 selective search 的方法得到很多可能含有目标的矩形框（region proposal）；然后将得到的不同大小的
region proposal 统一 resize 到某个固定的大小，并送入卷积神经网络提取固定长度的语义特征；之后使用每个类别的
SVM 分类器来识别目标的种类；并且利用 bounding-Box Regression 对 region proposal 进行调节，只是简单的学习四个
参数来调节 x,y,w,h，以更好的匹配目标。</dd><dt>Fast R-CNN</dt><dd>仍然使用 selective search 来得到 region proposal ；利用一个卷积神经网络同时进行分类和 bounding box 回归，
即使用卷积神经网络得到 feature maps，将 region proposal 对应到该 feature maps 上，然后将 feature maps 上
的每一个 region proposal 输入两个全连接层，一个用来分类，另一个用来进行 bbox 回归。而且使用 RoI max
pooling 的方法，将 feature maps 上的每个 region proposal 分割成 WxH 固定数量的 bins （每个 channel 独立
进行，不改变 channel 的个数），每个 bins 内执行 max pooling 得到最大值，这样无论 feature maps 上的
region proposal 尺寸的大小，都统一变成 WxH 后送入之后的全连接层。</dd><dt>Faster R-CNN</dt><dd><p>使用卷积神经网络来提取 region proposal 。作者设计 Region Proposal Network (RPN) ，先将任意尺寸的图片经
过一些卷积操作，然后在某层 feature maps 上使用固定大小的滑动窗口（文中使用 3x3 ）扫描 feature maps ，
每个窗口位置上提取固定长度的 feature （论文中提取 256d 的特征），然后将得到的所有特征经过两个全连接分
支，一个用于分类是前景还是背景，另一个用于输出 region proposal 的位置和大小。这就是 RPN 网络。当然由于
作者只使用固定的窗口而且只扫描一遍，为了得到较好的效果，作者提出了 anchor boxes 的概念，就是要让一个窗
口对应多个不同大小的矩形框。具体作者使用了 128^2, 256^2, 512^2 和 1:1, 1:2, 2:1 组合成的 k=9 种不同大
小的 anchor boxes （作者将输入图像都 rescale 到 1000*600，这些 anchor boxes 对应的都是输入图像上的矩形
框）。The design of multi-scale anchors is a key component for sharing features without extra cost for
addressing scales. 这样 RPN 中的每个窗口 ： 都对应了 k 个anchor boxes ，分类分支生成 2k 个输出，回归分
支生成 4k 个输出。根据卷积层尺寸的不同最终得到输出的维数也不同，假如卷积层大小为 WxH ，那么分类层最终
将有 WxHx2k个输出，回归层有 WxHx4k 个输出。并且滑动窗口可以使用 3x3 的卷积实现，后面的两个分类和回归分
支可以使用1x1 卷积实现。训练 RPN 网络时使用分类和回归两者的共同误差来训练网络。检测网络使用 Fast R-CNN
的方式实现，每个 RoI 输出 C+1 个类别概率以及 4C 个物体边框（C 为物体的种类）。注意，每一个 feature
maps 上的 region proposal 都需要独立经过 RoI max pooling （各个 channel 独立进行 max pooling ，即保持
channel 个数不变，只是空间上分成了许多 bins） 然后输入之后的全连接，这里并没有实现共享。另外作者让 RPN
网络和 Fast R-CNN 网络共享大部分卷积操作，具体作者采用 4 步训练法来训练网络</p><ol><li>使用 ImageNet 进行预训练，然后使用 RPN 网络进行 fine-tune</li><li>同样使用 ImageNet 上预训练的网络和上一步训练得到的 region proposal 来训练 Fast R-CNN 网络</li><li>利用第 2 步中得到的参数来初始化 RPN 网络中共享的卷积层参数，并固定这些卷积层的参数，只 fine-tune
RPN 独有的参数</li><li>固定共享层的参数，值训练检测网络的参数</li></ol></dd></dl></div></div><div id=outline-container-headline-7 class=outline-3><h3 id=headline-7>MASK R-CNN</h3><div id=outline-text-headline-7 class=outline-text-3><p>Mask R-CNN 最主要的共享在于图像分割，放在这里作为目标检测似乎有点不妥。</p><p>效果图：</p><p><img src=./mask.jpg alt=./mask.jpg title=./mask.jpg></p><p>在 Faster R-CNN 的基础上再增加一个用于分割的 branch ，采用 FCN 全卷积神经网络来对目标进行分割。采用逐元素分类的方法，对
每个 RoI 输出 kxmxm 个输出，而不是将其整合称一个向量（会损失空间位置信息），其中 k 表示目标的类别的个数，m 是经过
RoIAlign 之后得到的固定大小。使用二分类来判定每个像素点是否属于某个类别的目标。</p><p>由于采用 pixel-to-pixel 的形式，故需要让 RoI 和原图精准对应，作者在 RoI pooling 的基础上进行上进行了改进，不再对 RoI 的
边界坐标和 bins 的大小进行量化（当求取的是浮点数时进行取整），而是保留这些浮点值；并在每一个 bins 内使用双线性差值得到采
样个数个（论文中使用的是 4 ，即在每一个 bins 内使用双线性差值求得 4 个点的值，4 个点将一个 bin 分成大小相等的 9 份； We
note that the results are not sensitive to the exact sampling locations, or how many points are sampled, as long as no
quantization is performed.）点的值，然后使用 max 或者 average pooling （作者表明两者影响不大，并且论文中采用了 average
pooling）得到该 bin 的输出。No quantization is performed on any coordinates involved in the RoI, its bins, or the
sampling points.</p><p>损失采用分类边框回归和分割误差三者的和表示。 \(L = L_{cls} + L_{box} + L_{mask}\)</p><p>The mask branch has a Km 2 - dimensional output for each RoI, which encodes K binary masks of resolution m × m, one for
each of the K classes. To this we apply a per-pixel sigmoid, and define L mask as the average binary cross-entropy loss.
For an RoI associated with ground-truth class k, L mask is only defined on the k-th mask (other mask outputs do not
contribute to the loss).</p><p>MASK R-CNN 使用了特征金字塔 FPN</p></div></div><div id=outline-container-headline-8 class=outline-3><h3 id=headline-8>SSD</h3></div><div id=outline-container-headline-9 class=outline-3><h3 id=headline-9>FPN</h3><div id=outline-text-headline-9 class=outline-text-3><p>feature pyramid network : 由于 high level feature maps 有更强的语义信息，而 low level feature maps 有更强的空间位置信息，
结合两者可以更好的预测目标的位置。使用网络多层的 feature map 来组成不同 scale 的特征金字塔来检测。FPN 代替原来的图像金字
塔，使用卷积后的不同尺寸的 feature maps 组成特征金字塔。论文中用 stage 表明 feature map size 相同的层，每个 stage 的最后
一层 feature maps 用于生成特征金字塔。因为并不是直接使用这些特征 feature maps 组成特征金字塔，而是让顶层（靠近输出层）的
feature maps 进行上采样 upsample ，得到和下层 feature maps 相同的 size，下层的 feature maps 进行 1x1 卷积以减少 channel
的个数，然后逐元素相加来组成该层的 feature maps 。由于要执行 element-wise 相加，作者让特征金字塔的每一层的 feature maps
channel 固定为 256 个，首先将最顶层的 feature maps 使用 1x1 卷积将 channel 的个数降低为 256 个，组成特征金字塔的顶层。下
面层也都会先使用 1x1 卷积将 channel 个数减小为 256 ，然后再与上层的特征逐元素相加得到新的一层。最终得到特征金字塔。在每
一层上都独立进行目标检测。</p><p>RPN 采用 FPN ： 在一个特征层使用一个固定的大小的 anchor （不过仍然有三种比例）</p></div></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://kylestones.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/>目标检测,</a></li><li><a href=https://kylestones.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>深度学习</a></li></ul><nav class=paginav><a class=prev href=https://kylestones.github.io/blog/machinelearning/summarize/><span class=title>« Prev</span><br><span>something</span></a>
<a class=next href=https://kylestones.github.io/blog/machinelearning/seethemove/><span class=title>Next »</span><br><span>见招拆招</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share 目标检测 on twitter" href="https://twitter.com/intent/tweet/?text=%e7%9b%ae%e6%a0%87%e6%a3%80%e6%b5%8b&url=https%3a%2f%2fkylestones.github.io%2fblog%2fmachinelearning%2fobjectdetection%2f&hashtags=%e7%9b%ae%e6%a0%87%e6%a3%80%e6%b5%8b%2c%2c%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 目标检测 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fkylestones.github.io%2fblog%2fmachinelearning%2fobjectdetection%2f&title=%e7%9b%ae%e6%a0%87%e6%a3%80%e6%b5%8b&summary=%e7%9b%ae%e6%a0%87%e6%a3%80%e6%b5%8b&source=https%3a%2f%2fkylestones.github.io%2fblog%2fmachinelearning%2fobjectdetection%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 目标检测 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fkylestones.github.io%2fblog%2fmachinelearning%2fobjectdetection%2f&title=%e7%9b%ae%e6%a0%87%e6%a3%80%e6%b5%8b"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 目标检测 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fkylestones.github.io%2fblog%2fmachinelearning%2fobjectdetection%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 目标检测 on whatsapp" href="https://api.whatsapp.com/send?text=%e7%9b%ae%e6%a0%87%e6%a3%80%e6%b5%8b%20-%20https%3a%2f%2fkylestones.github.io%2fblog%2fmachinelearning%2fobjectdetection%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 目标检测 on telegram" href="https://telegram.me/share/url?text=%e7%9b%ae%e6%a0%87%e6%a3%80%e6%b5%8b&url=https%3a%2f%2fkylestones.github.io%2fblog%2fmachinelearning%2fobjectdetection%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://kylestones.github.io>Org Mode</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>