<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>TensorFlow | Org Mode</title><meta name=keywords content="TensorFlow,,深度学习"><meta name=description content="架构 阅读大神的 《TensorFlow 内核剖析》 对 TensorFlow 的整个代码框架有了一些了解，以下是读书笔记。
Graph (计算图)是 TensorFlow 领域模型的核心。计算图就是节点与边的集合，是一个 DAG (有向无环图)图。
Node(节点)持有零条或多条输入/输出的边，分别使用 in_edges， out_edges 表示。
Edge(边) 持有前驱节点与后驱节点，从而实现了计算图的连接，也是计算图前向遍历，后向遍历的衔接点。边上的数据以 Tensor 的形 式传递。计算图中存在两类边：
普通边：用于承载 Tensor，常用实线表示； 控制依赖：控制节点的执行顺序，常用虚线表示。 TensorFlow 计算的单位是 OP，它表示了某种抽象计算。通过定义 OP 来构建 DAG 图。OP 拥有 0 个或多个「输入/输出」，及其 0 个 或多个「属性」。其中，输入/输出以Tensor 的形式存在。在系统实现中，OP 的元数据使用 Protobuf 格式的 OpDef 描述，实现前端与 后端的数据交换，及其领域模型的统一。OpDef 定义包括 OP 的名字，输入输出列表，属性列表，优化选项等。其中，属性常常用于描述 输入/输出的类型，大小，默认值，约束，及OP 的其他特性。
计算图的执行过程将按照 DAG 的拓扑排序，依次启动 OP 的运算。其中，如果存在多个入度为 0 的节点，TensorFlow 运行时可以实现 并发，同时执行多个 OP 的运算，提高执行效率。
架构设计 TensorFlow 遵循良好的分层架构：
front end ： 用户接口，负责构造计算图 runtime ： 实现计算图的拆分。提供本地运行模式和分布式运行模式，两者共享大部分设计和实现 计算层 ： 基于 Eigen 实现计算的逻辑实现；同时支持各种硬件的并行加速 通信层 ： 基于 gRPC 实现组件间的数据交换。同时支持 RDMA 设备层 ： 支持多种异构计算设备。实际执行计算的载体 前端系统 Client 是前端系统的主要组成部分，它是一个支持多语言的编程环境，且对 Python 和 C++ 的支持比较完善。实现时通过 Swig 完成对 后端 C++ 的调用。基于这些编程接口来构造计算图。"><meta name=author content="Kyle Three Stones"><link rel=canonical href=https://kylestones.github.io/blog/machinelearning/tensorflow/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://kylestones.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://kylestones.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://kylestones.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://kylestones.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://kylestones.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="TensorFlow"><meta property="og:description" content="架构 阅读大神的 《TensorFlow 内核剖析》 对 TensorFlow 的整个代码框架有了一些了解，以下是读书笔记。
Graph (计算图)是 TensorFlow 领域模型的核心。计算图就是节点与边的集合，是一个 DAG (有向无环图)图。
Node(节点)持有零条或多条输入/输出的边，分别使用 in_edges， out_edges 表示。
Edge(边) 持有前驱节点与后驱节点，从而实现了计算图的连接，也是计算图前向遍历，后向遍历的衔接点。边上的数据以 Tensor 的形 式传递。计算图中存在两类边：
普通边：用于承载 Tensor，常用实线表示； 控制依赖：控制节点的执行顺序，常用虚线表示。 TensorFlow 计算的单位是 OP，它表示了某种抽象计算。通过定义 OP 来构建 DAG 图。OP 拥有 0 个或多个「输入/输出」，及其 0 个 或多个「属性」。其中，输入/输出以Tensor 的形式存在。在系统实现中，OP 的元数据使用 Protobuf 格式的 OpDef 描述，实现前端与 后端的数据交换，及其领域模型的统一。OpDef 定义包括 OP 的名字，输入输出列表，属性列表，优化选项等。其中，属性常常用于描述 输入/输出的类型，大小，默认值，约束，及OP 的其他特性。
计算图的执行过程将按照 DAG 的拓扑排序，依次启动 OP 的运算。其中，如果存在多个入度为 0 的节点，TensorFlow 运行时可以实现 并发，同时执行多个 OP 的运算，提高执行效率。
架构设计 TensorFlow 遵循良好的分层架构：
front end ： 用户接口，负责构造计算图 runtime ： 实现计算图的拆分。提供本地运行模式和分布式运行模式，两者共享大部分设计和实现 计算层 ： 基于 Eigen 实现计算的逻辑实现；同时支持各种硬件的并行加速 通信层 ： 基于 gRPC 实现组件间的数据交换。同时支持 RDMA 设备层 ： 支持多种异构计算设备。实际执行计算的载体 前端系统 Client 是前端系统的主要组成部分，它是一个支持多语言的编程环境，且对 Python 和 C++ 的支持比较完善。实现时通过 Swig 完成对 后端 C++ 的调用。基于这些编程接口来构造计算图。"><meta property="og:type" content="article"><meta property="og:url" content="https://kylestones.github.io/blog/machinelearning/tensorflow/"><meta property="og:image" content="https://kylestones.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2018-08-12T00:00:00+00:00"><meta property="article:modified_time" content="2018-08-12T00:00:00+00:00"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://kylestones.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="TensorFlow"><meta name=twitter:description content="架构 阅读大神的 《TensorFlow 内核剖析》 对 TensorFlow 的整个代码框架有了一些了解，以下是读书笔记。
Graph (计算图)是 TensorFlow 领域模型的核心。计算图就是节点与边的集合，是一个 DAG (有向无环图)图。
Node(节点)持有零条或多条输入/输出的边，分别使用 in_edges， out_edges 表示。
Edge(边) 持有前驱节点与后驱节点，从而实现了计算图的连接，也是计算图前向遍历，后向遍历的衔接点。边上的数据以 Tensor 的形 式传递。计算图中存在两类边：
普通边：用于承载 Tensor，常用实线表示； 控制依赖：控制节点的执行顺序，常用虚线表示。 TensorFlow 计算的单位是 OP，它表示了某种抽象计算。通过定义 OP 来构建 DAG 图。OP 拥有 0 个或多个「输入/输出」，及其 0 个 或多个「属性」。其中，输入/输出以Tensor 的形式存在。在系统实现中，OP 的元数据使用 Protobuf 格式的 OpDef 描述，实现前端与 后端的数据交换，及其领域模型的统一。OpDef 定义包括 OP 的名字，输入输出列表，属性列表，优化选项等。其中，属性常常用于描述 输入/输出的类型，大小，默认值，约束，及OP 的其他特性。
计算图的执行过程将按照 DAG 的拓扑排序，依次启动 OP 的运算。其中，如果存在多个入度为 0 的节点，TensorFlow 运行时可以实现 并发，同时执行多个 OP 的运算，提高执行效率。
架构设计 TensorFlow 遵循良好的分层架构：
front end ： 用户接口，负责构造计算图 runtime ： 实现计算图的拆分。提供本地运行模式和分布式运行模式，两者共享大部分设计和实现 计算层 ： 基于 Eigen 实现计算的逻辑实现；同时支持各种硬件的并行加速 通信层 ： 基于 gRPC 实现组件间的数据交换。同时支持 RDMA 设备层 ： 支持多种异构计算设备。实际执行计算的载体 前端系统 Client 是前端系统的主要组成部分，它是一个支持多语言的编程环境，且对 Python 和 C++ 的支持比较完善。实现时通过 Swig 完成对 后端 C++ 的调用。基于这些编程接口来构造计算图。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Blogs","item":"https://kylestones.github.io/blog/"},{"@type":"ListItem","position":3,"name":"TensorFlow","item":"https://kylestones.github.io/blog/machinelearning/tensorflow/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"TensorFlow","name":"TensorFlow","description":"架构 阅读大神的 《TensorFlow 内核剖析》 对 TensorFlow 的整个代码框架有了一些了解，以下是读书笔记。\nGraph (计算图)是 TensorFlow 领域模型的核心。计算图就是节点与边的集合，是一个 DAG (有向无环图)图。\nNode(节点)持有零条或多条输入/输出的边，分别使用 in_edges， out_edges 表示。\nEdge(边) 持有前驱节点与后驱节点，从而实现了计算图的连接，也是计算图前向遍历，后向遍历的衔接点。边上的数据以 Tensor 的形 式传递。计算图中存在两类边：\n普通边：用于承载 Tensor，常用实线表示； 控制依赖：控制节点的执行顺序，常用虚线表示。 TensorFlow 计算的单位是 OP，它表示了某种抽象计算。通过定义 OP 来构建 DAG 图。OP 拥有 0 个或多个「输入/输出」，及其 0 个 或多个「属性」。其中，输入/输出以Tensor 的形式存在。在系统实现中，OP 的元数据使用 Protobuf 格式的 OpDef 描述，实现前端与 后端的数据交换，及其领域模型的统一。OpDef 定义包括 OP 的名字，输入输出列表，属性列表，优化选项等。其中，属性常常用于描述 输入/输出的类型，大小，默认值，约束，及OP 的其他特性。\n计算图的执行过程将按照 DAG 的拓扑排序，依次启动 OP 的运算。其中，如果存在多个入度为 0 的节点，TensorFlow 运行时可以实现 并发，同时执行多个 OP 的运算，提高执行效率。\n架构设计 TensorFlow 遵循良好的分层架构：\nfront end ： 用户接口，负责构造计算图 runtime ： 实现计算图的拆分。提供本地运行模式和分布式运行模式，两者共享大部分设计和实现 计算层 ： 基于 Eigen 实现计算的逻辑实现；同时支持各种硬件的并行加速 通信层 ： 基于 gRPC 实现组件间的数据交换。同时支持 RDMA 设备层 ： 支持多种异构计算设备。实际执行计算的载体 前端系统 Client 是前端系统的主要组成部分，它是一个支持多语言的编程环境，且对 Python 和 C++ 的支持比较完善。实现时通过 Swig 完成对 后端 C++ 的调用。基于这些编程接口来构造计算图。","keywords":["TensorFlow,","深度学习"],"articleBody":" 架构 阅读大神的 《TensorFlow 内核剖析》 对 TensorFlow 的整个代码框架有了一些了解，以下是读书笔记。\nGraph (计算图)是 TensorFlow 领域模型的核心。计算图就是节点与边的集合，是一个 DAG (有向无环图)图。\nNode(节点)持有零条或多条输入/输出的边，分别使用 in_edges， out_edges 表示。\nEdge(边) 持有前驱节点与后驱节点，从而实现了计算图的连接，也是计算图前向遍历，后向遍历的衔接点。边上的数据以 Tensor 的形 式传递。计算图中存在两类边：\n普通边：用于承载 Tensor，常用实线表示； 控制依赖：控制节点的执行顺序，常用虚线表示。 TensorFlow 计算的单位是 OP，它表示了某种抽象计算。通过定义 OP 来构建 DAG 图。OP 拥有 0 个或多个「输入/输出」，及其 0 个 或多个「属性」。其中，输入/输出以Tensor 的形式存在。在系统实现中，OP 的元数据使用 Protobuf 格式的 OpDef 描述，实现前端与 后端的数据交换，及其领域模型的统一。OpDef 定义包括 OP 的名字，输入输出列表，属性列表，优化选项等。其中，属性常常用于描述 输入/输出的类型，大小，默认值，约束，及OP 的其他特性。\n计算图的执行过程将按照 DAG 的拓扑排序，依次启动 OP 的运算。其中，如果存在多个入度为 0 的节点，TensorFlow 运行时可以实现 并发，同时执行多个 OP 的运算，提高执行效率。\n架构设计 TensorFlow 遵循良好的分层架构：\nfront end ： 用户接口，负责构造计算图 runtime ： 实现计算图的拆分。提供本地运行模式和分布式运行模式，两者共享大部分设计和实现 计算层 ： 基于 Eigen 实现计算的逻辑实现；同时支持各种硬件的并行加速 通信层 ： 基于 gRPC 实现组件间的数据交换。同时支持 RDMA 设备层 ： 支持多种异构计算设备。实际执行计算的载体 前端系统 Client 是前端系统的主要组成部分，它是一个支持多语言的编程环境，且对 Python 和 C++ 的支持比较完善。实现时通过 Swig 完成对 后端 C++ 的调用。基于这些编程接口来构造计算图。\n此时，TensorFlow 并未执行任何的图计算，直至与后台计算引擎建立 Session，并以 Session 为桥梁，建立 Client 与 Master 之间的 通道，并将 Protobuf 格式的 GraphDef 序列化后传递给 Master，启动计算图的执行过程。\nruntime master 在分布式的运行时环境中，Client 执行 Session.run 时，传递整个计算图给后端的 Master。此时，计算图是完整的，常称为 Full Graph。随后，Master 根据 Session.run 传参数列表递给它的 fetches， feeds ，反向遍历 Full Graph ，并按照依赖关系，对其实施 剪枝，最终计算得到最小的依赖子图，常称为 Client Graph。\n接着，Master 负责将 Client Graph 按照任务的名称分裂 ( SplitByTask ) 为多个 Graph Partition；其中，每个 Worker 对应一个 Graph Partition。随后，Master 将 Graph Partition 分别注册到相应的 Worker 上，以便在不同的 Worker 上并发执行这些 Graph Partition。最后，Master 将通知所有 Work 启动相应 Graph Partition 的执行过程。\n其中，Work 之间可能存在数据依赖关系，Master 并不参与两者之间的数据交换，它们两两之间互相通信，独立地完成交换数据，直至完 成所有计算。\nworker 对于每一个任务，TensorFlow 都将启动一个 Worker 实例。Worker 主要负责如下 4 个方面的职责:\n处理来自 Master 的请求; 对注册的 Graph Partition 按照本地计算设备集实施二次分裂 ( SplitByDevice ) ，并通知各个计算设备并发执行各个 Graph Partition; 在分布式运行时，图分裂经过两级分裂过程。在 Master 上按照任务分裂，而在 Worker 按照设备分裂。因此，得到结果都 称为子图片段，它们仅存在范围，及其大小的差异。 按照拓扑排序算法在某个计算设备上执行本地子图，并调度 OP 的 Kernel 实现; 协同任务之间的数据通信。 kernel Kernel 是 OP 在 某 种 硬 件 设 备 的 特 定 实 现， 它 负 责 执 行 OP 的 具 体 运 算。 目 前， TensorFlow 系统中包含 200 多 个标准的 OP，包括数值计算，多维数组操作，控制流，状态管理等。\n一般每一个 OP 根据设备类型都会存在一个优化了的 Kernel 实现。在运行时，运行时根据 OP 的设备约束规范，及其本地设备的类型，为 OP 选择特定的 Kernel 实现，完成该 OP 的计算。\n其中，大多数 Kernel 基于 Eigen::Tensor 实现。 Eigen::Tensor 是一个使用 C++ 模板技术，为多核 CPU/GPU 生成高效的并发代码。但 是，TensorFlow 也可以灵活地直接使用 cuDNN， cuNCCL， cuBLAS 实现更高效的 Kernel。\n此外，TensorFlow 实现了矢量化技术，在高吞吐量、以数据为中心的应用需求中，及其移动设备中，实现更高效的推理。如果对于复合 OP 的子计算过程很难表示，或执行效率低下， TensorFlow 甚至支持更高效的 Kernel 注册，其扩展性表现非常优越。\nsession 首先，Client 首次执行 CreateSessionRequest tf.Session.run 时，会将整个图序列化后，通过 gRPC 发送消息，将图传递给 Master。随 后， Master 创建一个 CreateSessionResponse MasterSession 实例，并用全局唯一的 handle 标识，最终通过返回给 Client。\nClient 启动迭代执行的过程， 并称每次迭代为一次 Step。 此时， Client 发送 RunStepRequest MasterSession 消息给 Master，消息携 带 handle 标识，用于 Master 索引相应的实例。\nMaster 收到 RunStepRequest 消息后，将执行图剪枝，分裂，优化等操作。最终按照任务 (Task)，将图划分为多个子图片段 (Graph Partition)。随后，Master 向各个 Worker 发送 RegisterGraphRequest 当 Worker 收到消息，将子图片段依次注册到各个 Worker 节点上。 RegisterGraphRequest 消息后，再次实施分裂操作，最终按照设备(Device)，将图划分为多个子图片段 (Graph Partition)。\n当 Worker 完成子图注册后，通过返回 RegisterGraphReponse 消息，并携带 graph_handle 标识。这是因为 Worker 可以并发注册并运行多 个子图，每个子图使用 graph_handle 唯一标识。\nMaster 完成子图注册后，将广播所有 Worker 并发执行所有子图。这个过程是通过Master 发送 RunGraphRequest 收到消息消息给 Worker 完成的。\nWorker 收到 RunGraphRequest 消息后，Worker 根据 graph_handle 索引相应的子图。最终， Worker 启动本地所有计算设备并发执行所 有子图。其中，每个子图放置在单独的 Executor 中执行， Executor 将按照拓扑排序算法完成子图片段的计算。\n如果两个设备之间需要交换数据，则通过插入 Send/Recv 节点完成的。特殊地，如果两个 Worker 之间需要交换数据，则需要涉及跨进程间 的通信。此时，需要通过接收端主动发送里取出对应的 Tensor，并通过 RecvTensorRequest RecvTensorResponse 消息到发送方，再从发送 方的信箱返回。\n当计算完成后， Client 向 Master 发送始释放 MasterSession CloseSessionReq 消息。Master 收到消息后，开始释放所持有的所有资 源。\n使用 x_train， y_train = load_data() X = tf.placeholder(......) # 定义数据集占位符 Y = tf.placeholder(......) W = tf.Variable(......) # 定义变量 b = tf.Variable(......) init = tf.global_variables_initializer() # 定义初始化变量的方式 Z = tf.add(tf.matmul(W， X)， b) # 定义计算图 loss = tf.losses...... # 定义损失函数，Z和Y作为参数 optimizer = tf.train.AdamOptimizer().minimize(loss) # 定义优化器 with tf.Session as sess: sess.run(init) # 开始执行变量初始化 for _ in range(num_epochs): # 循环训练多轮 _ ， cost = sess.run([optimizer， cost]， feed_dict={X:x_train， Y:y_train }) # 执行训练 使用 batch normalation\ndef conv_layer(inpt, filter_shape, stride): out_channels = filter_shape[3] filter_ = weight_variable(filter_shape) conv = tf.nn.conv2d(inpt, filter=filter_, strides=[1, stride, stride, 1], padding=\"SAME\") mean, var = tf.nn.moments(conv, axes=[0,1,2])#计算一阶矩（均值），以及二阶矩（方差） beta = tf.Variable(tf.zeros([out_channels]), name=\"beta\") gamma = weight_variable([out_channels], name=\"gamma\") batch_norm = tf.nn.batch_norm_with_global_normalization( conv, mean, var, beta, gamma, 0.001, scale_after_normalization=True) out = tf.nn.relu(batch_norm) return out ","wordCount":"502","inLanguage":"en","datePublished":"2018-08-12T00:00:00Z","dateModified":"2018-08-12T00:00:00Z","author":{"@type":"Person","name":"Kyle Three Stones"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://kylestones.github.io/blog/machinelearning/tensorflow/"},"publisher":{"@type":"Organization","name":"Org Mode","logo":{"@type":"ImageObject","url":"https://kylestones.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://kylestones.github.io accesskey=h title="Home (Alt + H)"><img src=https://kylestones.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://kylestones.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://kylestones.github.io/tags/ title=tags><span>tags</span></a></li><li><a href=https://example.org title=example.org><span>example.org</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://kylestones.github.io>Home</a>&nbsp;»&nbsp;<a href=https://kylestones.github.io/blog/>Blogs</a></div><h1 class=post-title>TensorFlow</h1><div class=post-meta><span title='2018-08-12 00:00:00 +0000 UTC'>August 12, 2018</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;502 words&nbsp;·&nbsp;Kyle Three Stones&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/blog/machinelearning/tensorflow.org rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><div id=outline-container-headline-1 class=outline-3><h3 id=headline-1>架构</h3><div id=outline-text-headline-1 class=outline-text-3><p>阅读大神的 <a href=https://github.com/horance-liu/tensorflow-internals>《TensorFlow 内核剖析》</a> 对 TensorFlow 的整个代码框架有了一些了解，以下是读书笔记。</p><p>Graph (计算图)是 TensorFlow 领域模型的核心。计算图就是节点与边的集合，是一个 DAG (有向无环图)图。</p><p>Node(节点)持有零条或多条输入/输出的边，分别使用 in_edges， out_edges 表示。</p><p>Edge(边) 持有前驱节点与后驱节点，从而实现了计算图的连接，也是计算图前向遍历，后向遍历的衔接点。边上的数据以 Tensor 的形
式传递。计算图中存在两类边：</p><ul><li>普通边：用于承载 Tensor，常用实线表示；</li><li>控制依赖：控制节点的执行顺序，常用虚线表示。</li></ul><p>TensorFlow 计算的单位是 OP，它表示了某种抽象计算。通过定义 OP 来构建 DAG 图。OP 拥有 0 个或多个「输入/输出」，及其 0 个
或多个「属性」。其中，输入/输出以Tensor 的形式存在。在系统实现中，OP 的元数据使用 Protobuf 格式的 OpDef 描述，实现前端与
后端的数据交换，及其领域模型的统一。OpDef 定义包括 OP 的名字，输入输出列表，属性列表，优化选项等。其中，属性常常用于描述
输入/输出的类型，大小，默认值，约束，及OP 的其他特性。</p><p>计算图的执行过程将按照 DAG 的拓扑排序，依次启动 OP 的运算。其中，如果存在多个入度为 0 的节点，TensorFlow 运行时可以实现
并发，同时执行多个 OP 的运算，提高执行效率。</p><div id=outline-container-headline-2 class=outline-4><h4 id=headline-2>架构设计</h4><div id=outline-text-headline-2 class=outline-text-4><p>TensorFlow 遵循良好的分层架构：</p><ol><li>front end ： 用户接口，负责构造计算图</li><li>runtime ： 实现计算图的拆分。提供本地运行模式和分布式运行模式，两者共享大部分设计和实现</li><li>计算层 ： 基于 Eigen 实现计算的逻辑实现；同时支持各种硬件的并行加速</li><li>通信层 ： 基于 gRPC 实现组件间的数据交换。同时支持 RDMA</li><li>设备层 ： 支持多种异构计算设备。实际执行计算的载体</li></ol><div id=outline-container-headline-3 class=outline-5><h5 id=headline-3>前端系统</h5><div id=outline-text-headline-3 class=outline-text-5><p>Client 是前端系统的主要组成部分，它是一个支持多语言的编程环境，且对 Python 和 C++ 的支持比较完善。实现时通过 Swig 完成对
后端 C++ 的调用。基于这些编程接口来构造计算图。</p><p>此时，TensorFlow 并未执行任何的图计算，直至与后台计算引擎建立 Session，并以 Session 为桥梁，建立 Client 与 Master 之间的
通道，并将 Protobuf 格式的 GraphDef 序列化后传递给 Master，启动计算图的执行过程。</p></div></div><div id=outline-container-headline-4 class=outline-5><h5 id=headline-4>runtime</h5><div id=outline-text-headline-4 class=outline-text-5><div id=outline-container-headline-5 class=outline-6><h6 id=headline-5>master</h6><div id=outline-text-headline-5 class=outline-text-6><p>在分布式的运行时环境中，Client 执行 Session.run 时，传递整个计算图给后端的 Master。此时，计算图是完整的，常称为 Full
Graph。随后，Master 根据 Session.run 传参数列表递给它的 fetches， feeds ，反向遍历 Full Graph ，并按照依赖关系，对其实施
剪枝，最终计算得到最小的依赖子图，常称为 Client Graph。</p><p>接着，Master 负责将 Client Graph 按照任务的名称分裂 ( SplitByTask ) 为多个 Graph Partition；其中，每个 Worker 对应一个
Graph Partition。随后，Master 将 Graph Partition 分别注册到相应的 Worker 上，以便在不同的 Worker 上并发执行这些 Graph
Partition。最后，Master 将通知所有 Work 启动相应 Graph Partition 的执行过程。</p><p>其中，Work 之间可能存在数据依赖关系，Master 并不参与两者之间的数据交换，它们两两之间互相通信，独立地完成交换数据，直至完
成所有计算。</p></div></div><div id=outline-container-headline-6 class=outline-6><h6 id=headline-6>worker</h6><div id=outline-text-headline-6 class=outline-text-6><p>对于每一个任务，TensorFlow 都将启动一个 Worker 实例。Worker 主要负责如下 4 个方面的职责:</p><ol><li>处理来自 Master 的请求;</li><li>对注册的 Graph Partition 按照本地计算设备集实施二次分裂 ( SplitByDevice ) ，并通知各个计算设备并发执行各个 Graph
Partition; 在分布式运行时，图分裂经过两级分裂过程。在 Master 上按照任务分裂，而在 Worker 按照设备分裂。因此，得到结果都
称为子图片段，它们仅存在范围，及其大小的差异。</li><li>按照拓扑排序算法在某个计算设备上执行本地子图，并调度 OP 的 Kernel 实现;</li><li>协同任务之间的数据通信。</li></ol></div></div></div></div><div id=outline-container-headline-7 class=outline-5><h5 id=headline-7>kernel</h5><div id=outline-text-headline-7 class=outline-text-5><p>Kernel 是 OP 在 某 种 硬 件 设 备 的 特 定 实 现， 它 负 责 执 行 OP 的 具 体 运 算。 目 前， TensorFlow 系统中包含 200 多
个标准的 OP，包括数值计算，多维数组操作，控制流，状态管理等。</p><p>一般每一个 OP 根据设备类型都会存在一个优化了的 Kernel 实现。在运行时，运行时根据 OP 的设备约束规范，及其本地设备的类型，为
OP 选择特定的 Kernel 实现，完成该 OP 的计算。</p><p>其中，大多数 Kernel 基于 Eigen::Tensor 实现。 Eigen::Tensor 是一个使用 C++ 模板技术，为多核 CPU/GPU 生成高效的并发代码。但
是，TensorFlow 也可以灵活地直接使用 cuDNN， cuNCCL， cuBLAS 实现更高效的 Kernel。</p><p>此外，TensorFlow 实现了矢量化技术，在高吞吐量、以数据为中心的应用需求中，及其移动设备中，实现更高效的推理。如果对于复合 OP
的子计算过程很难表示，或执行效率低下， TensorFlow 甚至支持更高效的 Kernel 注册，其扩展性表现非常优越。</p></div></div><div id=outline-container-headline-8 class=outline-5><h5 id=headline-8>session</h5><div id=outline-text-headline-8 class=outline-text-5><p>首先，Client 首次执行 CreateSessionRequest tf.Session.run 时，会将整个图序列化后，通过 gRPC 发送消息，将图传递给 Master。随
后， Master 创建一个 CreateSessionResponse MasterSession 实例，并用全局唯一的 handle 标识，最终通过返回给 Client。</p><p>Client 启动迭代执行的过程， 并称每次迭代为一次 Step。 此时， Client 发送 RunStepRequest MasterSession 消息给 Master，消息携
带 handle 标识，用于 Master 索引相应的实例。</p><p>Master 收到 RunStepRequest 消息后，将执行图剪枝，分裂，优化等操作。最终按照任务 (Task)，将图划分为多个子图片段 (Graph
Partition)。随后，Master 向各个 Worker 发送 RegisterGraphRequest 当 Worker 收到消息，将子图片段依次注册到各个 Worker 节点上。
RegisterGraphRequest 消息后，再次实施分裂操作，最终按照设备(Device)，将图划分为多个子图片段 (Graph Partition)。</p><p>当 Worker 完成子图注册后，通过返回 RegisterGraphReponse 消息，并携带 graph_handle 标识。这是因为 Worker 可以并发注册并运行多
个子图，每个子图使用 graph_handle 唯一标识。</p><p>Master 完成子图注册后，将广播所有 Worker 并发执行所有子图。这个过程是通过Master 发送 RunGraphRequest 收到消息消息给
Worker 完成的。</p><p>Worker 收到 RunGraphRequest 消息后，Worker 根据 graph_handle 索引相应的子图。最终， Worker 启动本地所有计算设备并发执行所
有子图。其中，每个子图放置在单独的 Executor 中执行， Executor 将按照拓扑排序算法完成子图片段的计算。</p><p>如果两个设备之间需要交换数据，则通过插入 Send/Recv 节点完成的。特殊地，如果两个 Worker 之间需要交换数据，则需要涉及跨进程间
的通信。此时，需要通过接收端主动发送里取出对应的 Tensor，并通过 RecvTensorRequest RecvTensorResponse 消息到发送方，再从发送
方的信箱返回。</p><p>当计算完成后， Client 向 Master 发送始释放 MasterSession CloseSessionReq 消息。Master 收到消息后，开始释放所持有的所有资
源。</p></div></div></div></div></div></div><div id=outline-container-headline-9 class=outline-3><h3 id=headline-9>使用</h3><div id=outline-text-headline-9 class=outline-text-3><div class="src src-python"><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x_train</span><span class=err>，</span> <span class=n>y_train</span> <span class=o>=</span> <span class=n>load_data</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>placeholder</span><span class=p>(</span><span class=o>......</span><span class=p>)</span> <span class=c1># 定义数据集占位符</span>
</span></span><span class=line><span class=cl><span class=n>Y</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>placeholder</span><span class=p>(</span><span class=o>......</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>W</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>Variable</span><span class=p>(</span><span class=o>......</span><span class=p>)</span> <span class=c1># 定义变量</span>
</span></span><span class=line><span class=cl><span class=n>b</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>Variable</span><span class=p>(</span><span class=o>......</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>init</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>global_variables_initializer</span><span class=p>()</span> <span class=c1># 定义初始化变量的方式</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>Z</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>W</span><span class=err>，</span> <span class=n>X</span><span class=p>)</span><span class=err>，</span> <span class=n>b</span><span class=p>)</span> <span class=c1># 定义计算图</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>loss</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>losses</span><span class=o>......</span> <span class=c1># 定义损失函数，Z和Y作为参数</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>train</span><span class=o>.</span><span class=n>AdamOptimizer</span><span class=p>()</span><span class=o>.</span><span class=n>minimize</span><span class=p>(</span><span class=n>loss</span><span class=p>)</span> <span class=c1># 定义优化器</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>Session</span> <span class=k>as</span> <span class=n>sess</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>sess</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>init</span><span class=p>)</span> <span class=c1># 开始执行变量初始化</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>):</span> <span class=c1># 循环训练多轮</span>
</span></span><span class=line><span class=cl>        <span class=n>_</span> <span class=err>，</span> <span class=n>cost</span> <span class=o>=</span> <span class=n>sess</span><span class=o>.</span><span class=n>run</span><span class=p>([</span><span class=n>optimizer</span><span class=err>，</span> <span class=n>cost</span><span class=p>]</span><span class=err>，</span> <span class=n>feed_dict</span><span class=o>=</span><span class=p>{</span><span class=n>X</span><span class=p>:</span><span class=n>x_train</span><span class=err>，</span> <span class=n>Y</span><span class=p>:</span><span class=n>y_train</span> <span class=p>})</span> <span class=c1># 执行训练</span></span></span></code></pre></div></div><p>使用 batch normalation</p><div class="src src-python"><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>conv_layer</span><span class=p>(</span><span class=n>inpt</span><span class=p>,</span> <span class=n>filter_shape</span><span class=p>,</span> <span class=n>stride</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>out_channels</span> <span class=o>=</span> <span class=n>filter_shape</span><span class=p>[</span><span class=mi>3</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>filter_</span> <span class=o>=</span> <span class=n>weight_variable</span><span class=p>(</span><span class=n>filter_shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>conv</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>conv2d</span><span class=p>(</span><span class=n>inpt</span><span class=p>,</span> <span class=nb>filter</span><span class=o>=</span><span class=n>filter_</span><span class=p>,</span> <span class=n>strides</span><span class=o>=</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=n>stride</span><span class=p>,</span> <span class=n>stride</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>padding</span><span class=o>=</span><span class=s2>&#34;SAME&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>mean</span><span class=p>,</span> <span class=n>var</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>moments</span><span class=p>(</span><span class=n>conv</span><span class=p>,</span> <span class=n>axes</span><span class=o>=</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>1</span><span class=p>,</span><span class=mi>2</span><span class=p>])</span><span class=c1>#计算一阶矩（均值），以及二阶矩（方差）</span>
</span></span><span class=line><span class=cl>    <span class=n>beta</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>Variable</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>zeros</span><span class=p>([</span><span class=n>out_channels</span><span class=p>]),</span> <span class=n>name</span><span class=o>=</span><span class=s2>&#34;beta&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>gamma</span> <span class=o>=</span> <span class=n>weight_variable</span><span class=p>([</span><span class=n>out_channels</span><span class=p>],</span> <span class=n>name</span><span class=o>=</span><span class=s2>&#34;gamma&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>batch_norm</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>batch_norm_with_global_normalization</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>conv</span><span class=p>,</span> <span class=n>mean</span><span class=p>,</span> <span class=n>var</span><span class=p>,</span> <span class=n>beta</span><span class=p>,</span> <span class=n>gamma</span><span class=p>,</span> <span class=mf>0.001</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>scale_after_normalization</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>out</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>batch_norm</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>out</span></span></span></code></pre></div></div></div></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://kylestones.github.io/tags/tensorflow/>TensorFlow,</a></li><li><a href=https://kylestones.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>深度学习</a></li></ul><nav class=paginav><a class=prev href=https://kylestones.github.io/blog/machinelearning/loss-function/><span class=title>« Prev</span><br><span>损失函数</span></a>
<a class=next href=https://kylestones.github.io/blog/machinelearning/deeplearning/><span class=title>Next »</span><br><span>Deep Learning</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share TensorFlow on twitter" href="https://twitter.com/intent/tweet/?text=TensorFlow&url=https%3a%2f%2fkylestones.github.io%2fblog%2fmachinelearning%2ftensorflow%2f&hashtags=TensorFlow%2c%2c%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share TensorFlow on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fkylestones.github.io%2fblog%2fmachinelearning%2ftensorflow%2f&title=TensorFlow&summary=TensorFlow&source=https%3a%2f%2fkylestones.github.io%2fblog%2fmachinelearning%2ftensorflow%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share TensorFlow on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fkylestones.github.io%2fblog%2fmachinelearning%2ftensorflow%2f&title=TensorFlow"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share TensorFlow on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fkylestones.github.io%2fblog%2fmachinelearning%2ftensorflow%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share TensorFlow on whatsapp" href="https://api.whatsapp.com/send?text=TensorFlow%20-%20https%3a%2f%2fkylestones.github.io%2fblog%2fmachinelearning%2ftensorflow%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share TensorFlow on telegram" href="https://telegram.me/share/url?text=TensorFlow&url=https%3a%2f%2fkylestones.github.io%2fblog%2fmachinelearning%2ftensorflow%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://kylestones.github.io>Org Mode</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>