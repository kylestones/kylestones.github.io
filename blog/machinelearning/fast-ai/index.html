<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>fast.ai | Org Mode</title><meta name=keywords content="深度学习"><meta name=description content="调参 分阶段使用多个学习速率 越接近输入的层，在 fine-tune 的时候，参数需要调节的越小，越靠近输出层，参数需要调节的越大。因此在 fine-tune 的时候，不同 的层最好使用不同的学习速率。例如 1-2 层使用 0.001 ，3-4 层使用 0.01 ，5-6 层使用 0.1 等等，越远离输出层，逐渐让学习速率 缩小 10 倍。[ Once the last layers are producing good results, we implement differential learning rates to alter the lower layers as well. The lower layers want to be altered less, so it is good practice to set each learning rate to be 10 times lower than the last. ]
找到最优学习速率 学习速率几乎是训练神经网络时的最重要的超参。可以通过让学习速率从一个很小的值开始，然后每个 mini-batch 都以指数级增长，同 时记录每个学习速率所对应的 loss ，然后画出 loss 和学习速率的关系曲线图，找到学习速率最大，但 loss 仍然在下降的点，这个学 习速率就是最好的学习速率。[ Do a trial run and train the neural network using a low learning rate, but increase it exponentially with each batch."><meta name=author content="Kyle Three Stones"><link rel=canonical href=https://kylestones.github.io/blog/machinelearning/fast-ai/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://kylestones.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://kylestones.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://kylestones.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://kylestones.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://kylestones.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="fast.ai"><meta property="og:description" content="调参 分阶段使用多个学习速率 越接近输入的层，在 fine-tune 的时候，参数需要调节的越小，越靠近输出层，参数需要调节的越大。因此在 fine-tune 的时候，不同 的层最好使用不同的学习速率。例如 1-2 层使用 0.001 ，3-4 层使用 0.01 ，5-6 层使用 0.1 等等，越远离输出层，逐渐让学习速率 缩小 10 倍。[ Once the last layers are producing good results, we implement differential learning rates to alter the lower layers as well. The lower layers want to be altered less, so it is good practice to set each learning rate to be 10 times lower than the last. ]
找到最优学习速率 学习速率几乎是训练神经网络时的最重要的超参。可以通过让学习速率从一个很小的值开始，然后每个 mini-batch 都以指数级增长，同 时记录每个学习速率所对应的 loss ，然后画出 loss 和学习速率的关系曲线图，找到学习速率最大，但 loss 仍然在下降的点，这个学 习速率就是最好的学习速率。[ Do a trial run and train the neural network using a low learning rate, but increase it exponentially with each batch."><meta property="og:type" content="article"><meta property="og:url" content="https://kylestones.github.io/blog/machinelearning/fast-ai/"><meta property="og:image" content="https://kylestones.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2019-03-24T00:00:00+00:00"><meta property="article:modified_time" content="2019-03-24T00:00:00+00:00"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://kylestones.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="fast.ai"><meta name=twitter:description content="调参 分阶段使用多个学习速率 越接近输入的层，在 fine-tune 的时候，参数需要调节的越小，越靠近输出层，参数需要调节的越大。因此在 fine-tune 的时候，不同 的层最好使用不同的学习速率。例如 1-2 层使用 0.001 ，3-4 层使用 0.01 ，5-6 层使用 0.1 等等，越远离输出层，逐渐让学习速率 缩小 10 倍。[ Once the last layers are producing good results, we implement differential learning rates to alter the lower layers as well. The lower layers want to be altered less, so it is good practice to set each learning rate to be 10 times lower than the last. ]
找到最优学习速率 学习速率几乎是训练神经网络时的最重要的超参。可以通过让学习速率从一个很小的值开始，然后每个 mini-batch 都以指数级增长，同 时记录每个学习速率所对应的 loss ，然后画出 loss 和学习速率的关系曲线图，找到学习速率最大，但 loss 仍然在下降的点，这个学 习速率就是最好的学习速率。[ Do a trial run and train the neural network using a low learning rate, but increase it exponentially with each batch."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Blogs","item":"https://kylestones.github.io/blog/"},{"@type":"ListItem","position":3,"name":"fast.ai","item":"https://kylestones.github.io/blog/machinelearning/fast-ai/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"fast.ai","name":"fast.ai","description":"调参 分阶段使用多个学习速率 越接近输入的层，在 fine-tune 的时候，参数需要调节的越小，越靠近输出层，参数需要调节的越大。因此在 fine-tune 的时候，不同 的层最好使用不同的学习速率。例如 1-2 层使用 0.001 ，3-4 层使用 0.01 ，5-6 层使用 0.1 等等，越远离输出层，逐渐让学习速率 缩小 10 倍。[ Once the last layers are producing good results, we implement differential learning rates to alter the lower layers as well. The lower layers want to be altered less, so it is good practice to set each learning rate to be 10 times lower than the last. ]\n找到最优学习速率 学习速率几乎是训练神经网络时的最重要的超参。可以通过让学习速率从一个很小的值开始，然后每个 mini-batch 都以指数级增长，同 时记录每个学习速率所对应的 loss ，然后画出 loss 和学习速率的关系曲线图，找到学习速率最大，但 loss 仍然在下降的点，这个学 习速率就是最好的学习速率。[ Do a trial run and train the neural network using a low learning rate, but increase it exponentially with each batch.","keywords":["深度学习"],"articleBody":" 调参 分阶段使用多个学习速率 越接近输入的层，在 fine-tune 的时候，参数需要调节的越小，越靠近输出层，参数需要调节的越大。因此在 fine-tune 的时候，不同 的层最好使用不同的学习速率。例如 1-2 层使用 0.001 ，3-4 层使用 0.01 ，5-6 层使用 0.1 等等，越远离输出层，逐渐让学习速率 缩小 10 倍。[ Once the last layers are producing good results, we implement differential learning rates to alter the lower layers as well. The lower layers want to be altered less, so it is good practice to set each learning rate to be 10 times lower than the last. ]\n找到最优学习速率 学习速率几乎是训练神经网络时的最重要的超参。可以通过让学习速率从一个很小的值开始，然后每个 mini-batch 都以指数级增长，同 时记录每个学习速率所对应的 loss ，然后画出 loss 和学习速率的关系曲线图，找到学习速率最大，但 loss 仍然在下降的点，这个学 习速率就是最好的学习速率。[ Do a trial run and train the neural network using a low learning rate, but increase it exponentially with each batch. Meanwhile, the loss is recorded for every value of the learning rate. We then plot loss against learning rate. The optimum learning rate is determined by finding the value where the learning rate is highest and the loss is still descending. ]\ncosine annealing 使用 SGD 训练网络的时候，参数会逐渐接近全局最优解，当越来越接近最优解的时候，我们不希望以为学习速率过大，而导致跳过全局 最优解。余弦退火使得学习速率曲线类似余弦函数，先缓慢减小，然后迅速减小，最后右变成缓慢减小（目的应该就是希望使用这里的缓 慢减小）。[ With each batch of stochastic gradient descent (SGD), your network should be getting closer and closer to a global minimum value for the loss. As it gets closer to this minimum, it hence makes sense that the learning rate should get smaller so that your algorithm does not overshoot, and instead settles as close to this point as possible. Cosine annealing solves this problem by decreasing the learning rate following the cosine function. As we increase x the cosine value descends slowly at first, then more quickly and then slightly slower again. This mode of decreasing works well with the learning rate, yielding great results in a computationally efficient manner. ]\n带重启的随机梯度下降法 训练的时候很有可能陷入局部最优解，而非全局最优解。通过突然增大学习速率，可以跳出局部最优解，这种方法称为 SGDR。 [ During training it is possible for gradient descent to get stuck at local minima rather than the global minimum. By increasing the learning rate suddenly, gradient descent may “hop” out of the local minima and find its way toward the global minimum. Doing this is called stochastic gradient descent with restarts (SGDR). ]\n带重启的随机梯度下降法结合余弦退火以及分阶段不同的学习速率是 fast.ai 取得良好的图像分类结果的关键。\nfast.ai 库中有两个参数 cycle_mult 和 cycle_len 两个参数用于设置余弦退火周期的长度，以及之后周期长度需要乘以的倍数。（随 着迭代次数的增加，周期会变得越来越长。\n创造力是关键 硅谷的大公司可能有成千上万的 GPU ，但是你可以利用自己的思考、直觉与创新打败他们。有时候因为有限制才有了更好的事物被创造 出来。\n为什么深度网络难以训练 When we look closely, we'll discover that the different layers in our deep network are learning at vastly different speeds. In particular, when later layers in the network are learning well, early layers often get stuck during training, learning almost nothing at all.\nAs we delve into the problem more deeply, we'll learn that the opposite phenomenon can also occur: the early layers may be learning well, but later layers can become stuck.\nTo generate these results, I used batch gradient descent with just 1,000 training images, trained over 500 epochs. Rather than using all images and mini-batch.\nThe random initialization means the first layer throws away most information about the input image. Even if later layers have been extensively trained, they will still find it extremely difficult to identify the input image, simply because they don't have enough information.\nThe unstable gradient problem: The fundamental problem here isn't so much the vanishing gradient problem or the exploding gradient problem. It's that the gradient in early layers is the product of terms from all the later layers. When there are many layers, that's an intrinsically unstable situation. The only way all layers can learn at close to the same speed is if all those products of terms come close to balancing out. Without some mechanism or underlying reason for that balancing to occur, it's highly unlikely to happen simply by chance. In short, the real problem here is that neural networks suffer from an unstable gradient problem. As a result, if we use standard gradient-based learning techniques, different layers in the network will tend to learn at wildly different speeds.\n下载自己的图片 在谷歌浏览器查找自己感兴趣的图片，然后输入 Ctrl+shift+J 然后粘贴下面代码，并回车即可下载图片的 url\nurls = Array.from(document.querySelectorAll('.rg_di .rg_meta')).map(e1=\u003eJSON.parse(e1.textContent).ou); window.open('data:text/csv;charset=utf-8,' + escape(urls.join('\\n'))); 模型压缩 通常直接训练一个小型的网络，比训练一个大型的网络，然后压缩到同等规模的小网络，效果要差。所以一般都是先训练一个大型的网络， 然后进行压缩。\nCNN 模型的大小，也就是网络的复杂度代表了其学习能力的容量。在没训练出模型之前，我们并不知道究竟多大的网络才适合我们给定的 任务和数据集，我们并不知道多少的学习容量才是合适的。所以上面说的大小可能根本无法估计。\n普遍观念认为模型压缩通常能大幅减少参数数量，压缩空间，从而降低计算量。从而更好的部署到手机或者无人机上。\n在剪枝过程中，根据一定的标准，对冗余权重进行修剪并保留重要权重，以最大限度地保持精确性。\n步骤\n首先训练一个大型模型 然后进行剪枝 最后微调 方法\npruning – 剪枝 quantization knowledge distillation – 知识蒸馏 low-rank decomposition compact architecture design 文章 Rethinking the Value of Network Pruning 指出，自动剪枝，选择的并不是重要的权重，而是在进行网络架构搜索 network architecture search\n记得微软有一篇文章提出更好的训练网络的方法：由于剪枝后的网络比同等规模的网络效果好，说明网络没有被很好的训练。\n参考\n为什么要压缩模型，而不是直接训练一个小的CNN？ 参考 Ten Techniques Learned From fast.ai Why are deep neural networks hard to train? ","wordCount":"670","inLanguage":"en","datePublished":"2019-03-24T00:00:00Z","dateModified":"2019-03-24T00:00:00Z","author":{"@type":"Person","name":"Kyle Three Stones"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://kylestones.github.io/blog/machinelearning/fast-ai/"},"publisher":{"@type":"Organization","name":"Org Mode","logo":{"@type":"ImageObject","url":"https://kylestones.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://kylestones.github.io accesskey=h title="Home (Alt + H)"><img src=https://kylestones.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://kylestones.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://kylestones.github.io/tags/ title=tags><span>tags</span></a></li><li><a href=https://example.org title=example.org><span>example.org</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://kylestones.github.io>Home</a>&nbsp;»&nbsp;<a href=https://kylestones.github.io/blog/>Blogs</a></div><h1 class=post-title>fast.ai</h1><div class=post-meta><span title='2019-03-24 00:00:00 +0000 UTC'>March 24, 2019</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;670 words&nbsp;·&nbsp;Kyle Three Stones&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/blog/machinelearning/fast-ai.org rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><div id=outline-container-headline-1 class=outline-3><h3 id=headline-1>调参</h3><div id=outline-text-headline-1 class=outline-text-3><div id=outline-container-headline-2 class=outline-4><h4 id=headline-2>分阶段使用多个学习速率</h4><div id=outline-text-headline-2 class=outline-text-4><p>越接近输入的层，在 fine-tune 的时候，参数需要调节的越小，越靠近输出层，参数需要调节的越大。因此在 fine-tune 的时候，不同
的层最好使用不同的学习速率。例如 1-2 层使用 0.001 ，3-4 层使用 0.01 ，5-6 层使用 0.1 等等，越远离输出层，逐渐让学习速率
缩小 10 倍。[ Once the last layers are producing good results, we implement differential learning rates to alter the
lower layers as well. The lower layers want to be altered less, so it is good practice to set each learning rate to be
10 times lower than the last. ]</p></div></div><div id=outline-container-headline-3 class=outline-4><h4 id=headline-3>找到最优学习速率</h4><div id=outline-text-headline-3 class=outline-text-4><p>学习速率几乎是训练神经网络时的最重要的超参。可以通过让学习速率从一个很小的值开始，然后每个 mini-batch 都以指数级增长，同
时记录每个学习速率所对应的 loss ，然后画出 loss 和学习速率的关系曲线图，找到学习速率最大，但 loss 仍然在下降的点，这个学
习速率就是最好的学习速率。[ Do a trial run and train the neural network using a low learning rate, but increase it
exponentially with each batch. Meanwhile, the loss is recorded for every value of the learning rate. We then plot loss
against learning rate. The optimum learning rate is determined by finding the value where the learning rate is highest
and the loss is still descending. ]</p></div></div><div id=outline-container-headline-4 class=outline-4><h4 id=headline-4>cosine annealing</h4><div id=outline-text-headline-4 class=outline-text-4><p>使用 SGD 训练网络的时候，参数会逐渐接近全局最优解，当越来越接近最优解的时候，我们不希望以为学习速率过大，而导致跳过全局
最优解。余弦退火使得学习速率曲线类似余弦函数，先缓慢减小，然后迅速减小，最后右变成缓慢减小（目的应该就是希望使用这里的缓
慢减小）。[ With each batch of stochastic gradient descent (SGD), your network should be getting closer and closer to a
global minimum value for the loss. As it gets closer to this minimum, it hence makes sense that the learning rate should
get smaller so that your algorithm does not overshoot, and instead settles as close to this point as possible. Cosine
annealing solves this problem by decreasing the learning rate following the cosine function. As we increase x the cosine
value descends slowly at first, then more quickly and then slightly slower again. This mode of decreasing works well
with the learning rate, yielding great results in a computationally efficient manner. ]</p></div></div><div id=outline-container-headline-5 class=outline-4><h4 id=headline-5>带重启的随机梯度下降法</h4><div id=outline-text-headline-5 class=outline-text-4><p>训练的时候很有可能陷入局部最优解，而非全局最优解。通过突然增大学习速率，可以跳出局部最优解，这种方法称为 SGDR。 [ During
training it is possible for gradient descent to get stuck at local minima rather than the global minimum. By increasing
the learning rate suddenly, gradient descent may “hop” out of the local minima and find its way toward the global
minimum. Doing this is called stochastic gradient descent with restarts (SGDR). ]</p><p><strong>带重启的随机梯度下降法结合余弦退火以及分阶段不同的学习速率是 fast.ai 取得良好的图像分类结果的关键。</strong></p><p>fast.ai 库中有两个参数 cycle_mult 和 cycle_len 两个参数用于设置余弦退火周期的长度，以及之后周期长度需要乘以的倍数。（随
着迭代次数的增加，周期会变得越来越长。</p></div></div><div id=outline-container-headline-6 class=outline-4><h4 id=headline-6>创造力是关键</h4><div id=outline-text-headline-6 class=outline-text-4><p>硅谷的大公司可能有成千上万的 GPU ，但是你可以利用自己的思考、直觉与创新打败他们。有时候因为有限制才有了更好的事物被创造
出来。</p></div></div></div></div><div id=outline-container-headline-7 class=outline-3><h3 id=headline-7>为什么深度网络难以训练</h3><div id=outline-text-headline-7 class=outline-text-3><p>When we look closely, we'll discover that the different layers in our deep network are learning at vastly different
speeds. In particular, when later layers in the network are learning well, early layers often get stuck during training,
learning almost nothing at all.</p><p>As we delve into the problem more deeply, we'll learn that the opposite phenomenon can also occur: the early layers may
be learning well, but later layers can become stuck.</p><p>To generate these results, I used batch gradient descent with just 1,000 training images, trained over 500 epochs.
Rather than using all images and mini-batch.</p><p>The random initialization means the first layer throws away most information about the input image. Even if later layers
have been extensively trained, they will still find it extremely difficult to identify the input image, simply because
they don't have enough information.</p><p>The unstable gradient problem: The fundamental problem here isn't so much the vanishing gradient problem or the
exploding gradient problem. It's that the gradient in early layers is the product of terms from all the later layers.
When there are many layers, that's an intrinsically unstable situation. The only way all layers can learn at close to
the same speed is if all those products of terms come close to balancing out. Without some mechanism or underlying
reason for that balancing to occur, it's highly unlikely to happen simply by chance. In short, the real problem here is
that neural networks suffer from an unstable gradient problem. As a result, if we use standard gradient-based learning
techniques, different layers in the network will tend to learn at wildly different speeds.</p></div></div><div id=outline-container-headline-8 class=outline-3><h3 id=headline-8>下载自己的图片</h3><div id=outline-text-headline-8 class=outline-text-3><p>在谷歌浏览器查找自己感兴趣的图片，然后输入 Ctrl+shift+J 然后粘贴下面代码，并回车即可下载图片的 url</p><div class="src src-javascript"><div class=highlight><pre tabindex=0 class=chroma><code class=language-javascript data-lang=javascript><span class=line><span class=cl><span class=nx>urls</span> <span class=o>=</span> <span class=nb>Array</span><span class=p>.</span><span class=nx>from</span><span class=p>(</span><span class=nb>document</span><span class=p>.</span><span class=nx>querySelectorAll</span><span class=p>(</span><span class=s1>&#39;.rg_di .rg_meta&#39;</span><span class=p>)).</span><span class=nx>map</span><span class=p>(</span><span class=nx>e1</span><span class=p>=&gt;</span><span class=nx>JSON</span><span class=p>.</span><span class=nx>parse</span><span class=p>(</span><span class=nx>e1</span><span class=p>.</span><span class=nx>textContent</span><span class=p>).</span><span class=nx>ou</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=nb>window</span><span class=p>.</span><span class=nx>open</span><span class=p>(</span><span class=s1>&#39;data:text/csv;charset=utf-8,&#39;</span> <span class=o>+</span> <span class=nx>escape</span><span class=p>(</span><span class=nx>urls</span><span class=p>.</span><span class=nx>join</span><span class=p>(</span><span class=s1>&#39;\n&#39;</span><span class=p>)));</span></span></span></code></pre></div></div></div></div><div id=outline-container-headline-9 class=outline-3><h3 id=headline-9>模型压缩</h3><div id=outline-text-headline-9 class=outline-text-3><p>通常直接训练一个小型的网络，比训练一个大型的网络，然后压缩到同等规模的小网络，效果要差。所以一般都是先训练一个大型的网络，
然后进行压缩。</p><p>CNN 模型的大小，也就是网络的复杂度代表了其学习能力的容量。在没训练出模型之前，我们并不知道究竟多大的网络才适合我们给定的
任务和数据集，我们并不知道多少的学习容量才是合适的。所以上面说的大小可能根本无法估计。</p><p>普遍观念认为模型压缩通常能大幅减少参数数量，压缩空间，从而降低计算量。从而更好的部署到手机或者无人机上。</p><p>在剪枝过程中，根据一定的标准，对冗余权重进行修剪并保留重要权重，以最大限度地保持精确性。</p><p>步骤</p><ol><li>首先训练一个大型模型</li><li>然后进行剪枝</li><li>最后微调</li></ol><p>方法</p><ol><li>pruning – 剪枝</li><li>quantization</li><li>knowledge distillation – 知识蒸馏</li><li>low-rank decomposition</li><li>compact architecture design</li></ol><p>文章 <a href=https://arxiv.org/pdf/1810.05270v2.pdf>Rethinking the Value of Network Pruning</a> 指出，自动剪枝，选择的并不是重要的权重，而是在进行网络架构搜索 network
architecture search</p><p>记得微软有一篇文章提出更好的训练网络的方法：由于剪枝后的网络比同等规模的网络效果好，说明网络没有被很好的训练。</p><p>参考</p><ol><li><a href=https://www.zhihu.com/question/303922732>为什么要压缩模型，而不是直接训练一个小的CNN？</a></li></ol></div></div><div id=outline-container-headline-10 class=outline-3><h3 id=headline-10>参考</h3><div id=outline-text-headline-10 class=outline-text-3><ol><li><a href=https://blog.floydhub.com/ten-techniques-from-fast-ai/>Ten Techniques Learned From fast.ai</a></li><li><a href=https://neuralnetworksanddeeplearning.com/chap5.html>Why are deep neural networks hard to train?</a></li></ol></div></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://kylestones.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>深度学习</a></li></ul><nav class=paginav><a class=prev href=https://kylestones.github.io/blog/machinelearning/guess/><span class=title>« Prev</span><br><span>乱想</span></a>
<a class=next href=https://kylestones.github.io/blog/machinelearning/todo/><span class=title>Next »</span><br><span>TODO</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share fast.ai on twitter" href="https://twitter.com/intent/tweet/?text=fast.ai&url=https%3a%2f%2fkylestones.github.io%2fblog%2fmachinelearning%2ffast-ai%2f&hashtags=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share fast.ai on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fkylestones.github.io%2fblog%2fmachinelearning%2ffast-ai%2f&title=fast.ai&summary=fast.ai&source=https%3a%2f%2fkylestones.github.io%2fblog%2fmachinelearning%2ffast-ai%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share fast.ai on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fkylestones.github.io%2fblog%2fmachinelearning%2ffast-ai%2f&title=fast.ai"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share fast.ai on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fkylestones.github.io%2fblog%2fmachinelearning%2ffast-ai%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share fast.ai on whatsapp" href="https://api.whatsapp.com/send?text=fast.ai%20-%20https%3a%2f%2fkylestones.github.io%2fblog%2fmachinelearning%2ffast-ai%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share fast.ai on telegram" href="https://telegram.me/share/url?text=fast.ai&url=https%3a%2f%2fkylestones.github.io%2fblog%2fmachinelearning%2ffast-ai%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://kylestones.github.io>Org Mode</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>