<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>卷积神经网络进化 | Org Mode</title><meta name=keywords content="深度学习"><meta name=description content="总体趋势：选取的函数越来越简单，手工设计的部分越来越少 CNN   Yann Lecun 在 1998 的 LeNet 奠定了神经网络的基本架构 : CONV - POLING - FC 。 激活函数   在经典的神经网络以及 LeNet 中使用的激活函数都是 sigmoid 函数。sigmoid 函数是非线性函数，且在输入较大或者较小的时候斜率会 变得很小，不利于参数的学习。  从 AlexNet 开始，激活函数变成了 ReLU ，为分段线性，且 non-saturating，大大加快了网络的训练速度。同时为防止过拟合，提出了 Dropout 方法， Dropout 随机的使网络中的一些节点失活，使得节点不能过度依赖某一个输入，从而权重得以分散开来，另外使用随机 失活的网络，有预训练的效果，类似于先训练一个简单的网络，然后在没有失活的大型网络上 fine-tune 。虽然 AlexNet 网络与 LeNet 的架构基本相同，但由于其 ReLU 和 Dropout 等方法的使用，网络使用了 120 万张训练图片，从数据中学到了更本质的特征，将 cumulative match character (CMC) top5的正确率一下子提升了 10% ，成功掀起了深度学习的研究热潮。  何凯明大神在一次报告中使用 RevoLUtion 来表示 ReLU 对深度学习的贡献，同时使用红色字体高亮了单词中的 ReLU ，非常形象。 Network in NetWork   除了 mini-batch size 外，网络的一层的输入维度为 height * width * channels ，可以通过 polling 操作来减小 height 和 width ，但是怎样减少 channel 的个数呢？ 1 * 1 卷积可以大显身手。当然，如果你愿意也可以用来增加 channel 的个数。 DepthWise convolution   卷积时，并不再是同时对所有的通道进行卷积，而是分别对每个通道使用不同的滤波器进行卷积，得到同等数量的新的 feature maps， 然后使用 1x1 的卷积宽通道进行卷积操作。  不同的通道得到的是不同的特征，需要分别对每个通道进行单独的处理，所以使用 DW ，各个通道之间也可能有关联，所以最后进行 1x1 卷积综合各个通道的特征。  这样不仅大大减少了参数的个数，也加速了网络的计算速度。 网络架构    AlexNet    有大量的超参需要手工调节。 需要仔细设计了什么时候使用卷积层、什么时候使用池化层以及卷积核的大小   VGG    没有太多的超参。虽然有 16 个权重层，但总体结构并不复杂。固定卷积核大小为 3 * 3、步长为 1、same padding，池化层 2 * 2、步长为 2 ；网络的结构很规整：总是几个卷积层后接一个池化层、滤波器的个数不断更新为原来的 2 倍， 从而图像 的宽和高每次池化后都缩减到一半、每组卷积的通道数都增加一倍。   GoogLeNet    采用模块化结构。将 1 * 1 卷积、3 * 3 卷积、5 * 5 卷积、max pooling （需要 same padding ，且步长改为 1 使 得图像的高和宽保持不变）全部在一个网络层中使用，将每一种操作得到的结果堆叠起来得到网络的输出，让网络自己 决定这一层网络到底需要什么操作，而不人工指定该层就是卷积层或者池化层或者全连接层。   网络深度   最初使用 BP 算法的神经网络只有两层；LeNet 进化成了 7 层；AlexNet 8 层；VGG 飞升到 19 层；GoogLeNet 22 层；而 ResNet 使用 skip connection 直接进化到了 152 层，后来成功训练了 1000 层的网络。 Batch Normalization   为了让模型更加容易训练，通常会先将样本进行预处理，其中一个关键的预处理方法就是将样本进行归一化处理。归一化之后样本在不同 的维度分布更加合理，有利于加速模型训练。  为了让后一层网络更容易训练，Batch Normalization 让网络的每一层输出都进行归一化，显著减小了多层之间协调更新的问题，输入的 稳定使得网络的每一层可以单独训练。  很好的正则化方法，何凯明大神说可以替代其他所有的正则化方法。  Batch normalization 是优化深度神经网络中最激动人心的创新之一。另外并不希望所有网络层的输出都是0 均值、方差为 1 ，所以 batch norm 为每个节点增加了均值和方差两个参数来调节归一化结果的分布，这两个参数由网络学习得到。又由于增加了均值这个参数 使得节点原来的偏移参数 b 不再有意义，可以去掉。  可以有两种不同的使用方法：在求取激活函数之前进行归一化，然后再利用激活函数得到该层网络的输出；也可以先计算激活函数的输出， 然后再进行归一化。第一种方法较为常用。  \begin{align*} μ = \frac{1}{m} ∑_i Z[l](i) σ^2 = \frac{1}{m} ∑_i (Z[l](i) - μ)^2 Znorm[l](i) = \frac{Z[l](i) - μ}{\sqrt{σ^2+ε}} {\widetilde{Z}}[l](i) = γ Znorm[l](i) + β \end{align*}  使用 mini-batch 前向传播的时候在计算激活函数之前先使用 batch norm ，然后计算激活函数，继续传播；反向传播时使用和求取权重 参数 W 一样的方法来求取均值和方差参数 \(d\gamma, \ d\beta\) 。在卷积层之后使用需要计算所有 channel 的平均值。  batch norm 使得网络每一层的输出值都得到归一化，归一化到某个分布。这将减小前面层网络参数的变化对后面层权重的影响，因为不 论前面层如何变化，都始终服从某个固定的分布，当前面层的输入变化时，其输出变化不会很大，所以后面的网络层的输入不会变化很大， 从而前面输入的变化对后面层网络权重参数的训练的影响减小，类似 达到了让每层网络参数独立训练的效果 。另外 batch norm 还有 正则化的效果，由于使用 mini-batch 只是所有训练样本的一小部分，所以其均值和方法都含有一定的噪声，每次使用 mini-batch的样 本去训练网络，并用含有噪声的均值和方法去归一化每一层的输出，就类似于 Dropout 随机丢弃网络中神经元节点一样，达到了的正则 化的效果。  测试时一般一次只输入一个样本，而不是像训练时那样，每次使用 mini-batch size 数量的样本。需要使用训练样本来估计网络每一层 输出的均值和方差，并用于测试时使用。一般使用不同的 mini-batch 的各个层输出值的指数加权平均来估计  \begin{align*} {μmean}[l] & = β {μmean}[l] + (1-β) {μ}^{\{i\}[l]} {σmean}2[l] & = β {σmean}2[l] + (1-β) {σ}^{2\{i\}[l]} \end{align*}  疑问：这里求取平均值只是穿越了不同的 mini-batch ，那么不用关系 epoch 吗？是不是取最后一个 epoch 的所有 mini-batch 的平均 效果更好？感觉这个好像就是训练好网络之后，又重新将所有训练样本训练一般一样。吴恩达说两者的效果都不错。这里的平均值次数是 不是应该选的比较大一点？0."><meta name=author content="Kyle Three Stones"><link rel=canonical href=https://kylestones.github.io/hugo-blog/blog/machinelearning/revolution/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/hugo-blog/assets/css/stylesheet.abc7c82c3d415a6df50430738d1cbcc4c76fea558bc5a0c830d3babf78167a35.css integrity="sha256-q8fILD1BWm31BDBzjRy8xMdv6lWLxaDIMNO6v3gWejU=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/hugo-blog/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad();></script><link rel=icon href=https://kylestones.github.io/hugo-blog/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://kylestones.github.io/hugo-blog/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://kylestones.github.io/hugo-blog/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://kylestones.github.io/hugo-blog/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://kylestones.github.io/hugo-blog/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme: rgb(29, 30, 32);--entry: rgb(46, 46, 51);--primary: rgb(218, 218, 219);--secondary: rgb(155, 156, 157);--tertiary: rgb(65, 66, 68);--content: rgb(196, 196, 197);--hljs-bg: rgb(46, 46, 51);--code-bg: rgb(55, 56, 62);--border: rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','UA-123-45','auto');ga('send','pageview');}</script><meta property="og:title" content="卷积神经网络进化"><meta property="og:description" content="总体趋势：选取的函数越来越简单，手工设计的部分越来越少 CNN   Yann Lecun 在 1998 的 LeNet 奠定了神经网络的基本架构 : CONV - POLING - FC 。 激活函数   在经典的神经网络以及 LeNet 中使用的激活函数都是 sigmoid 函数。sigmoid 函数是非线性函数，且在输入较大或者较小的时候斜率会 变得很小，不利于参数的学习。  从 AlexNet 开始，激活函数变成了 ReLU ，为分段线性，且 non-saturating，大大加快了网络的训练速度。同时为防止过拟合，提出了 Dropout 方法， Dropout 随机的使网络中的一些节点失活，使得节点不能过度依赖某一个输入，从而权重得以分散开来，另外使用随机 失活的网络，有预训练的效果，类似于先训练一个简单的网络，然后在没有失活的大型网络上 fine-tune 。虽然 AlexNet 网络与 LeNet 的架构基本相同，但由于其 ReLU 和 Dropout 等方法的使用，网络使用了 120 万张训练图片，从数据中学到了更本质的特征，将 cumulative match character (CMC) top5的正确率一下子提升了 10% ，成功掀起了深度学习的研究热潮。  何凯明大神在一次报告中使用 RevoLUtion 来表示 ReLU 对深度学习的贡献，同时使用红色字体高亮了单词中的 ReLU ，非常形象。 Network in NetWork   除了 mini-batch size 外，网络的一层的输入维度为 height * width * channels ，可以通过 polling 操作来减小 height 和 width ，但是怎样减少 channel 的个数呢？ 1 * 1 卷积可以大显身手。当然，如果你愿意也可以用来增加 channel 的个数。 DepthWise convolution   卷积时，并不再是同时对所有的通道进行卷积，而是分别对每个通道使用不同的滤波器进行卷积，得到同等数量的新的 feature maps， 然后使用 1x1 的卷积宽通道进行卷积操作。  不同的通道得到的是不同的特征，需要分别对每个通道进行单独的处理，所以使用 DW ，各个通道之间也可能有关联，所以最后进行 1x1 卷积综合各个通道的特征。  这样不仅大大减少了参数的个数，也加速了网络的计算速度。 网络架构    AlexNet    有大量的超参需要手工调节。 需要仔细设计了什么时候使用卷积层、什么时候使用池化层以及卷积核的大小   VGG    没有太多的超参。虽然有 16 个权重层，但总体结构并不复杂。固定卷积核大小为 3 * 3、步长为 1、same padding，池化层 2 * 2、步长为 2 ；网络的结构很规整：总是几个卷积层后接一个池化层、滤波器的个数不断更新为原来的 2 倍， 从而图像 的宽和高每次池化后都缩减到一半、每组卷积的通道数都增加一倍。   GoogLeNet    采用模块化结构。将 1 * 1 卷积、3 * 3 卷积、5 * 5 卷积、max pooling （需要 same padding ，且步长改为 1 使 得图像的高和宽保持不变）全部在一个网络层中使用，将每一种操作得到的结果堆叠起来得到网络的输出，让网络自己 决定这一层网络到底需要什么操作，而不人工指定该层就是卷积层或者池化层或者全连接层。   网络深度   最初使用 BP 算法的神经网络只有两层；LeNet 进化成了 7 层；AlexNet 8 层；VGG 飞升到 19 层；GoogLeNet 22 层；而 ResNet 使用 skip connection 直接进化到了 152 层，后来成功训练了 1000 层的网络。 Batch Normalization   为了让模型更加容易训练，通常会先将样本进行预处理，其中一个关键的预处理方法就是将样本进行归一化处理。归一化之后样本在不同 的维度分布更加合理，有利于加速模型训练。  为了让后一层网络更容易训练，Batch Normalization 让网络的每一层输出都进行归一化，显著减小了多层之间协调更新的问题，输入的 稳定使得网络的每一层可以单独训练。  很好的正则化方法，何凯明大神说可以替代其他所有的正则化方法。  Batch normalization 是优化深度神经网络中最激动人心的创新之一。另外并不希望所有网络层的输出都是0 均值、方差为 1 ，所以 batch norm 为每个节点增加了均值和方差两个参数来调节归一化结果的分布，这两个参数由网络学习得到。又由于增加了均值这个参数 使得节点原来的偏移参数 b 不再有意义，可以去掉。  可以有两种不同的使用方法：在求取激活函数之前进行归一化，然后再利用激活函数得到该层网络的输出；也可以先计算激活函数的输出， 然后再进行归一化。第一种方法较为常用。  \begin{align*} μ = \frac{1}{m} ∑_i Z[l](i) σ^2 = \frac{1}{m} ∑_i (Z[l](i) - μ)^2 Znorm[l](i) = \frac{Z[l](i) - μ}{\sqrt{σ^2+ε}} {\widetilde{Z}}[l](i) = γ Znorm[l](i) + β \end{align*}  使用 mini-batch 前向传播的时候在计算激活函数之前先使用 batch norm ，然后计算激活函数，继续传播；反向传播时使用和求取权重 参数 W 一样的方法来求取均值和方差参数 \(d\gamma, \ d\beta\) 。在卷积层之后使用需要计算所有 channel 的平均值。  batch norm 使得网络每一层的输出值都得到归一化，归一化到某个分布。这将减小前面层网络参数的变化对后面层权重的影响，因为不 论前面层如何变化，都始终服从某个固定的分布，当前面层的输入变化时，其输出变化不会很大，所以后面的网络层的输入不会变化很大， 从而前面输入的变化对后面层网络权重参数的训练的影响减小，类似 达到了让每层网络参数独立训练的效果 。另外 batch norm 还有 正则化的效果，由于使用 mini-batch 只是所有训练样本的一小部分，所以其均值和方法都含有一定的噪声，每次使用 mini-batch的样 本去训练网络，并用含有噪声的均值和方法去归一化每一层的输出，就类似于 Dropout 随机丢弃网络中神经元节点一样，达到了的正则 化的效果。  测试时一般一次只输入一个样本，而不是像训练时那样，每次使用 mini-batch size 数量的样本。需要使用训练样本来估计网络每一层 输出的均值和方差，并用于测试时使用。一般使用不同的 mini-batch 的各个层输出值的指数加权平均来估计  \begin{align*} {μmean}[l] & = β {μmean}[l] + (1-β) {μ}^{\{i\}[l]} {σmean}2[l] & = β {σmean}2[l] + (1-β) {σ}^{2\{i\}[l]} \end{align*}  疑问：这里求取平均值只是穿越了不同的 mini-batch ，那么不用关系 epoch 吗？是不是取最后一个 epoch 的所有 mini-batch 的平均 效果更好？感觉这个好像就是训练好网络之后，又重新将所有训练样本训练一般一样。吴恩达说两者的效果都不错。这里的平均值次数是 不是应该选的比较大一点？0."><meta property="og:type" content="article"><meta property="og:url" content="https://kylestones.github.io/hugo-blog/blog/machinelearning/revolution/"><meta property="og:image" content="https://kylestones.github.io/hugo-blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://kylestones.github.io/hugo-blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="卷积神经网络进化"><meta name=twitter:description content="总体趋势：选取的函数越来越简单，手工设计的部分越来越少 CNN   Yann Lecun 在 1998 的 LeNet 奠定了神经网络的基本架构 : CONV - POLING - FC 。 激活函数   在经典的神经网络以及 LeNet 中使用的激活函数都是 sigmoid 函数。sigmoid 函数是非线性函数，且在输入较大或者较小的时候斜率会 变得很小，不利于参数的学习。  从 AlexNet 开始，激活函数变成了 ReLU ，为分段线性，且 non-saturating，大大加快了网络的训练速度。同时为防止过拟合，提出了 Dropout 方法， Dropout 随机的使网络中的一些节点失活，使得节点不能过度依赖某一个输入，从而权重得以分散开来，另外使用随机 失活的网络，有预训练的效果，类似于先训练一个简单的网络，然后在没有失活的大型网络上 fine-tune 。虽然 AlexNet 网络与 LeNet 的架构基本相同，但由于其 ReLU 和 Dropout 等方法的使用，网络使用了 120 万张训练图片，从数据中学到了更本质的特征，将 cumulative match character (CMC) top5的正确率一下子提升了 10% ，成功掀起了深度学习的研究热潮。  何凯明大神在一次报告中使用 RevoLUtion 来表示 ReLU 对深度学习的贡献，同时使用红色字体高亮了单词中的 ReLU ，非常形象。 Network in NetWork   除了 mini-batch size 外，网络的一层的输入维度为 height * width * channels ，可以通过 polling 操作来减小 height 和 width ，但是怎样减少 channel 的个数呢？ 1 * 1 卷积可以大显身手。当然，如果你愿意也可以用来增加 channel 的个数。 DepthWise convolution   卷积时，并不再是同时对所有的通道进行卷积，而是分别对每个通道使用不同的滤波器进行卷积，得到同等数量的新的 feature maps， 然后使用 1x1 的卷积宽通道进行卷积操作。  不同的通道得到的是不同的特征，需要分别对每个通道进行单独的处理，所以使用 DW ，各个通道之间也可能有关联，所以最后进行 1x1 卷积综合各个通道的特征。  这样不仅大大减少了参数的个数，也加速了网络的计算速度。 网络架构    AlexNet    有大量的超参需要手工调节。 需要仔细设计了什么时候使用卷积层、什么时候使用池化层以及卷积核的大小   VGG    没有太多的超参。虽然有 16 个权重层，但总体结构并不复杂。固定卷积核大小为 3 * 3、步长为 1、same padding，池化层 2 * 2、步长为 2 ；网络的结构很规整：总是几个卷积层后接一个池化层、滤波器的个数不断更新为原来的 2 倍， 从而图像 的宽和高每次池化后都缩减到一半、每组卷积的通道数都增加一倍。   GoogLeNet    采用模块化结构。将 1 * 1 卷积、3 * 3 卷积、5 * 5 卷积、max pooling （需要 same padding ，且步长改为 1 使 得图像的高和宽保持不变）全部在一个网络层中使用，将每一种操作得到的结果堆叠起来得到网络的输出，让网络自己 决定这一层网络到底需要什么操作，而不人工指定该层就是卷积层或者池化层或者全连接层。   网络深度   最初使用 BP 算法的神经网络只有两层；LeNet 进化成了 7 层；AlexNet 8 层；VGG 飞升到 19 层；GoogLeNet 22 层；而 ResNet 使用 skip connection 直接进化到了 152 层，后来成功训练了 1000 层的网络。 Batch Normalization   为了让模型更加容易训练，通常会先将样本进行预处理，其中一个关键的预处理方法就是将样本进行归一化处理。归一化之后样本在不同 的维度分布更加合理，有利于加速模型训练。  为了让后一层网络更容易训练，Batch Normalization 让网络的每一层输出都进行归一化，显著减小了多层之间协调更新的问题，输入的 稳定使得网络的每一层可以单独训练。  很好的正则化方法，何凯明大神说可以替代其他所有的正则化方法。  Batch normalization 是优化深度神经网络中最激动人心的创新之一。另外并不希望所有网络层的输出都是0 均值、方差为 1 ，所以 batch norm 为每个节点增加了均值和方差两个参数来调节归一化结果的分布，这两个参数由网络学习得到。又由于增加了均值这个参数 使得节点原来的偏移参数 b 不再有意义，可以去掉。  可以有两种不同的使用方法：在求取激活函数之前进行归一化，然后再利用激活函数得到该层网络的输出；也可以先计算激活函数的输出， 然后再进行归一化。第一种方法较为常用。  \begin{align*} μ = \frac{1}{m} ∑_i Z[l](i) σ^2 = \frac{1}{m} ∑_i (Z[l](i) - μ)^2 Znorm[l](i) = \frac{Z[l](i) - μ}{\sqrt{σ^2+ε}} {\widetilde{Z}}[l](i) = γ Znorm[l](i) + β \end{align*}  使用 mini-batch 前向传播的时候在计算激活函数之前先使用 batch norm ，然后计算激活函数，继续传播；反向传播时使用和求取权重 参数 W 一样的方法来求取均值和方差参数 \(d\gamma, \ d\beta\) 。在卷积层之后使用需要计算所有 channel 的平均值。  batch norm 使得网络每一层的输出值都得到归一化，归一化到某个分布。这将减小前面层网络参数的变化对后面层权重的影响，因为不 论前面层如何变化，都始终服从某个固定的分布，当前面层的输入变化时，其输出变化不会很大，所以后面的网络层的输入不会变化很大， 从而前面输入的变化对后面层网络权重参数的训练的影响减小，类似 达到了让每层网络参数独立训练的效果 。另外 batch norm 还有 正则化的效果，由于使用 mini-batch 只是所有训练样本的一小部分，所以其均值和方法都含有一定的噪声，每次使用 mini-batch的样 本去训练网络，并用含有噪声的均值和方法去归一化每一层的输出，就类似于 Dropout 随机丢弃网络中神经元节点一样，达到了的正则 化的效果。  测试时一般一次只输入一个样本，而不是像训练时那样，每次使用 mini-batch size 数量的样本。需要使用训练样本来估计网络每一层 输出的均值和方差，并用于测试时使用。一般使用不同的 mini-batch 的各个层输出值的指数加权平均来估计  \begin{align*} {μmean}[l] & = β {μmean}[l] + (1-β) {μ}^{\{i\}[l]} {σmean}2[l] & = β {σmean}2[l] + (1-β) {σ}^{2\{i\}[l]} \end{align*}  疑问：这里求取平均值只是穿越了不同的 mini-batch ，那么不用关系 epoch 吗？是不是取最后一个 epoch 的所有 mini-batch 的平均 效果更好？感觉这个好像就是训练好网络之后，又重新将所有训练样本训练一般一样。吴恩达说两者的效果都不错。这里的平均值次数是 不是应该选的比较大一点？0."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":3,"name":"卷积神经网络进化","item":"https://kylestones.github.io/hugo-blog/blog/machinelearning/revolution/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"卷积神经网络进化","name":"卷积神经网络进化","description":"总体趋势：选取的函数越来越简单，手工设计的部分越来越少 CNN   Yann Lecun 在 1998 的 LeNet 奠定了神经网络的基本架构 : CONV - POLING - FC 。 激活函数   在经典的神经网络以及 LeNet 中使用的激活函数都是 sigmoid 函数。sigmoid 函数是非线性函数，且在输入较大或者较小的时候斜率会 变得很小，不利于参数的学习。  从 AlexNet 开始，激活函数变成了 ReLU ，为分段线性，且 non-saturating，大大加快了网络的训练速度。同时为防止过拟合，提出了 Dropout 方法， Dropout 随机的使网络中的一些节点失活，使得节点不能过度依赖某一个输入，从而权重得以分散开来，另外使用随机 失活的网络，有预训练的效果，类似于先训练一个简单的网络，然后在没有失活的大型网络上 fine-tune 。虽然 AlexNet 网络与 LeNet 的架构基本相同，但由于其 ReLU 和 Dropout 等方法的使用，网络使用了 120 万张训练图片，从数据中学到了更本质的特征，将 cumulative match character (CMC) top5的正确率一下子提升了 10% ，成功掀起了深度学习的研究热潮。  何凯明大神在一次报告中使用 RevoLUtion 来表示 ReLU 对深度学习的贡献，同时使用红色字体高亮了单词中的 ReLU ，非常形象。 Network in NetWork   除了 mini-batch size 外，网络的一层的输入维度为 height * width * channels ，可以通过 polling 操作来减小 height 和 width ，但是怎样减少 channel 的个数呢？ 1 * 1 卷积可以大显身手。当然，如果你愿意也可以用来增加 channel 的个数。 DepthWise convolution   卷积时，并不再是同时对所有的通道进行卷积，而是分别对每个通道使用不同的滤波器进行卷积，得到同等数量的新的 feature maps， 然后使用 1x1 的卷积宽通道进行卷积操作。  不同的通道得到的是不同的特征，需要分别对每个通道进行单独的处理，所以使用 DW ，各个通道之间也可能有关联，所以最后进行 1x1 卷积综合各个通道的特征。  这样不仅大大减少了参数的个数，也加速了网络的计算速度。 网络架构    AlexNet    有大量的超参需要手工调节。 需要仔细设计了什么时候使用卷积层、什么时候使用池化层以及卷积核的大小   VGG    没有太多的超参。虽然有 16 个权重层，但总体结构并不复杂。固定卷积核大小为 3 * 3、步长为 1、same padding，池化层 2 * 2、步长为 2 ；网络的结构很规整：总是几个卷积层后接一个池化层、滤波器的个数不断更新为原来的 2 倍， 从而图像 的宽和高每次池化后都缩减到一半、每组卷积的通道数都增加一倍。   GoogLeNet    采用模块化结构。将 1 * 1 卷积、3 * 3 卷积、5 * 5 卷积、max pooling （需要 same padding ，且步长改为 1 使 得图像的高和宽保持不变）全部在一个网络层中使用，将每一种操作得到的结果堆叠起来得到网络的输出，让网络自己 决定这一层网络到底需要什么操作，而不人工指定该层就是卷积层或者池化层或者全连接层。   网络深度   最初使用 BP 算法的神经网络只有两层；LeNet 进化成了 7 层；AlexNet 8 层；VGG 飞升到 19 层；GoogLeNet 22 层；而 ResNet 使用 skip connection 直接进化到了 152 层，后来成功训练了 1000 层的网络。 Batch Normalization   为了让模型更加容易训练，通常会先将样本进行预处理，其中一个关键的预处理方法就是将样本进行归一化处理。归一化之后样本在不同 的维度分布更加合理，有利于加速模型训练。  为了让后一层网络更容易训练，Batch Normalization 让网络的每一层输出都进行归一化，显著减小了多层之间协调更新的问题，输入的 稳定使得网络的每一层可以单独训练。  很好的正则化方法，何凯明大神说可以替代其他所有的正则化方法。  Batch normalization 是优化深度神经网络中最激动人心的创新之一。另外并不希望所有网络层的输出都是0 均值、方差为 1 ，所以 batch norm 为每个节点增加了均值和方差两个参数来调节归一化结果的分布，这两个参数由网络学习得到。又由于增加了均值这个参数 使得节点原来的偏移参数 b 不再有意义，可以去掉。  可以有两种不同的使用方法：在求取激活函数之前进行归一化，然后再利用激活函数得到该层网络的输出；也可以先计算激活函数的输出， 然后再进行归一化。第一种方法较为常用。  \\begin{align*} μ = \\frac{1}{m} ∑_i Z[l](i) σ^2 = \\frac{1}{m} ∑_i (Z[l](i) - μ)^2 Znorm[l](i) = \\frac{Z[l](i) - μ}{\\sqrt{σ^2+ε}} {\\widetilde{Z}}[l](i) = γ Znorm[l](i) + β \\end{align*}  使用 mini-batch 前向传播的时候在计算激活函数之前先使用 batch norm ，然后计算激活函数，继续传播；反向传播时使用和求取权重 参数 W 一样的方法来求取均值和方差参数 \\(d\\gamma, \\ d\\beta\\) 。在卷积层之后使用需要计算所有 channel 的平均值。  batch norm 使得网络每一层的输出值都得到归一化，归一化到某个分布。这将减小前面层网络参数的变化对后面层权重的影响，因为不 论前面层如何变化，都始终服从某个固定的分布，当前面层的输入变化时，其输出变化不会很大，所以后面的网络层的输入不会变化很大， 从而前面输入的变化对后面层网络权重参数的训练的影响减小，类似 达到了让每层网络参数独立训练的效果 。另外 batch norm 还有 正则化的效果，由于使用 mini-batch 只是所有训练样本的一小部分，所以其均值和方法都含有一定的噪声，每次使用 mini-batch的样 本去训练网络，并用含有噪声的均值和方法去归一化每一层的输出，就类似于 Dropout 随机丢弃网络中神经元节点一样，达到了的正则 化的效果。  测试时一般一次只输入一个样本，而不是像训练时那样，每次使用 mini-batch size 数量的样本。需要使用训练样本来估计网络每一层 输出的均值和方差，并用于测试时使用。一般使用不同的 mini-batch 的各个层输出值的指数加权平均来估计  \\begin{align*} {μmean}[l] \u0026amp; = β {μmean}[l] + (1-β) {μ}^{\\{i\\}[l]} {σmean}2[l] \u0026amp; = β {σmean}2[l] + (1-β) {σ}^{2\\{i\\}[l]} \\end{align*}  疑问：这里求取平均值只是穿越了不同的 mini-batch ，那么不用关系 epoch 吗？是不是取最后一个 epoch 的所有 mini-batch 的平均 效果更好？感觉这个好像就是训练好网络之后，又重新将所有训练样本训练一般一样。吴恩达说两者的效果都不错。这里的平均值次数是 不是应该选的比较大一点？0.","keywords":["深度学习"],"articleBody":"  总体趋势：选取的函数越来越简单，手工设计的部分越来越少 CNN   Yann Lecun 在 1998 的 LeNet 奠定了神经网络的基本架构 : CONV - POLING - FC 。 激活函数   在经典的神经网络以及 LeNet 中使用的激活函数都是 sigmoid 函数。sigmoid 函数是非线性函数，且在输入较大或者较小的时候斜率会 变得很小，不利于参数的学习。  从 AlexNet 开始，激活函数变成了 ReLU ，为分段线性，且 non-saturating，大大加快了网络的训练速度。同时为防止过拟合，提出了 Dropout 方法， Dropout 随机的使网络中的一些节点失活，使得节点不能过度依赖某一个输入，从而权重得以分散开来，另外使用随机 失活的网络，有预训练的效果，类似于先训练一个简单的网络，然后在没有失活的大型网络上 fine-tune 。虽然 AlexNet 网络与 LeNet 的架构基本相同，但由于其 ReLU 和 Dropout 等方法的使用，网络使用了 120 万张训练图片，从数据中学到了更本质的特征，将 cumulative match character (CMC) top5的正确率一下子提升了 10% ，成功掀起了深度学习的研究热潮。  何凯明大神在一次报告中使用 RevoLUtion 来表示 ReLU 对深度学习的贡献，同时使用红色字体高亮了单词中的 ReLU ，非常形象。 Network in NetWork   除了 mini-batch size 外，网络的一层的输入维度为 height * width * channels ，可以通过 polling 操作来减小 height 和 width ，但是怎样减少 channel 的个数呢？ 1 * 1 卷积可以大显身手。当然，如果你愿意也可以用来增加 channel 的个数。 DepthWise convolution   卷积时，并不再是同时对所有的通道进行卷积，而是分别对每个通道使用不同的滤波器进行卷积，得到同等数量的新的 feature maps， 然后使用 1x1 的卷积宽通道进行卷积操作。  不同的通道得到的是不同的特征，需要分别对每个通道进行单独的处理，所以使用 DW ，各个通道之间也可能有关联，所以最后进行 1x1 卷积综合各个通道的特征。  这样不仅大大减少了参数的个数，也加速了网络的计算速度。 网络架构    AlexNet    有大量的超参需要手工调节。 需要仔细设计了什么时候使用卷积层、什么时候使用池化层以及卷积核的大小   VGG    没有太多的超参。虽然有 16 个权重层，但总体结构并不复杂。固定卷积核大小为 3 * 3、步长为 1、same padding，池化层 2 * 2、步长为 2 ；网络的结构很规整：总是几个卷积层后接一个池化层、滤波器的个数不断更新为原来的 2 倍， 从而图像 的宽和高每次池化后都缩减到一半、每组卷积的通道数都增加一倍。   GoogLeNet    采用模块化结构。将 1 * 1 卷积、3 * 3 卷积、5 * 5 卷积、max pooling （需要 same padding ，且步长改为 1 使 得图像的高和宽保持不变）全部在一个网络层中使用，将每一种操作得到的结果堆叠起来得到网络的输出，让网络自己 决定这一层网络到底需要什么操作，而不人工指定该层就是卷积层或者池化层或者全连接层。   网络深度   最初使用 BP 算法的神经网络只有两层；LeNet 进化成了 7 层；AlexNet 8 层；VGG 飞升到 19 层；GoogLeNet 22 层；而 ResNet 使用 skip connection 直接进化到了 152 层，后来成功训练了 1000 层的网络。 Batch Normalization   为了让模型更加容易训练，通常会先将样本进行预处理，其中一个关键的预处理方法就是将样本进行归一化处理。归一化之后样本在不同 的维度分布更加合理，有利于加速模型训练。  为了让后一层网络更容易训练，Batch Normalization 让网络的每一层输出都进行归一化，显著减小了多层之间协调更新的问题，输入的 稳定使得网络的每一层可以单独训练。  很好的正则化方法，何凯明大神说可以替代其他所有的正则化方法。  Batch normalization 是优化深度神经网络中最激动人心的创新之一。另外并不希望所有网络层的输出都是0 均值、方差为 1 ，所以 batch norm 为每个节点增加了均值和方差两个参数来调节归一化结果的分布，这两个参数由网络学习得到。又由于增加了均值这个参数 使得节点原来的偏移参数 b 不再有意义，可以去掉。  可以有两种不同的使用方法：在求取激活函数之前进行归一化，然后再利用激活函数得到该层网络的输出；也可以先计算激活函数的输出， 然后再进行归一化。第一种方法较为常用。  \\begin{align*} μ = \\frac{1}{m} ∑_i Z[l](i) σ^2 = \\frac{1}{m} ∑_i (Z[l](i) - μ)^2 Znorm[l](i) = \\frac{Z[l](i) - μ}{\\sqrt{σ^2+ε}} {\\widetilde{Z}}[l](i) = γ Znorm[l](i) + β \\end{align*}  使用 mini-batch 前向传播的时候在计算激活函数之前先使用 batch norm ，然后计算激活函数，继续传播；反向传播时使用和求取权重 参数 W 一样的方法来求取均值和方差参数 \\(d\\gamma, \\ d\\beta\\) 。在卷积层之后使用需要计算所有 channel 的平均值。  batch norm 使得网络每一层的输出值都得到归一化，归一化到某个分布。这将减小前面层网络参数的变化对后面层权重的影响，因为不 论前面层如何变化，都始终服从某个固定的分布，当前面层的输入变化时，其输出变化不会很大，所以后面的网络层的输入不会变化很大， 从而前面输入的变化对后面层网络权重参数的训练的影响减小，类似 达到了让每层网络参数独立训练的效果 。另外 batch norm 还有 正则化的效果，由于使用 mini-batch 只是所有训练样本的一小部分，所以其均值和方法都含有一定的噪声，每次使用 mini-batch的样 本去训练网络，并用含有噪声的均值和方法去归一化每一层的输出，就类似于 Dropout 随机丢弃网络中神经元节点一样，达到了的正则 化的效果。  测试时一般一次只输入一个样本，而不是像训练时那样，每次使用 mini-batch size 数量的样本。需要使用训练样本来估计网络每一层 输出的均值和方差，并用于测试时使用。一般使用不同的 mini-batch 的各个层输出值的指数加权平均来估计  \\begin{align*} {μmean}[l] \u0026 = β {μmean}[l] + (1-β) {μ}^{\\{i\\}[l]} {σmean}2[l] \u0026 = β {σmean}2[l] + (1-β) {σ}^{2\\{i\\}[l]} \\end{align*}  疑问：这里求取平均值只是穿越了不同的 mini-batch ，那么不用关系 epoch 吗？是不是取最后一个 epoch 的所有 mini-batch 的平均 效果更好？感觉这个好像就是训练好网络之后，又重新将所有训练样本训练一般一样。吴恩达说两者的效果都不错。这里的平均值次数是 不是应该选的比较大一点？0.9999 Loss Funtcion   Learning 和 纯优化并不等价。机器学习问题中，我们关注某个性能度量 P ，但通常无法直接求取，只能间接的优化 P 。一般设置某个 代价函数 J ，通过最小化 J 来提高 P 。因此需要思考 softmax-loss 是否能够较好的代表 P ？   FaceNet    使用 triplet loss 来减小类内离散度、增大类间离散度。需要挑选 hard positive 和 hard negative 人脸对。   contrastive loss    使用欧氏距离，但给不同的类别增加间隔 margin 。margin 肯定是从 SVM 中获取的灵感。   sphereface    将 Softmax-loss 表示成角度距离，且通过设置 m 增加不同类型的角度 margin 。   optimization   由于高维空间中鞍点的个数远远多余局部最优解，而二阶方法，比如牛顿法会寻找梯度为零的点，因此会被吸引到鞍点。无法很好的移植 到深度学习中。梯度下降法似乎是唯一的选择。   SGD    通常使用 mini-batch 梯度下降法。mini-batch 应该相互独立   SGDM    SGD with Momentum 使用导数的指数加权移动平均值来更新参数   RMSProp    使用平方梯度的部分历史来控制学习率，使其抑制较大震荡而快速收敛   Adam    使用指数加权平均后的导致值来更新参数；其超参非常鲁棒，通常使用作者建议的值   Global Average Pooling   全连接层参数过多，容易过拟合，需要 Dropout 等方法来预防。而且这种将前面提取到的特征直接堆叠起来的方法有点不自然。所有作 者提出让每一个 feature map 的全局平均值作为一个节点，有多少个类别就生成多少个 feature map ，然后将所有 feature map 的平 均值输入到 Softmax 进行分类，这样每一个 feature map 代表一类，相比于全连接层，其意义更加明确。而且无需额外的参数，同时可 以融合空间信息。  Take the average of each feature map, and the resulting map is fed directly into the softmax layer. Generate one feature map for each corresponding category of the classification task. Feature map can be easily interpreted as categories confidence map. deconvolution networks   卷积操作可以展开称矩阵操作，以加速计算。在 caffe 中实现为 img2col 。 \\(Y=WX\\)  反卷积就是转置卷积，为卷积的逆向操作，用于增大图像的分辨率。所以在 tensorflow 中表示为 conv_trans 。当卷积的步长大于 1 的时候，反卷积的步长将变成小数，此时的反卷积称为小数步长卷积，需要在输入中间增加 s-1 个值为 0 的单元。  \\begin{gather*} Z = WA dW = dZA^T dA = W^T dZ \\end{gather*} 其他   可参考知乎上大神的总结 卷积神经网络中十大拍案叫绝的操作 ","wordCount":"446","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Kyle Three Stones"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://kylestones.github.io/hugo-blog/blog/machinelearning/revolution/"},"publisher":{"@type":"Organization","name":"Org Mode","logo":{"@type":"ImageObject","url":"https://kylestones.github.io/hugo-blog/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>if(localStorage.getItem("pref-theme")==="dark"){document.body.classList.add('dark');}else if(localStorage.getItem("pref-theme")==="light"){document.body.classList.remove('dark')}else if(window.matchMedia('(prefers-color-scheme: dark)').matches){document.body.classList.add('dark');}</script><header class=header><nav class=nav><div class=logo><a href=https://kylestones.github.io/hugo-blog accesskey=h title="Home (Alt + H)"><img src=https://kylestones.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://kylestones.github.io/hugo-blog/categories/ title=categories><span>categories</span></a></li><li><a href=https://kylestones.github.io/hugo-blog/tags/ title=tags><span>tags</span></a></li><li><a href=https://example.org title=example.org><span>example.org</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://kylestones.github.io/hugo-blog>Home</a></div><h1 class=post-title>卷积神经网络进化</h1><div class=post-meta>3 min&nbsp;·&nbsp;446 words&nbsp;·&nbsp;Kyle Three Stones&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/blog/machinelearning/revolution.org rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><p>总体趋势：选取的函数越来越简单，手工设计的部分越来越少</p><h3 id=headline-1>CNN</h3><p>Yann Lecun 在 1998 的 LeNet 奠定了神经网络的基本架构 : CONV - POLING - FC 。</p><h3 id=headline-2>激活函数</h3><p>在经典的神经网络以及 LeNet 中使用的激活函数都是 sigmoid 函数。sigmoid 函数是非线性函数，且在输入较大或者较小的时候斜率会
变得很小，不利于参数的学习。</p><p>从 AlexNet 开始，激活函数变成了 ReLU ，为分段线性，且 non-saturating，大大加快了网络的训练速度。同时为防止过拟合，提出了
Dropout 方法， Dropout 随机的使网络中的一些节点失活，使得节点不能过度依赖某一个输入，从而权重得以分散开来，另外使用随机
失活的网络，有预训练的效果，类似于先训练一个简单的网络，然后在没有失活的大型网络上 fine-tune 。虽然 AlexNet 网络与 LeNet
的架构基本相同，但由于其 ReLU 和 Dropout 等方法的使用，网络使用了 120 万张训练图片，从数据中学到了更本质的特征，将
cumulative match character (CMC) top5的正确率一下子提升了 10% ，成功掀起了深度学习的研究热潮。</p><p>何凯明大神在一次报告中使用 RevoLUtion 来表示 ReLU 对深度学习的贡献，同时使用红色字体高亮了单词中的 ReLU ，非常形象。</p><h3 id=headline-3>Network in NetWork</h3><p>除了 mini-batch size 外，网络的一层的输入维度为 height * width * channels ，可以通过 polling 操作来减小 height 和 width
，但是怎样减少 channel 的个数呢？ 1 * 1 卷积可以大显身手。当然，如果你愿意也可以用来增加 channel 的个数。</p><h3 id=headline-4>DepthWise convolution</h3><p>卷积时，并不再是同时对所有的通道进行卷积，而是分别对每个通道使用不同的滤波器进行卷积，得到同等数量的新的 feature maps，
然后使用 1x1 的卷积宽通道进行卷积操作。</p><p>不同的通道得到的是不同的特征，需要分别对每个通道进行单独的处理，所以使用 DW ，各个通道之间也可能有关联，所以最后进行 1x1
卷积综合各个通道的特征。</p><p>这样不仅大大减少了参数的个数，也加速了网络的计算速度。</p><h3 id=headline-5>网络架构</h3><dl><dt>AlexNet</dt><dd><p>有大量的超参需要手工调节。 需要仔细设计了什么时候使用卷积层、什么时候使用池化层以及卷积核的大小</p></dd><dt>VGG</dt><dd><p>没有太多的超参。虽然有 16 个权重层，但总体结构并不复杂。固定卷积核大小为 3 * 3、步长为 1、same padding，池化层
2 * 2、步长为 2 ；网络的结构很规整：总是几个卷积层后接一个池化层、滤波器的个数不断更新为原来的 2 倍， 从而图像
的宽和高每次池化后都缩减到一半、每组卷积的通道数都增加一倍。</p></dd><dt>GoogLeNet</dt><dd><p>采用模块化结构。将 1 * 1 卷积、3 * 3 卷积、5 * 5 卷积、max pooling （需要 same padding ，且步长改为 1 使
得图像的高和宽保持不变）全部在一个网络层中使用，将每一种操作得到的结果堆叠起来得到网络的输出，让网络自己
决定这一层网络到底需要什么操作，而不人工指定该层就是卷积层或者池化层或者全连接层。</p></dd></dl><h3 id=headline-6>网络深度</h3><p>最初使用 BP 算法的神经网络只有两层；LeNet 进化成了 7 层；AlexNet 8 层；VGG 飞升到 19 层；GoogLeNet 22 层；而 ResNet 使用
skip connection 直接进化到了 152 层，后来成功训练了 1000 层的网络。</p><h3 id=headline-7>Batch Normalization</h3><p>为了让模型更加容易训练，通常会先将样本进行预处理，其中一个关键的预处理方法就是将样本进行归一化处理。归一化之后样本在不同
的维度分布更加合理，有利于加速模型训练。</p><p>为了让后一层网络更容易训练，Batch Normalization 让网络的每一层输出都进行归一化，显著减小了多层之间协调更新的问题，输入的
稳定使得网络的每一层可以单独训练。</p><p>很好的正则化方法，何凯明大神说可以替代其他所有的正则化方法。</p><p>Batch normalization 是优化深度神经网络中最激动人心的创新之一。另外并不希望所有网络层的输出都是0 均值、方差为 1 ，所以
batch norm 为每个节点增加了均值和方差两个参数来调节归一化结果的分布，这两个参数由网络学习得到。又由于增加了均值这个参数
使得节点原来的偏移参数 b 不再有意义，可以去掉。</p><p>可以有两种不同的使用方法：在求取激活函数之前进行归一化，然后再利用激活函数得到该层网络的输出；也可以先计算激活函数的输出，
然后再进行归一化。第一种方法较为常用。</p><p>\begin{align*}
μ = \frac{1}{m} ∑_i Z<sup>[l](i)</sup><br>σ^2 = \frac{1}{m} ∑_i (Z<sup>[l](i)</sup> - μ)^2<br>Z<sub>norm</sub><sup>[l](i)</sup> = \frac{Z<sup>[l](i)</sup> - μ}{\sqrt{σ^2+ε}}<br>{\widetilde{Z}}<sup>[l](i)</sup> = γ Z<sub>norm</sub><sup>[l](i)</sup> + β
\end{align*}</p><p>使用 mini-batch 前向传播的时候在计算激活函数之前先使用 batch norm ，然后计算激活函数，继续传播；反向传播时使用和求取权重
参数 W 一样的方法来求取均值和方差参数 \(d\gamma, \ d\beta\) 。在卷积层之后使用需要计算所有 channel 的平均值。</p><p>batch norm 使得网络每一层的输出值都得到归一化，归一化到某个分布。这将减小前面层网络参数的变化对后面层权重的影响，因为不
论前面层如何变化，都始终服从某个固定的分布，当前面层的输入变化时，其输出变化不会很大，所以后面的网络层的输入不会变化很大，
从而前面输入的变化对后面层网络权重参数的训练的影响减小，类似 <strong>达到了让每层网络参数独立训练的效果</strong> 。另外 batch norm 还有
正则化的效果，由于使用 mini-batch 只是所有训练样本的一小部分，所以其均值和方法都含有一定的噪声，每次使用 mini-batch的样
本去训练网络，并用含有噪声的均值和方法去归一化每一层的输出，就类似于 Dropout 随机丢弃网络中神经元节点一样，达到了的正则
化的效果。</p><p>测试时一般一次只输入一个样本，而不是像训练时那样，每次使用 mini-batch size 数量的样本。需要使用训练样本来估计网络每一层
输出的均值和方差，并用于测试时使用。一般使用不同的 mini-batch 的各个层输出值的指数加权平均来估计</p><p>\begin{align*}
{μ<sub>mean</sub>}<sup>[l]</sup> & = β {μ<sub>mean</sub>}<sup>[l]</sup> + (1-β) {μ}^{\{i\}[l]}<br>{σ<sub>mean</sub>}<sup>2[l]</sup> & = β {σ<sub>mean</sub>}<sup>2[l]</sup> + (1-β) {σ}^{2\{i\}[l]}<br>\end{align*}</p><p>疑问：这里求取平均值只是穿越了不同的 mini-batch ，那么不用关系 epoch 吗？是不是取最后一个 epoch 的所有 mini-batch 的平均
效果更好？感觉这个好像就是训练好网络之后，又重新将所有训练样本训练一般一样。吴恩达说两者的效果都不错。这里的平均值次数是
不是应该选的比较大一点？0.9999</p><h3 id=headline-8>Loss Funtcion</h3><p>Learning 和 纯优化并不等价。机器学习问题中，我们关注某个性能度量 P ，但通常无法直接求取，只能间接的优化 P 。一般设置某个
代价函数 J ，通过最小化 J 来提高 P 。因此需要思考 softmax-loss 是否能够较好的代表 P ？</p><dl><dt>FaceNet</dt><dd><p>使用 triplet loss 来减小类内离散度、增大类间离散度。需要挑选 hard positive 和 hard negative 人脸对。</p></dd><dt>contrastive loss</dt><dd><p>使用欧氏距离，但给不同的类别增加间隔 margin 。margin 肯定是从 SVM 中获取的灵感。</p></dd><dt>sphereface</dt><dd><p>将 Softmax-loss 表示成角度距离，且通过设置 m 增加不同类型的角度 margin 。</p></dd></dl><h3 id=headline-9>optimization</h3><p>由于高维空间中鞍点的个数远远多余局部最优解，而二阶方法，比如牛顿法会寻找梯度为零的点，因此会被吸引到鞍点。无法很好的移植
到深度学习中。梯度下降法似乎是唯一的选择。</p><dl><dt>SGD</dt><dd><p>通常使用 mini-batch 梯度下降法。mini-batch 应该相互独立</p></dd><dt>SGDM</dt><dd><p>SGD with Momentum 使用导数的指数加权移动平均值来更新参数</p></dd><dt>RMSProp</dt><dd><p>使用平方梯度的部分历史来控制学习率，使其抑制较大震荡而快速收敛</p></dd><dt>Adam</dt><dd><p>使用指数加权平均后的导致值来更新参数；其超参非常鲁棒，通常使用作者建议的值</p></dd></dl><h3 id=headline-10>Global Average Pooling</h3><p>全连接层参数过多，容易过拟合，需要 Dropout 等方法来预防。而且这种将前面提取到的特征直接堆叠起来的方法有点不自然。所有作
者提出让每一个 feature map 的全局平均值作为一个节点，有多少个类别就生成多少个 feature map ，然后将所有 feature map 的平
均值输入到 Softmax 进行分类，这样每一个 feature map 代表一类，相比于全连接层，其意义更加明确。而且无需额外的参数，同时可
以融合空间信息。</p><p>Take the average of each feature map, and the resulting map is fed directly into the softmax layer.
Generate one feature map for each corresponding category of the classification task.
Feature map can be easily interpreted as categories confidence map.</p><h3 id=headline-11>deconvolution networks</h3><p>卷积操作可以展开称矩阵操作，以加速计算。在 caffe 中实现为 img2col 。 \(Y=WX\)</p><p>反卷积就是转置卷积，为卷积的逆向操作，用于增大图像的分辨率。所以在 tensorflow 中表示为 conv_trans 。当卷积的步长大于 1
的时候，反卷积的步长将变成小数，此时的反卷积称为小数步长卷积，需要在输入中间增加 s-1 个值为 0 的单元。</p><p>\begin{gather*}
Z = WA<br>dW = dZA^T<br>dA = W^T dZ
\end{gather*}</p><h3 id=headline-12>其他</h3><p>可参考知乎上大神的总结 <a href="https://zhuanlan.zhihu.com/p/28749411?from=singlemessage">卷积神经网络中十大拍案叫绝的操作</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://kylestones.github.io/hugo-blog/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>深度学习</a></li></ul><nav class=paginav><a class=prev href=https://kylestones.github.io/hugo-blog/blog/emotion/experience/><span class=title>« Prev</span><br><span>他山之石</span></a>
<a class=next href=https://kylestones.github.io/hugo-blog/blog/idea/something-find/><span class=title>Next »</span><br><span>发现</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share 卷积神经网络进化 on twitter" href="https://twitter.com/intent/tweet/?text=%e5%8d%b7%e7%a7%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e8%bf%9b%e5%8c%96&url=https%3a%2f%2fkylestones.github.io%2fhugo-blog%2fblog%2fmachinelearning%2frevolution%2f&hashtags=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512h-386.892c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892zm-253.927 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z" /></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 卷积神经网络进化 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fkylestones.github.io%2fhugo-blog%2fblog%2fmachinelearning%2frevolution%2f&title=%e5%8d%b7%e7%a7%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e8%bf%9b%e5%8c%96&summary=%e5%8d%b7%e7%a7%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e8%bf%9b%e5%8c%96&source=https%3a%2f%2fkylestones.github.io%2fhugo-blog%2fblog%2fmachinelearning%2frevolution%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512h-386.892c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0v-129.439c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02v-126.056c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768h75.024zm-307.552-334.556c-25.674.0-42.448 16.879-42.448 39.002.0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z" /></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 卷积神经网络进化 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fkylestones.github.io%2fhugo-blog%2fblog%2fmachinelearning%2frevolution%2f&title=%e5%8d%b7%e7%a7%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e8%bf%9b%e5%8c%96"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512h-386.892c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zm-119.474 108.193c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zm-160.386-29.702c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z" /></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 卷积神经网络进化 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fkylestones.github.io%2fhugo-blog%2fblog%2fmachinelearning%2frevolution%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978v-192.915h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892z" /></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 卷积神经网络进化 on whatsapp" href="https://api.whatsapp.com/send?text=%e5%8d%b7%e7%a7%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e8%bf%9b%e5%8c%96%20-%20https%3a%2f%2fkylestones.github.io%2fhugo-blog%2fblog%2fmachinelearning%2frevolution%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512h-386.892c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23-13.314-11.876-22.304-26.542-24.916-31.026s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z" /></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 卷积神经网络进化 on telegram" href="https://telegram.me/share/url?text=%e5%8d%b7%e7%a7%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e8%bf%9b%e5%8c%96&url=https%3a%2f%2fkylestones.github.io%2fhugo-blog%2fblog%2fmachinelearning%2frevolution%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47A3.38 3.38.0 0126.49 29.86zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z" /></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://kylestones.github.io/hugo-blog>Org Mode</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z" /></svg></a><script>let menu=document.getElementById('menu')
if(menu){menu.scrollLeft=localStorage.getItem("menu-scroll-position");menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft);}}
document.querySelectorAll('a[href^="#"]').forEach(anchor=>{anchor.addEventListener("click",function(e){e.preventDefault();var id=this.getAttribute("href").substr(1);if(!window.matchMedia('(prefers-reduced-motion: reduce)').matches){document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({behavior:"smooth"});}else{document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();}
if(id==="top"){history.replaceState(null,null," ");}else{history.pushState(null,null,`#${id}`);}});});</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){if(document.body.scrollTop>800||document.documentElement.scrollTop>800){mybutton.style.visibility="visible";mybutton.style.opacity="1";}else{mybutton.style.visibility="hidden";mybutton.style.opacity="0";}};</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{if(document.body.className.includes("dark")){document.body.classList.remove('dark');localStorage.setItem("pref-theme",'light');}else{document.body.classList.add('dark');localStorage.setItem("pref-theme",'dark');}})</script></body></html>